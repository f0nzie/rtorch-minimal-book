# (PART) Basic Tensor Operations {.unnumbered}

# Tensors

*Last update: `r system("git log -1 --format=\"%ad (%h)\" -- 0201-tensors.Rmd", intern = TRUE)`*

In this chapter, we describe the most important PyTorch methods.

```{r tensors-load-rtorch}
library(rTorch)
```

## Tensor data types

```{r default-tensor}
# Default data type
torch$tensor(list(1.2, 3))$dtype  # default for floating point is torch.float32
```

```{r tensor-float64}
# change default data type to float64
torch$set_default_dtype(torch$float64)
torch$tensor(list(1.2, 3))$dtype         # a new floating point tensor
```

### Major tensor types

There are five major type of tensors in PyTorch: byte, float, double, long, and boolean.

```{r all-tensor-types}
library(rTorch)

byte    <- torch$ByteTensor(3L, 3L)
float   <- torch$FloatTensor(3L, 3L)
double  <- torch$DoubleTensor(3L, 3L)
long    <- torch$LongTensor(3L, 3L)
boolean <- torch$BoolTensor(5L, 5L)
```

```{r byte-tensor, collapse=TRUE}
message("byte tensor")
byte
```

```{r float-tensor, collapse=TRUE}
message("float tensor")
float
```

```{r double-tensor, collapse=TRUE}
message("double")
double
```

```{r long-tensor, collapse=TRUE}
message("long")
long
```

```{r bool-tensor, collapse=TRUE}
message("boolean")
boolean
```

### Example: A 4D tensor

A 4D tensor like in MNIST hand-written digits recognition dataset:

```{r mnist-4d}
mnist_4d <- torch$FloatTensor(60000L, 3L, 28L, 28L)
```

```{r mnist-4d-basic-attr, collapse=TRUE, results="markup"}
message("size")
mnist_4d$size()

message("length")
length(mnist_4d)

message("shape, like in numpy")
mnist_4d$shape

message("number of elements")
mnist_4d$numel()
```

### Example: A 3D tensor

Given a 3D tensor:

```{r 3d-tensor}
ft3d <- torch$FloatTensor(4L, 3L, 2L)
ft3d
```

```{r 3d-tensor-basic-attr, collapse=TRUE, results="markup"}
ft3d$size()
length(ft3d)
ft3d$shape
ft3d$numel
```

## Arithmetic of tensors

### Add tensors

```{r add-tensor-3x5}
# add a scalar to a tensor
# 3x5 matrix uniformly distributed between 0 and 1
mat0 <- torch$FloatTensor(3L, 5L)$uniform_(0L, 1L)
mat0 + 0.1
```

### Add tensor elements

```{r add-tensor-element, collapse=TRUE, results="markup"}
# fill a 3x5 matrix with 0.1
mat1 <- torch$FloatTensor(3L, 5L)$uniform_(0.1, 0.1)
print(mat1)

# a vector with all ones
mat2 <- torch$FloatTensor(5L)$uniform_(1, 1)
print(mat2)

# add element (1,1) to another tensor
mat1[1, 1] + mat2
```

Add two tensors using the function `add()`:

```{r add-tensors-with-add}
# PyTorch add two tensors
x = torch$rand(5L, 4L)
y = torch$rand(5L, 4L)

print(x$add(y))
```

Add two tensors using the generic `+`:

```{r}
print(x + y)
```

### Multiply a tensor by a scalar

```{r}
# Multiply tensor by scalar
tensor = torch$ones(4L, dtype=torch$float64)
scalar = np$float64(4.321)
print(scalar)
print(torch$scalar_tensor(scalar))
```

> Notice that we used a NumPy function to create the scalar object `np$float64()`.

Multiply two tensors using the function `mul`:

```{r}
(prod = torch$mul(tensor, torch$scalar_tensor(scalar)))
```

Short version using R generics:

```{r}
(prod = tensor * scalar)
```

## NumPy and PyTorch

`numpy` has been made available as a module in `rTorch`, which means that as soon as rTorch is loaded, you also get all the `numpy` functions available to you. We can call functions from `numpy` referring to it as `np$_a_function`. Examples:

```{r}
# a 2D numpy array  
syn0 <- np$random$rand(3L, 5L)
print(syn0)
```

```{r}
# numpy arrays of zeros
syn1 <- np$zeros(c(5L, 10L))
print(syn1)
```

```{r}
# add a scalar to a numpy array
syn1 = syn1 + 0.1
print(syn1)
```

And the dot product of both:

```{r}
np$dot(syn0, syn1)
```

### Python tuples and R vectors

In `numpy` the shape of a multidimensional array needs to be defined using a `tuple`. in R we do it instead with a `vector`. There are not tuples in R.

In Python, we use a tuple, `(5, 5)` to indicate the shape of the array:

```{python}
import numpy as np
print(np.ones((5, 5)))
```

In R, we use a vector `c(5L, 5L)`. The `L` indicates an integer.

```{r}
l1 <- np$ones(c(5L, 5L))
print(l1)
```

### A numpy array from R vectors

For this matrix, or 2D tensor, we use three R vectors:

```{r}
X <- np$array(rbind(c(1,2,3), c(4,5,6), c(7,8,9)))
print(X)
```

And we could transpose the array using `numpy` as well:

```{r}
np$transpose(X)
```

### numpy arrays to tensors

```{r}
a = np$array(list(1, 2, 3))   # a numpy array
t = torch$as_tensor(a)        # convert it to tensor
print(t)
```

### Create and fill a tensor

We can create the tensor directly from R using `tensor()`:

```{r}
torch$tensor(list( 1,  2,  3))   # create a tensor
t[1L]$fill_(-1)                  # fill element with -1
print(a)
```

### Tensor to array, and viceversa

This is a very common operation in machine learning:

```{r}
# convert tensor to a numpy array
a = torch$rand(5L, 4L)
b = a$numpy()
print(b)
```

```{r}
# convert a numpy array to a tensor
np_a = np$array(c(c(3, 4), c(3, 6)))
t_a = torch$from_numpy(np_a)
print(t_a)
```

## Create tensors

A random 1D tensor:

```{r}
ft1 <- torch$FloatTensor(np$random$rand(5L))
print(ft1)
```

Force a tensor as a `float` of 64-bits:

```{r}
ft2 <- torch$as_tensor(np$random$rand(5L), dtype= torch$float64)
print(ft2)
```

Convert the tensor to a `float` of 16-bits:

```{r}
ft2_dbl <- torch$as_tensor(ft2, dtype = torch$float16)
ft2_dbl
```

Create a tensor of size (5 x 7) with uninitialized memory:

```{r}
a <- torch$FloatTensor(5L, 7L)
print(a)
```

Using arange to create a tensor. `arange` starts at 0.

```{r}
v = torch$arange(9L)
print(v)
```

```{r}
# reshape
(v = v$view(3L, 3L))
```

### Tensor fill

On this tensor:

```{r}
(v = torch$ones(3L, 3L))
```

Fill row 1 with 2s:

```{r}
invisible(v[1L, ]$fill_(2L))
print(v)
```

Fill row 2 with 3s:

```{r}
invisible(v[2L, ]$fill_(3L))
print(v)
```

Fill column 3 with fours (4):

```{r}
invisible(v[, 3]$fill_(4L))
print(v)
```

### Tensor with a range of values

```{r}
# Initialize Tensor with a range of value
v = torch$arange(10L)             # similar to range(5) but creating a Tensor
(v = torch$arange(0L, 10L, step = 1L))  # Size 5. Similar to range(0, 5, 1)
```

### Linear or log scale Tensor

Create a tensor with 10 linear points for (1, 10) inclusive:

```{r}
(v = torch$linspace(1L, 10L, steps = 10L)) 
```

Create a tensor with 10 logarithmic points for (1, 10) inclusive:

```{r}
(v = torch$logspace(start=-10L, end = 10L, steps = 5L)) 
```

### In-place / Out-of-place fill

On this uninitialized tensor:

```{r}
(a <- torch$FloatTensor(5L, 7L))
```

Fill the tensor with the value 3.5:

```{r}
a$fill_(3.5)
```

Add a scalar to the tensor:

```{r}
b <- a$add(4.0)
```

The tensor `a` is still filled with 3.5. A new tensor `b` is returned with values 3.5 + 4.0 = 7.5

```{r}
print(a)
print(b)
```

## Tensor resizing

```{r collapse=TRUE, results="markup"}
x = torch$randn(2L, 3L)            # Size 2x3
print(x)

y = x$view(6L)                     # Resize x to size 6
print(y)

z = x$view(-1L, 2L)                # Size 3x2
print(z)
print(z$shape)
```

### Exercise

Reproduce this tensor:

     0 1 2
     3 4 5
     6 7 8

```{r}
# create a vector with the number of elements
v = torch$arange(9L)

# resize to a 3x3 tensor
(v = v$view(3L, 3L))
```

## Concatenate tensors

```{r}
x = torch$randn(2L, 3L)
print(x)
print(x$shape)
```

### Concatenate by rows

```{r}
(x0 <- torch$cat(list(x, x, x), 0L))
print(x0$shape)
```

### Concatenate by columns

```{r}
(x1 <- torch$cat(list(x, x, x), 1L))
print(x1$shape)
```

## Reshape tensors

### With `chunk()`:

Let's say this is an image tensor with the 3-channels and 28x28 pixels

```{r}
# ----- Reshape tensors -----
img <- torch$ones(3L, 28L, 28L)  # Create the tensor of ones
print(img$size())
```

On the first dimension `dim = 0L`, reshape the tensor:

```{r}
img_chunks <- torch$chunk(img, chunks = 3L, dim = 0L)
print(length(img_chunks))
print(class(img_chunks))
```

`img_chunks` is a `list` of three members.

The first chunk member:

```{r}
# 1st chunk member
img_chunk <- img_chunks[[1]]
print(img_chunk$size())
print(img_chunk$sum())      # if the tensor had all ones, what is the sum?
```

The second chunk member:

```{r}
# 2nd chunk member
img_chunk <- img_chunks[[2]]
print(img_chunk$size())
print(img_chunk$sum())        # if the tensor had all ones, what is the sum?
```

```{r}
# 3rd chunk member
img_chunk <- img_chunks[[3]]
print(img_chunk$size())
print(img_chunk$sum())        # if the tensor had all ones, what is the sum?
```

#### Exercise

1.  Create a tensor of shape 3x28x28 filled with values 0.25 on the first channel
2.  The second channel with 0.5
3.  The third chanel with 0.75
4.  Find the sum for ecah separate channel
5.  Find the sum of all channels

### With `index_select()`:

```{r}
img <- torch$ones(3L, 28L, 28L)  # Create the tensor of ones
img$size()
```

This is the layer 1:

```{r}
# index_select. get layer 1
indices = torch$tensor(c(0L))
img_layer_1 <- torch$index_select(img, dim = 0L, index = indices)
```

The size of the layer:

```{r}
print(img_layer_1$size())
```

The sum of all elements in that layer:

```{r}
print(img_layer_1$sum())
```

This is the layer 2:

```{r}
# index_select. get layer 2
indices = torch$tensor(c(1L))
img_layer_2 <- torch$index_select(img, dim = 0L, index = indices)
print(img_layer_2$size())
print(img_layer_2$sum())
```

This is the layer 3:

```{r}
# index_select. get layer 3
indices = torch$tensor(c(2L))
img_layer_3 <- torch$index_select(img, dim = 0L, index = indices)
print(img_layer_3$size())
print(img_layer_3$sum())
```

## Special tensors

### Identity matrix

```{r}
# identity matrix
eye = torch$eye(3L)              # Create an identity 3x3 tensor
print(eye)
```

```{r}
# a 5x5 identity or unit matrix
torch$eye(5L)
```

### Ones

```{r}
(v = torch$ones(10L))               # A tensor of size 10 containing all ones

# reshape
(v = torch$ones(2L, 1L, 2L, 1L))     # Size 2x1x2x1, a 4D tensor
```

The *matrix of ones* is also called \``unitary matrix`. This is a `4x4` unitary matrix.

```{r}
torch$ones(c(4L, 4L))
```

```{r}
# eye tensor
eye = torch$eye(3L)
print(eye)
# like eye tensor
v = torch$ones_like(eye)     # A tensor with same shape as eye. Fill it with 1.
v
```

### Zeros

```{r}
(z = torch$zeros(10L))             # A tensor of size 10 containing all zeros
```

```{r}
# matrix of zeros
torch$zeros(c(4L, 4L))
```

```{r}
# a 3D tensor of zeros
torch$zeros(c(3L, 4L, 2L))
```

### Diagonal operations

Given the 1D tensor

```{r}
a <- torch$tensor(c(1L, 2L, 3L))
a
```

#### Diagonal matrix

We want to fill the main diagonal with the vector:

```{r}
torch$diag(a)
```

What about filling the diagonal above the main:

```{r}
torch$diag(a, 1L)
```

Or the diagonal below the main:

```{r}
torch$diag(a, -1L)
```

## Access to tensor elements

```{r}
# replace an element at position 0, 0
(new_tensor = torch$Tensor(list(list(1, 2), list(3, 4))))
```

Print element at position `1,1`:

```{r}
print(new_tensor[1L, 1L])
```

Fill element at position `1,1` with 5:

```{r}
new_tensor[1L, 1L]$fill_(5)
```

Show the modified tensor:

```{r}
print(new_tensor)   # tensor([[ 5.,  2.],[ 3.,  4.]])
```

Access an element at position `1, 0`:

```{r}
print(new_tensor[2L, 1L])           # tensor([ 3.])
print(new_tensor[2L, 1L]$item())    # 3.
```

### Indices to tensor elements

On this tensor:

```{r}
x = torch$randn(3L, 4L)
print(x)
```

Select indices, `dim=0`:

```{r}
indices = torch$tensor(list(0L, 2L))
torch$index_select(x, 0L, indices)
```

Select indices, `dim=1`:

```{r}
torch$index_select(x, 1L, indices)
```

### Using the `take` function

```{r}
# Take by indices
src = torch$tensor(list(list(4, 3, 5),
                        list(6, 7, 8)) )
print(src)
print( torch$take(src, torch$tensor(list(0L, 2L, 5L))) )
```

## Other tensor operations

### Cross product

```{r}
m1 = torch$ones(3L, 5L)
m2 = torch$ones(3L, 5L)
v1 = torch$ones(3L)
# Cross product
# Size 3x5
(r = torch$cross(m1, m2))
```

### Dot product

```{r collapse=TRUE, results="markup"}
# Dot product of 2 tensors
# Dot product of 2 tensors

p <- torch$Tensor(list(4L, 2L))
q <- torch$Tensor(list(3L, 1L))                   

(r = torch$dot(p, q))  # 14
(r <- p %.*% q)        # 14
```

## Logical operations

```{r echo=FALSE}
knitr::opts_chunk$set(
    collapse = TRUE,
    results = "markup"
)
```

```{r}
m0 = torch$zeros(3L, 5L)
m1 = torch$ones(3L, 5L)
m2 = torch$eye(3L, 5L)

print(m1 == m0)
```

```{r}
print(m1 != m1)
```

```{r}
print(m2 == m2)
```

```{r}
# AND
m1 & m1
```

```{r}
# OR
m0 | m2
```

```{r}
# OR
m1 | m2
```

### Extract a unique logical result

With `all`:

```{r}
# tensor is less than
A <- torch$ones(60000L, 1L, 28L, 28L)
C <- A * 0.5

# is C < A
all(torch$lt(C, A))
all(C < A)
# is A < C
all(A < C)
```

With function `all_boolean`:

```{r}
all_boolean <- function(x) {
  # convert tensor of 1s and 0s to a unique boolean
  as.logical(torch$all(x)$numpy())
}

# is C < A
all_boolean(torch$lt(C, A))
all_boolean(C < A)

# is A < C
all_boolean(A < C)
```

### Greater than (`gt`)

```{r}
# tensor is greater than
A <- torch$ones(60000L, 1L, 28L, 28L)
D <- A * 2.0
all(torch$gt(D, A))
all(torch$gt(A, D))
```

### Less than or equal (`le`)

```{r}
# tensor is less than or equal
A1 <- torch$ones(60000L, 1L, 28L, 28L)
all(torch$le(A1, A1))
all(A1 <= A1)

# tensor is greater than or equal
A0 <- torch$zeros(60000L, 1L, 28L, 28L)
all(torch$ge(A0, A0))
all(A0 >= A0)

all(A1 >= A0)
all(A1 <= A0)
```

### Logical NOT (`!`)

```{r}
all_true <- torch$BoolTensor(list(TRUE, TRUE, TRUE, TRUE))
all_true

# logical NOT
not_all_true <- !all_true
not_all_true
```

```{r}
diag <- torch$eye(5L)
diag

# logical NOT
not_diag <- !diag

# convert to integer
not_diag$to(dtype=torch$uint8)
```

## Distributions

Initialize a tensor randomized with a normal distribution with `mean=0`, `var=1`:

```{r}
n <- torch$randn(3500L)
n
plot(n$numpy())
hist(n$numpy())
```

```{r}
a  <- torch$randn(8L, 5L, 6L)
# print(a)
print(a$size())

plot(a$flatten()$numpy())
hist(a$flatten()$numpy())
```

### Uniform matrix

```{r}
library(rTorch)

# 3x5 matrix uniformly distributed between 0 and 1
mat0 <- torch$FloatTensor(13L, 15L)$uniform_(0L, 1L)
plot(mat0$flatten()$numpy())
hist(mat0$flatten()$numpy())

```

```{r}
# fill a 3x5 matrix with 0.1
mat1 <- torch$FloatTensor(30L, 50L)$uniform_(0.1, 0.2)
plot(mat1$flatten()$numpy())
hist(mat1$flatten()$numpy())
```

```{r}
# a vector with all ones
mat2 <- torch$FloatTensor(500L)$uniform_(1, 2)
plot(mat2$flatten()$numpy())
hist(mat2$flatten()$numpy())
```

### Binomial distribution

```{r}
Binomial <- torch$distributions$binomial$Binomial

m = Binomial(100, torch$tensor(list(0 , .2, .8, 1)))
(x = m$sample())
```

```{r}
m = Binomial(torch$tensor(list(list(5.), list(10.))), 
             torch$tensor(list(0.5, 0.8)))
(x = m$sample())
```

```{r}
binom <- Binomial(100, torch$FloatTensor(5L, 10L))
print(binom)
```

```{r}
print(binom$sample_n(100L)$shape)
plot(binom$sample_n(100L)$flatten()$numpy())
hist(binom$sample_n(100L)$flatten()$numpy())
```

### Exponential distribution

```{r}
Exponential <- torch$distributions$exponential$Exponential

m = Exponential(torch$tensor(list(1.0)))
m
m$sample()  # Exponential distributed with rate=1
```

```{r}
expo <- Exponential(rate=0.25)
expo_sample <- expo$sample_n(250L)   # generate 250 samples
print(expo_sample$shape)
plot(expo_sample$flatten()$numpy())
hist(expo_sample$flatten()$numpy())
```

### Weibull distribution

```{r}
Weibull <- torch$distributions$weibull$Weibull

m = Weibull(torch$tensor(list(1.0)), torch$tensor(list(1.0)))
m$sample()  # sample from a Weibull distribution with scale=1, concentration=1

```

#### Constant `scale`

```{r}
# constant scale
for (k in 1:10) {
    wei <- Weibull(scale=100, concentration=k)
    wei_sample <- wei$sample_n(500L)
    # plot(wei_sample$flatten()$numpy())
    hist(main=paste0("Scale=100; Concentration=", k),
        wei_sample$flatten()$numpy())
}
```

#### Constant `concentration`

```{r}
# constant concentration
for (s in seq(100, 1000, 100)) {
    wei <- Weibull(scale=s, concentration=1)
    wei_sample <- wei$sample_n(500L)
    # plot(wei_sample$flatten()$numpy())
    hist(main=paste0("Concentration=1; Scale=", s),
        wei_sample$flatten()$numpy())
}
```
