# A neural network step-by-step

_Last update: `r system("git log -1 --format=\"%ad (%h)\" -- 0502-neural_networks-steps.Rmd", intern = TRUE)`_

## Introduction
Source: https://github.com/jcjohnson/pytorch-examples#pytorch-nn

In this example we use the torch `nn` package to implement our two-layer network:

## Select device

```{r select-device-cpu}
library(rTorch)

device = torch$device('cpu')
# device = torch.device('cuda') # Uncomment this to run on GPU
```

* `N` is batch size; 
* `D_in` is input dimension;
* `H` is hidden dimension; 
* `D_out` is output dimension.


## Create the dataset

```{r datasets-64x1000x10}
invisible(torch$manual_seed(0))   # do not show the generator output
N <- 64L; D_in <- 1000L; H <- 100L; D_out <- 10L

# Create random Tensors to hold inputs and outputs
x = torch$randn(N, D_in, device=device)
y = torch$randn(N, D_out, device=device)
```


## Define the model
We use the `nn` package to define our model as a sequence of layers. `nn.Sequential` applies these leayers in sequence to produce an output. Each _Linear Module_ computes the output by using a linear function, and holds also tensors for its weights and biases. After constructing the model we use the `.to()` method to move it to the desired device, which could be `CPU` or `GPU`. Remember that we selected `CPU` with `torch$device('cpu')`.

```{r model-layers}
model <- torch$nn$Sequential(
  torch$nn$Linear(D_in, H),              # first layer
  torch$nn$ReLU(),
  torch$nn$Linear(H, D_out))$to(device)  # output layer

print(model)
```


## The Loss function
The `nn` package also contains definitions of several loss functions; in this case we will use __Mean Squared Error__ ($MSE$) as our loss function. Setting `reduction='sum'` means that we are computing the *sum* of squared errors rather than the __mean__; this is for consistency with the examples above where we manually compute the loss, but in practice it is more common to use the mean squared error as a loss by setting `reduction='elementwise_mean'`.

```{r loss-function}
loss_fn = torch$nn$MSELoss(reduction = 'sum')
```


## Iterate through the dataset

```{r iterate}
learning_rate = 1e-4

for (t in 1:500) {
  # Forward pass: compute predicted y by passing x to the model. Module objects
  # override the __call__ operator so you can call them like functions. When
  # doing so you pass a Tensor of input data to the Module and it produces
  # a Tensor of output data.
  y_pred = model(x)

  # Compute and print loss. We pass Tensors containing the predicted and true
  # values of y, and the loss function returns a Tensor containing the loss.
  loss = loss_fn(y_pred, y)
  cat(t, "\t")
  cat(loss$item(), "\n")
  
  # Zero the gradients before running the backward pass.
  model$zero_grad()

  # Backward pass: compute gradient of the loss with respect to all the learnable
  # parameters of the model. Internally, the parameters of each Module are stored
  # in Tensors with requires_grad=True, so this call will compute gradients for
  # all learnable parameters in the model.
  loss$backward()

  # Update the weights using gradient descent. Each parameter is a Tensor, so
  # we can access its data and gradients like we did before.
  with(torch$no_grad(), {
      for (param in iterate(model$parameters())) {
        # in Python this code is much simpler. In R we have to do some conversions
        # param$data <- torch$sub(param$data,
        #                         torch$mul(param$grad$float(),
        #                           torch$scalar_tensor(learning_rate)))
        param$data <- param$data - param$grad * learning_rate
      }
   })
}  
```


## Using R generics

### Simplify tensor operations

The following two expressions are equivalent, with the first being the long version natural way of doing it in __PyTorch__. The second is using the generics in R for subtraction, multiplication and scalar conversion.

```{r eval=FALSE}
param$data <- torch$sub(param$data,
                        torch$mul(param$grad$float(),
                          torch$scalar_tensor(learning_rate)))
```

```{r eval=FALSE}
param$data <- param$data - param$grad * learning_rate
```


## An elegant neural network

```{r nn-rtorch-elegant}
invisible(torch$manual_seed(0))   # do not show the generator output
# layer properties
N <- 64L; D_in <- 1000L; H <- 100L; D_out <- 10L

# Create random Tensors to hold inputs and outputs
x = torch$randn(N, D_in, device=device)
y = torch$randn(N, D_out, device=device)

# set up the neural network
model <- torch$nn$Sequential(
  torch$nn$Linear(D_in, H),              # first layer
  torch$nn$ReLU(),                       # activation
  torch$nn$Linear(H, D_out))$to(device)  # output layer

# specify how we will be computing the loss
loss_fn = torch$nn$MSELoss(reduction = 'sum')

learning_rate = 1e-4
loss_row <- list(vector())     # collect a list for the final dataframe

for (t in 1:500) {
  # Forward pass: compute predicted y by passing x to the model. Module objects
  # override the __call__ operator so you can call them like functions. When
  # doing so you pass a Tensor of input data to the Module and it produces
  # a Tensor of output data.
  y_pred = model(x)

  # Compute and print loss. We pass Tensors containing the predicted and true
  # values of y, and the loss function returns a Tensor containing the loss.
  loss = loss_fn(y_pred, y)  # (y_pred - y) is a tensor; loss_fn output is a scalar
  loss_row[[t]] <- c(t, loss$item())
  
  # Zero the gradients before running the backward pass.
  model$zero_grad()

  # Backward pass: compute gradient of the loss with respect to all the learnable
  # parameters of the model. Internally, the parameters of each module are stored
  # in tensors with `requires_grad=True`, so this call will compute gradients for
  # all learnable parameters in the model.
  loss$backward()

  # Update the weights using gradient descent. Each parameter is a tensor, so
  # we can access its data and gradients like we did before.
  with(torch$no_grad(), {
      for (param in iterate(model$parameters())) {
        # using R generics
        param$data <- param$data - param$grad * learning_rate
      }
   })
}  
```

## A browseable dataframe

```{r dt-loss-reduce}
library(DT)
loss_df <- data.frame(Reduce(rbind, loss_row), row.names = NULL)
names(loss_df)[1] <- "iter"
names(loss_df)[2] <- "loss"
DT::datatable(loss_df)
```

## Plot the loss at each iteration

```{r plot-loss}
library(ggplot2)
# plot
ggplot(loss_df, aes(x = iter, y = loss)) +
    geom_point()
```

