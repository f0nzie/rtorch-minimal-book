<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 12 Neural Networks using NumPy, r-base, rTorch and PyTorch | A Minimal rTorch Book</title>
  <meta name="description" content="This is a minimal tutorial about using the rTorch package to have fun while doing machine learning. This book was written with bookdown." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 12 Neural Networks using NumPy, r-base, rTorch and PyTorch | A Minimal rTorch Book" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a minimal tutorial about using the rTorch package to have fun while doing machine learning. This book was written with bookdown." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 12 Neural Networks using NumPy, r-base, rTorch and PyTorch | A Minimal rTorch Book" />
  
  <meta name="twitter:description" content="This is a minimal tutorial about using the rTorch package to have fun while doing machine learning. This book was written with bookdown." />
  

<meta name="author" content="Alfonso R. Reyes" />


<meta name="date" content="2020-10-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="rainfall-prediction-with-linear-regression.html"/>
<link rel="next" href="a-step-by-step-neural-network-in-rtorch.html"/>
<script src="libs/header-attrs-2.4.6/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.2/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.15/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>

<script>
/* ========================================================================
 * Bootstrap: transition.js v3.3.7
 * http://getbootstrap.com/javascript/#transitions
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // CSS TRANSITION SUPPORT (Shoutout: http://www.modernizr.com/)
  // ============================================================

  function transitionEnd() {
    var el = document.createElement('bootstrap')

    var transEndEventNames = {
      WebkitTransition : 'webkitTransitionEnd',
      MozTransition    : 'transitionend',
      OTransition      : 'oTransitionEnd otransitionend',
      transition       : 'transitionend'
    }

    for (var name in transEndEventNames) {
      if (el.style[name] !== undefined) {
        return { end: transEndEventNames[name] }
      }
    }

    return false // explicit for ie8 (  ._.)
  }

  // http://blog.alexmaccaw.com/css-transitions
  $.fn.emulateTransitionEnd = function (duration) {
    var called = false
    var $el = this
    $(this).one('bsTransitionEnd', function () { called = true })
    var callback = function () { if (!called) $($el).trigger($.support.transition.end) }
    setTimeout(callback, duration)
    return this
  }

  $(function () {
    $.support.transition = transitionEnd()

    if (!$.support.transition) return

    $.event.special.bsTransitionEnd = {
      bindType: $.support.transition.end,
      delegateType: $.support.transition.end,
      handle: function (e) {
        if ($(e.target).is(this)) return e.handleObj.handler.apply(this, arguments)
      }
    }
  })

}(jQuery);
</script>
<script>
/* ========================================================================
 * Bootstrap: collapse.js v3.3.7
 * http://getbootstrap.com/javascript/#collapse
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */

/* jshint latedef: false */

+function ($) {
  'use strict';

  // COLLAPSE PUBLIC CLASS DEFINITION
  // ================================

  var Collapse = function (element, options) {
    this.$element      = $(element)
    this.options       = $.extend({}, Collapse.DEFAULTS, options)
    this.$trigger      = $('[data-toggle="collapse"][href="#' + element.id + '"],' +
                           '[data-toggle="collapse"][data-target="#' + element.id + '"]')
    this.transitioning = null

    if (this.options.parent) {
      this.$parent = this.getParent()
    } else {
      this.addAriaAndCollapsedClass(this.$element, this.$trigger)
    }

    if (this.options.toggle) this.toggle()
  }

  Collapse.VERSION  = '3.3.7'

  Collapse.TRANSITION_DURATION = 350

  Collapse.DEFAULTS = {
    toggle: true
  }

  Collapse.prototype.dimension = function () {
    var hasWidth = this.$element.hasClass('width')
    return hasWidth ? 'width' : 'height'
  }

  Collapse.prototype.show = function () {
    if (this.transitioning || this.$element.hasClass('in')) return

    var activesData
    var actives = this.$parent && this.$parent.children('.panel').children('.in, .collapsing')

    if (actives && actives.length) {
      activesData = actives.data('bs.collapse')
      if (activesData && activesData.transitioning) return
    }

    var startEvent = $.Event('show.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    if (actives && actives.length) {
      Plugin.call(actives, 'hide')
      activesData || actives.data('bs.collapse', null)
    }

    var dimension = this.dimension()

    this.$element
      .removeClass('collapse')
      .addClass('collapsing')[dimension](0)
      .attr('aria-expanded', true)

    this.$trigger
      .removeClass('collapsed')
      .attr('aria-expanded', true)

    this.transitioning = 1

    var complete = function () {
      this.$element
        .removeClass('collapsing')
        .addClass('collapse in')[dimension]('')
      this.transitioning = 0
      this.$element
        .trigger('shown.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    var scrollSize = $.camelCase(['scroll', dimension].join('-'))

    this.$element
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)[dimension](this.$element[0][scrollSize])
  }

  Collapse.prototype.hide = function () {
    if (this.transitioning || !this.$element.hasClass('in')) return

    var startEvent = $.Event('hide.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    var dimension = this.dimension()

    this.$element[dimension](this.$element[dimension]())[0].offsetHeight

    this.$element
      .addClass('collapsing')
      .removeClass('collapse in')
      .attr('aria-expanded', false)

    this.$trigger
      .addClass('collapsed')
      .attr('aria-expanded', false)

    this.transitioning = 1

    var complete = function () {
      this.transitioning = 0
      this.$element
        .removeClass('collapsing')
        .addClass('collapse')
        .trigger('hidden.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    this.$element
      [dimension](0)
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)
  }

  Collapse.prototype.toggle = function () {
    this[this.$element.hasClass('in') ? 'hide' : 'show']()
  }

  Collapse.prototype.getParent = function () {
    return $(this.options.parent)
      .find('[data-toggle="collapse"][data-parent="' + this.options.parent + '"]')
      .each($.proxy(function (i, element) {
        var $element = $(element)
        this.addAriaAndCollapsedClass(getTargetFromTrigger($element), $element)
      }, this))
      .end()
  }

  Collapse.prototype.addAriaAndCollapsedClass = function ($element, $trigger) {
    var isOpen = $element.hasClass('in')

    $element.attr('aria-expanded', isOpen)
    $trigger
      .toggleClass('collapsed', !isOpen)
      .attr('aria-expanded', isOpen)
  }

  function getTargetFromTrigger($trigger) {
    var href
    var target = $trigger.attr('data-target')
      || (href = $trigger.attr('href')) && href.replace(/.*(?=#[^\s]+$)/, '') // strip for ie7

    return $(target)
  }


  // COLLAPSE PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this   = $(this)
      var data    = $this.data('bs.collapse')
      var options = $.extend({}, Collapse.DEFAULTS, $this.data(), typeof option == 'object' && option)

      if (!data && options.toggle && /show|hide/.test(option)) options.toggle = false
      if (!data) $this.data('bs.collapse', (data = new Collapse(this, options)))
      if (typeof option == 'string') data[option]()
    })
  }

  var old = $.fn.collapse

  $.fn.collapse             = Plugin
  $.fn.collapse.Constructor = Collapse


  // COLLAPSE NO CONFLICT
  // ====================

  $.fn.collapse.noConflict = function () {
    $.fn.collapse = old
    return this
  }


  // COLLAPSE DATA-API
  // =================

  $(document).on('click.bs.collapse.data-api', '[data-toggle="collapse"]', function (e) {
    var $this   = $(this)

    if (!$this.attr('data-target')) e.preventDefault()

    var $target = getTargetFromTrigger($this)
    var data    = $target.data('bs.collapse')
    var option  = data ? 'toggle' : $this.data()

    Plugin.call($target, option)
  })

}(jQuery);
</script>
<script>
window.initializeCodeFolding = function(show) {

  // handlers for show-all and hide all
  $("#rmd-show-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
      // R show code
      $('div.r-code-collapse').each(function() {
        $(this).collapse('show');
      });
      // Python show code
      $('div.py-code-collapse').each(function() {
        $(this).collapse('show');
      }); 
      // Bash show code
      $('div.sh-code-collapse').each(function() {
        $(this).collapse('show');
      }); 
      
  });
  $("#rmd-hide-all-code").click(function() {
      // close the dropdown menu when an option is clicked
      $("#allCodeButton").dropdown("toggle");
      // Hide R code
      $('div.r-code-collapse').each(function() {
        $(this).collapse('hide');
      });
      // Hide Python code
      $('div.py-code-collapse').each(function() {
        $(this).collapse('hide');
      });
      // Hide Bash code
      $('div.sh-code-collapse').each(function() {
        $(this).collapse('hide');
      });
  });


  // index for unique code element ids
  var r_currentIndex  = 1;   // for R code
  var py_currentIndex = 1;   // for Python code
  var sh_currentIndex  = 1;   // for shell code

  // select Python chunks
  var pyCodeBlocks = $('pre.python');
  pyCodeBlocks.each(function() {
    // create a collapsable div to wrap the code in
    var div = $('<div class="collapse py-code-collapse"></div>');
    if (show)
      div.addClass('in');
    var id = 'pycode-643E0F36' + py_currentIndex++;
    div.attr('id', id);
    // "this" refers the code chunk
    $(this).before(div);
    $(this).detach().appendTo(div);
    $(this).css('background-color','#ebfaeb');  // change color of chunk background
    
    // add a show code button right above
    var showCodeText = $('<span>' + (show ? 'Hide Python code' : 'Python code') + '</span>');
    var showCodeButton = $('<button type="button" class="btn btn-default btn-xs code-folding-btn pull-right"></button>');
    showCodeButton.append(showCodeText);
    showCodeButton
        .attr('data-toggle', 'collapse')
        .attr('data-target', '#' + id)
        .attr('aria-expanded', show)
        .attr('aria-controls', id);    
        
    // change the background color of the button
    showCodeButton.css('background-color','#009900');
        
    var buttonRow = $('<div class="row"></div>');
    var buttonCol = $('<div class="col-md-12"></div>');
    buttonCol.append(showCodeButton);
    buttonRow.append(buttonCol);
    div.before(buttonRow);    
    
    // update state of button on show/hide
    div.on('hidden.bs.collapse', function () {
      showCodeText.text('Python code');
    });
    div.on('show.bs.collapse', function () {
      showCodeText.text('Hide Python code');
    });  
   });
  
  

  // select Bash shell chunks
  var shCodeBlocks = $('pre.bash');
  shCodeBlocks.each(function() {
    // create a collapsable div to wrap the code in
    var div = $('<div class="collapse sh-code-collapse"></div>');
    if (show)
      div.addClass('in');
    var id = 'shcode-643E0F36' + sh_currentIndex++;
    div.attr('id', id);
    // "this" refers the code chunk
    $(this).before(div);
    $(this).detach().appendTo(div);
    $(this).css('background-color','#A0A0A0');  // change color of chunk background
    
    // add a show code button right above
    var showCodeText = $('<span>' + (show ? 'Hide Bash code' : 'Bash code') + '</span>');
    var showCodeButton = $('<button type="button" class="btn btn-default btn-xs code-folding-btn pull-right"></button>');
    showCodeButton.append(showCodeText);
    showCodeButton
        .attr('data-toggle', 'collapse')
        .attr('data-target', '#' + id)
        .attr('aria-expanded', show)
        .attr('aria-controls', id);    
        
    // change the background color of the button
    showCodeButton.css('background-color','#cc7a00');
        
    var buttonRow = $('<div class="row"></div>');
    var buttonCol = $('<div class="col-md-12"></div>');
    buttonCol.append(showCodeButton);
    buttonRow.append(buttonCol);
    div.before(buttonRow);    
    
    // update state of button on show/hide
    div.on('hidden.bs.collapse', function () {
      showCodeText.text('Bash code');
    });
    div.on('show.bs.collapse', function () {
      showCodeText.text('Hide Bash code');
    });  
   });  


  // select all R code blocks
  // var rCodeBlocks = $('pre.sourceCode, pre.r, pre.bash, pre.sql, pre.cpp, pre.stan');
  // adding pre.sourceCode confuses the Python button
  var rCodeBlocks = $('pre.r, pre.sql, pre.cpp, pre.stan');
  rCodeBlocks.each(function() {
    // create a collapsable div to wrap the code in
    var div = $('<div class="collapse r-code-collapse"></div>');
    if (show)
      div.addClass('in');
    var id = 'rcode-643E0F36' + r_currentIndex++;
    div.attr('id', id);
    $(this).before(div);
    $(this).detach().appendTo(div);
    $(this).css('background-color','#e6faff'); // change color of chunk background

    // add a show code button right above
    var showCodeText = $('<span>' + (show ? 'Hide R code' : 'R code') + '</span>');
    var showCodeButton = $('<button type="button" class="btn btn-default btn-xs code-folding-btn pull-right"></button>');
    showCodeButton.append(showCodeText);
    showCodeButton
        .attr('data-toggle', 'collapse')
        .attr('data-target', '#' + id)
        .attr('aria-expanded', show)
        .attr('aria-controls', id);
    
    // change the background color of the button        
    showCodeButton.css('background-color','#0000ff');
    
    var buttonRow = $('<div class="row"></div>');
    var buttonCol = $('<div class="col-md-12"></div>');
    buttonCol.append(showCodeButton);
    buttonRow.append(buttonCol);
    div.before(buttonRow);

    // update state of button on show/hide
    div.on('hidden.bs.collapse', function () {
      showCodeText.text('R code');
    });
    div.on('show.bs.collapse', function () {
      showCodeText.text('Hide R code');
    });
  });

}
</script>
<script>
/* ========================================================================
 * Bootstrap: dropdown.js v3.3.7
 * http://getbootstrap.com/javascript/#dropdowns
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // DROPDOWN CLASS DEFINITION
  // =========================

  var backdrop = '.dropdown-backdrop'
  var toggle   = '[data-toggle="dropdown"]'
  var Dropdown = function (element) {
    $(element).on('click.bs.dropdown', this.toggle)
  }

  Dropdown.VERSION = '3.3.7'

  function getParent($this) {
    var selector = $this.attr('data-target')

    if (!selector) {
      selector = $this.attr('href')
      selector = selector && /#[A-Za-z]/.test(selector) && selector.replace(/.*(?=#[^\s]*$)/, '') // strip for ie7
    }

    var $parent = selector && $(selector)

    return $parent && $parent.length ? $parent : $this.parent()
  }

  function clearMenus(e) {
    if (e && e.which === 3) return
    $(backdrop).remove()
    $(toggle).each(function () {
      var $this         = $(this)
      var $parent       = getParent($this)
      var relatedTarget = { relatedTarget: this }

      if (!$parent.hasClass('open')) return

      if (e && e.type == 'click' && /input|textarea/i.test(e.target.tagName) && $.contains($parent[0], e.target)) return

      $parent.trigger(e = $.Event('hide.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this.attr('aria-expanded', 'false')
      $parent.removeClass('open').trigger($.Event('hidden.bs.dropdown', relatedTarget))
    })
  }

  Dropdown.prototype.toggle = function (e) {
    var $this = $(this)

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    clearMenus()

    if (!isActive) {
      if ('ontouchstart' in document.documentElement && !$parent.closest('.navbar-nav').length) {
        // if mobile we use a backdrop because click events don't delegate
        $(document.createElement('div'))
          .addClass('dropdown-backdrop')
          .insertAfter($(this))
          .on('click', clearMenus)
      }

      var relatedTarget = { relatedTarget: this }
      $parent.trigger(e = $.Event('show.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this
        .trigger('focus')
        .attr('aria-expanded', 'true')

      $parent
        .toggleClass('open')
        .trigger($.Event('shown.bs.dropdown', relatedTarget))
    }

    return false
  }

  Dropdown.prototype.keydown = function (e) {
    if (!/(38|40|27|32)/.test(e.which) || /input|textarea/i.test(e.target.tagName)) return

    var $this = $(this)

    e.preventDefault()
    e.stopPropagation()

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    if (!isActive && e.which != 27 || isActive && e.which == 27) {
      if (e.which == 27) $parent.find(toggle).trigger('focus')
      return $this.trigger('click')
    }

    var desc = ' li:not(.disabled):visible a'
    var $items = $parent.find('.dropdown-menu' + desc)

    if (!$items.length) return

    var index = $items.index(e.target)

    if (e.which == 38 && index > 0)                 index--         // up
    if (e.which == 40 && index < $items.length - 1) index++         // down
    if (!~index)                                    index = 0

    $items.eq(index).trigger('focus')
  }


  // DROPDOWN PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this = $(this)
      var data  = $this.data('bs.dropdown')

      if (!data) $this.data('bs.dropdown', (data = new Dropdown(this)))
      if (typeof option == 'string') data[option].call($this)
    })
  }

  var old = $.fn.dropdown

  $.fn.dropdown             = Plugin
  $.fn.dropdown.Constructor = Dropdown


  // DROPDOWN NO CONFLICT
  // ====================

  $.fn.dropdown.noConflict = function () {
    $.fn.dropdown = old
    return this
  }


  // APPLY TO STANDARD DROPDOWN ELEMENTS
  // ===================================

  $(document)
    .on('click.bs.dropdown.data-api', clearMenus)
    .on('click.bs.dropdown.data-api', '.dropdown form', function (e) { e.stopPropagation() })
    .on('click.bs.dropdown.data-api', toggle, Dropdown.prototype.toggle)
    .on('keydown.bs.dropdown.data-api', toggle, Dropdown.prototype.keydown)
    .on('keydown.bs.dropdown.data-api', '.dropdown-menu', Dropdown.prototype.keydown)

}(jQuery);
</script>
<style type="text/css">
.code-folding-btn {
  margin-bottom: 4px;
}

.row { display: flex; }
.collapse { display: none; }
.in { display:block }
.pull-right > .dropdown-menu {
    right: 0;
    left: auto;
}

.dropdown-menu {
    position: absolute;
    top: 100%;
    left: 0;
    z-index: 1000;
    display: none;
    float: left;
    min-width: 160px;
    padding: 5px 0;
    margin: 2px 0 0;
    font-size: 14px;
    text-align: left;
    list-style: none;
    background-color: #fff;
    -webkit-background-clip: padding-box;
    background-clip: padding-box;
    border: 1px solid #ccc;
    border: 1px solid rgba(0,0,0,.15);
    border-radius: 4px;
    -webkit-box-shadow: 0 6px 12px rgba(0,0,0,.175);
    box-shadow: 0 6px 12px rgba(0,0,0,.175);
}

.open > .dropdown-menu {
    display: block;
    color: #ffffff;
    background-color: #ffffff;
    background-image: none;
    border-color: #92897e;
}

.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: 400;
  line-height: 1.42857143;
  color: #000000;
  white-space: nowrap;
}

.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
}

.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
  outline: 0;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #aea79f;
}

.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  cursor: not-allowed;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
}

.btn {
  display: inline-block;
  margin-bottom: 1;
  font-weight: normal;
  text-align: center;
  white-space: nowrap;
  vertical-align: middle;
  -ms-touch-action: manipulation;
      touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  padding: 4px 8px;
  font-size: 14px;
  line-height: 1.42857143;
  border-radius: 4px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #ffffff;
  text-decoration: none;
}
.btn:active,
.btn.active {
  background-image: none;
  outline: 0;
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  filter: alpha(opacity=65);
  opacity: 0.65;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #ffffff;
  background-color: #aea79f; #important
  border-color: #aea79f;
}

.btn-default:focus,
.btn-default.focus {
  color: #ffffff;
  background-color: #978e83;
  border-color: #6f675e;
}

.btn-default:hover {
  color: #ffffff;
  background-color: #978e83;
  border-color: #92897e;
}
.btn-default:active,
.btn-default.active,
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-right: 8px;
  padding-left: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-right: 12px;
  padding-left: 12px;
}
.btn-group.open .dropdown-toggle {
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  box-shadow: none;
}

</style>
<script>
var str = '<div class="btn-group pull-right" style="position: fixed; right: 50px; top: 10px; z-index: 200"><button type="button" class="btn btn-default btn-xs dropdown-toggle" id="allCodeButton" data-toggle="dropdown" aria-haspopup="true" aria-expanded="true" data-_extension-text-contrast=""><span>Code</span> <span class="caret"></span></button><ul class="dropdown-menu" style="min-width: 50px;"><li><a id="rmd-show-all-code" href="#">Show All Code</a></li><li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li></ul></div>';
document.write(str);
</script>
<script>
$(document).ready(function () {
  // show code by default. Use "show" === "hide" to hide
  window.initializeCodeFolding("show" === "show");
});
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    background-color: #ffffff;
    color: #a0a0a0;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0;  padding-left: 4px; }
div.sourceCode
  { color: #1f1c1b; background-color: #ffffff; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span { color: #1f1c1b; } /* Normal */
code span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */
code span.an { color: #ca60ca; } /* Annotation */
code span.at { color: #0057ae; } /* Attribute */
code span.bn { color: #b08000; } /* BaseN */
code span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code span.ch { color: #924c9d; } /* Char */
code span.cn { color: #aa5500; } /* Constant */
code span.co { color: #898887; } /* Comment */
code span.cv { color: #0095ff; } /* CommentVar */
code span.do { color: #607880; } /* Documentation */
code span.dt { color: #0057ae; } /* DataType */
code span.dv { color: #b08000; } /* DecVal */
code span.er { color: #bf0303; text-decoration: underline; } /* Error */
code span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code span.fl { color: #b08000; } /* Float */
code span.fu { color: #644a9b; } /* Function */
code span.im { color: #ff5500; } /* Import */
code span.in { color: #b08000; } /* Information */
code span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code span.op { color: #1f1c1b; } /* Operator */
code span.ot { color: #006e28; } /* Other */
code span.pp { color: #006e28; } /* Preprocessor */
code span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */
code span.sc { color: #3daee9; } /* SpecialChar */
code span.ss { color: #ff5500; } /* SpecialString */
code span.st { color: #bf0303; } /* String */
code span.va { color: #0057ae; } /* Variable */
code span.vs { color: #bf0303; } /* VerbatimString */
code span.wa { color: #bf0303; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The rTorch Minimal Book</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prerequisites</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#installation"><i class="fa fa-check"></i>Installation</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#python-anaconda"><i class="fa fa-check"></i>Python Anaconda</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#example"><i class="fa fa-check"></i>Example</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#automatic-installation"><i class="fa fa-check"></i>Automatic installation</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>I Getting Started</b></span></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#motivation"><i class="fa fa-check"></i><b>1.1</b> Motivation</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#how-do-we-start-using-rtorch"><i class="fa fa-check"></i><b>1.2</b> How do we start using <code>rTorch</code></a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="intro.html"><a href="intro.html#getting-the-pytorch-version"><i class="fa fa-check"></i><b>1.2.1</b> Getting the PyTorch version</a></li>
<li class="chapter" data-level="1.2.2" data-path="intro.html"><a href="intro.html#pytorch-configuration"><i class="fa fa-check"></i><b>1.2.2</b> PyTorch configuration</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#what-can-you-do-with-rtorch"><i class="fa fa-check"></i><b>1.3</b> What can you do with <code>rTorch</code></a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#getting-help"><i class="fa fa-check"></i><b>1.4</b> Getting help</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="pytorch-and-numpy.html"><a href="pytorch-and-numpy.html"><i class="fa fa-check"></i><b>2</b> PyTorch and NumPy</a>
<ul>
<li class="chapter" data-level="2.1" data-path="pytorch-and-numpy.html"><a href="pytorch-and-numpy.html#callable-pytorch-modules-from-rtorch"><i class="fa fa-check"></i><b>2.1</b> Callable PyTorch modules from rTorch</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="pytorch-and-numpy.html"><a href="pytorch-and-numpy.html#the-torchvision-module"><i class="fa fa-check"></i><b>2.1.1</b> The <code>torchvision</code> module</a></li>
<li class="chapter" data-level="2.1.2" data-path="pytorch-and-numpy.html"><a href="pytorch-and-numpy.html#the-numpy-module"><i class="fa fa-check"></i><b>2.1.2</b> The <code>numpy</code> module</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="pytorch-and-numpy.html"><a href="pytorch-and-numpy.html#common-array-and-tensor-operations-in-numpy-and-pytorch"><i class="fa fa-check"></i><b>2.2</b> Common array and tensor operations in NumPy and PyTorch</a></li>
<li class="chapter" data-level="2.3" data-path="pytorch-and-numpy.html"><a href="pytorch-and-numpy.html#python-built-in-functions"><i class="fa fa-check"></i><b>2.3</b> Python built-in functions</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html"><i class="fa fa-check"></i><b>3</b> rTorch vs PyTorch: What’s different</a>
<ul>
<li class="chapter" data-level="3.1" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#calling-objects-from-pytorch"><i class="fa fa-check"></i><b>3.1</b> Calling objects from PyTorch</a></li>
<li class="chapter" data-level="3.2" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#call-modules-and-functions-from-torch"><i class="fa fa-check"></i><b>3.2</b> Call modules and functions from <code>torch</code></a></li>
<li class="chapter" data-level="3.3" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#show-the-attributes-methods-of-a-class-or-pytorch-object"><i class="fa fa-check"></i><b>3.3</b> Show the attributes (methods) of a class or PyTorch object</a></li>
<li class="chapter" data-level="3.4" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#how-to-iterate-through-datasets"><i class="fa fa-check"></i><b>3.4</b> How to iterate through datasets</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#enumeration"><i class="fa fa-check"></i><b>3.4.1</b> Enumeration</a></li>
<li class="chapter" data-level="3.4.2" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#using-enumerate-and-iterate"><i class="fa fa-check"></i><b>3.4.2</b> Using <code>enumerate</code> and <code>iterate</code></a></li>
<li class="chapter" data-level="3.4.3" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#using-a-for-loop-to-iterate"><i class="fa fa-check"></i><b>3.4.3</b> Using a <code>for-loop</code> to iterate</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#zero-gradient"><i class="fa fa-check"></i><b>3.5</b> Zero gradient</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#version-in-python"><i class="fa fa-check"></i><b>3.5.1</b> Version in Python</a></li>
<li class="chapter" data-level="3.5.2" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#version-in-r"><i class="fa fa-check"></i><b>3.5.2</b> Version in R</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#r-generics-for-pytorch-functions"><i class="fa fa-check"></i><b>3.6</b> R generics for PyTorch functions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="converting-tensors.html"><a href="converting-tensors.html"><i class="fa fa-check"></i><b>4</b> Converting tensors</a>
<ul>
<li class="chapter" data-level="4.1" data-path="converting-tensors.html"><a href="converting-tensors.html#transforming-a-tensor-from-numpy-and-viceversa"><i class="fa fa-check"></i><b>4.1</b> Transforming a tensor from <code>numpy</code> and viceversa</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="converting-tensors.html"><a href="converting-tensors.html#convert-a-tensor-to-numpy-object"><i class="fa fa-check"></i><b>4.1.1</b> Convert a tensor to <code>numpy</code> object</a></li>
<li class="chapter" data-level="4.1.2" data-path="converting-tensors.html"><a href="converting-tensors.html#convert-a-numpy-object-to-an-r-object"><i class="fa fa-check"></i><b>4.1.2</b> Convert a <code>numpy</code> object to an <code>R</code> object</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="converting-tensors.html"><a href="converting-tensors.html#transforming-a-tensor-from-pytorch-to-r-and-viceversa"><i class="fa fa-check"></i><b>4.2</b> Transforming a tensor from PyTorch to R and viceversa</a></li>
</ul></li>
<li class="part"><span><b>II Basic Tensor Operations</b></span></li>
<li class="chapter" data-level="5" data-path="tensors.html"><a href="tensors.html"><i class="fa fa-check"></i><b>5</b> Tensors</a>
<ul>
<li class="chapter" data-level="5.1" data-path="tensors.html"><a href="tensors.html#tensor-data-types"><i class="fa fa-check"></i><b>5.1</b> Tensor data types</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="tensors.html"><a href="tensors.html#major-tensor-types"><i class="fa fa-check"></i><b>5.1.1</b> Major tensor types</a></li>
<li class="chapter" data-level="5.1.2" data-path="tensors.html"><a href="tensors.html#example-basic-attributes-of-a-4d-tensor"><i class="fa fa-check"></i><b>5.1.2</b> Example: Basic attributes of a 4D tensor</a></li>
<li class="chapter" data-level="5.1.3" data-path="tensors.html"><a href="tensors.html#example-attributes-of-a-3d-tensor"><i class="fa fa-check"></i><b>5.1.3</b> Example: Attributes of a 3D tensor</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="tensors.html"><a href="tensors.html#arithmetic-of-tensors"><i class="fa fa-check"></i><b>5.2</b> Arithmetic of tensors</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="tensors.html"><a href="tensors.html#add-tensors"><i class="fa fa-check"></i><b>5.2.1</b> Add tensors</a></li>
<li class="chapter" data-level="5.2.2" data-path="tensors.html"><a href="tensors.html#add-an-element-of-a-tensor-to-another-tensor"><i class="fa fa-check"></i><b>5.2.2</b> Add an element of a tensor to another tensor</a></li>
<li class="chapter" data-level="5.2.3" data-path="tensors.html"><a href="tensors.html#multiply-a-tensor-by-a-scalar"><i class="fa fa-check"></i><b>5.2.3</b> Multiply a tensor by a scalar</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="tensors.html"><a href="tensors.html#numpy-and-pytorch"><i class="fa fa-check"></i><b>5.3</b> NumPy and PyTorch</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="tensors.html"><a href="tensors.html#in-python-we-use-tuples-in-r-we-use-vectors"><i class="fa fa-check"></i><b>5.3.1</b> In Python we use Tuples, in R we use vectors</a></li>
<li class="chapter" data-level="5.3.2" data-path="tensors.html"><a href="tensors.html#build-a-numpy-array-from-three-r-vectors"><i class="fa fa-check"></i><b>5.3.2</b> Build a numpy array from three R vectors</a></li>
<li class="chapter" data-level="5.3.3" data-path="tensors.html"><a href="tensors.html#convert-a-numpy-array-to-a-tensor-with-as_tensor"><i class="fa fa-check"></i><b>5.3.3</b> Convert a numpy array to a tensor with <code>as_tensor()</code></a></li>
<li class="chapter" data-level="5.3.4" data-path="tensors.html"><a href="tensors.html#create-and-fill-a-tensor"><i class="fa fa-check"></i><b>5.3.4</b> Create and fill a tensor</a></li>
<li class="chapter" data-level="5.3.5" data-path="tensors.html"><a href="tensors.html#tensor-to-array-and-viceversa"><i class="fa fa-check"></i><b>5.3.5</b> Tensor to array, and viceversa</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="tensors.html"><a href="tensors.html#create-tensors"><i class="fa fa-check"></i><b>5.4</b> Create tensors</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="tensors.html"><a href="tensors.html#tensor-fill"><i class="fa fa-check"></i><b>5.4.1</b> Tensor fill</a></li>
<li class="chapter" data-level="5.4.2" data-path="tensors.html"><a href="tensors.html#initialize-tensor-with-a-range-of-values"><i class="fa fa-check"></i><b>5.4.2</b> Initialize Tensor with a range of values</a></li>
<li class="chapter" data-level="5.4.3" data-path="tensors.html"><a href="tensors.html#initialize-a-linear-or-log-scale-tensor"><i class="fa fa-check"></i><b>5.4.3</b> Initialize a linear or log scale Tensor</a></li>
<li class="chapter" data-level="5.4.4" data-path="tensors.html"><a href="tensors.html#fill-a-tensor-in-place-out-of-place"><i class="fa fa-check"></i><b>5.4.4</b> Fill a tensor In-place / Out-of-place</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="tensors.html"><a href="tensors.html#tensor-resizing"><i class="fa fa-check"></i><b>5.5</b> Tensor resizing</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="tensors.html"><a href="tensors.html#exercise"><i class="fa fa-check"></i><b>5.5.1</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="tensors.html"><a href="tensors.html#concatenate-tensors"><i class="fa fa-check"></i><b>5.6</b> Concatenate tensors</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="tensors.html"><a href="tensors.html#concatenate-tensors-by-dim0-rows"><i class="fa fa-check"></i><b>5.6.1</b> Concatenate tensors by <code>dim=0</code> (rows)</a></li>
<li class="chapter" data-level="5.6.2" data-path="tensors.html"><a href="tensors.html#concatenate-tensors-by-dim1-columns"><i class="fa fa-check"></i><b>5.6.2</b> Concatenate tensors by <code>dim=1</code> (columns)</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="tensors.html"><a href="tensors.html#reshape-tensors"><i class="fa fa-check"></i><b>5.7</b> Reshape tensors</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="tensors.html"><a href="tensors.html#with-chunk"><i class="fa fa-check"></i><b>5.7.1</b> With <code>chunk()</code>:</a></li>
<li class="chapter" data-level="5.7.2" data-path="tensors.html"><a href="tensors.html#with-index_select"><i class="fa fa-check"></i><b>5.7.2</b> With <code>index_select()</code>:</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="tensors.html"><a href="tensors.html#special-tensors"><i class="fa fa-check"></i><b>5.8</b> Special tensors</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="tensors.html"><a href="tensors.html#identity-matrix"><i class="fa fa-check"></i><b>5.8.1</b> Identity matrix</a></li>
<li class="chapter" data-level="5.8.2" data-path="tensors.html"><a href="tensors.html#ones"><i class="fa fa-check"></i><b>5.8.2</b> Ones</a></li>
<li class="chapter" data-level="5.8.3" data-path="tensors.html"><a href="tensors.html#zeros"><i class="fa fa-check"></i><b>5.8.3</b> Zeros</a></li>
<li class="chapter" data-level="5.8.4" data-path="tensors.html"><a href="tensors.html#diagonal-operations"><i class="fa fa-check"></i><b>5.8.4</b> Diagonal operations</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="tensors.html"><a href="tensors.html#access-to-tensor-elements"><i class="fa fa-check"></i><b>5.9</b> Access to tensor elements</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="tensors.html"><a href="tensors.html#using-indices-to-access-elements"><i class="fa fa-check"></i><b>5.9.1</b> Using indices to access elements</a></li>
<li class="chapter" data-level="5.9.2" data-path="tensors.html"><a href="tensors.html#using-the-take-function"><i class="fa fa-check"></i><b>5.9.2</b> Using the <code>take</code> function</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="tensors.html"><a href="tensors.html#other-tensor-operations"><i class="fa fa-check"></i><b>5.10</b> Other tensor operations</a>
<ul>
<li class="chapter" data-level="5.10.1" data-path="tensors.html"><a href="tensors.html#cross-product"><i class="fa fa-check"></i><b>5.10.1</b> Cross product</a></li>
<li class="chapter" data-level="5.10.2" data-path="tensors.html"><a href="tensors.html#dot-product"><i class="fa fa-check"></i><b>5.10.2</b> Dot product</a></li>
</ul></li>
<li class="chapter" data-level="5.11" data-path="tensors.html"><a href="tensors.html#logical-operations"><i class="fa fa-check"></i><b>5.11</b> Logical operations</a>
<ul>
<li class="chapter" data-level="5.11.1" data-path="tensors.html"><a href="tensors.html#using-a-function-to-extract-a-unique-logical-result"><i class="fa fa-check"></i><b>5.11.1</b> Using a function to extract a unique logical result</a></li>
<li class="chapter" data-level="5.11.2" data-path="tensors.html"><a href="tensors.html#greater-than-gt"><i class="fa fa-check"></i><b>5.11.2</b> Greater than (<code>gt</code>)</a></li>
<li class="chapter" data-level="5.11.3" data-path="tensors.html"><a href="tensors.html#less-than-or-equal-le"><i class="fa fa-check"></i><b>5.11.3</b> Less than or equal (<code>le</code>)</a></li>
<li class="chapter" data-level="5.11.4" data-path="tensors.html"><a href="tensors.html#logical-not"><i class="fa fa-check"></i><b>5.11.4</b> Logical NOT (<code>!</code>)</a></li>
</ul></li>
<li class="chapter" data-level="5.12" data-path="tensors.html"><a href="tensors.html#distributions"><i class="fa fa-check"></i><b>5.12</b> Distributions</a>
<ul>
<li class="chapter" data-level="5.12.1" data-path="tensors.html"><a href="tensors.html#uniform-matrix"><i class="fa fa-check"></i><b>5.12.1</b> Uniform matrix</a></li>
<li class="chapter" data-level="5.12.2" data-path="tensors.html"><a href="tensors.html#binomial-distribution"><i class="fa fa-check"></i><b>5.12.2</b> Binomial distribution</a></li>
<li class="chapter" data-level="5.12.3" data-path="tensors.html"><a href="tensors.html#exponential-distribution"><i class="fa fa-check"></i><b>5.12.3</b> Exponential distribution</a></li>
<li class="chapter" data-level="5.12.4" data-path="tensors.html"><a href="tensors.html#weibull-distribution"><i class="fa fa-check"></i><b>5.12.4</b> Weibull distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linearalgebra.html"><a href="linearalgebra.html"><i class="fa fa-check"></i><b>6</b> Linear Algebra with Torch</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linearalgebra.html"><a href="linearalgebra.html#scalars"><i class="fa fa-check"></i><b>6.1</b> Scalars</a></li>
<li class="chapter" data-level="6.2" data-path="linearalgebra.html"><a href="linearalgebra.html#vectors"><i class="fa fa-check"></i><b>6.2</b> Vectors</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="linearalgebra.html"><a href="linearalgebra.html#vector-to-matrix-matrix-to-tensor"><i class="fa fa-check"></i><b>6.2.1</b> Vector to matrix, matrix to tensor</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="linearalgebra.html"><a href="linearalgebra.html#matrices"><i class="fa fa-check"></i><b>6.3</b> Matrices</a></li>
<li class="chapter" data-level="6.4" data-path="linearalgebra.html"><a href="linearalgebra.html#d-tensors"><i class="fa fa-check"></i><b>6.4</b> 3D+ tensors</a></li>
<li class="chapter" data-level="6.5" data-path="linearalgebra.html"><a href="linearalgebra.html#transpose-of-a-matrix"><i class="fa fa-check"></i><b>6.5</b> Transpose of a matrix</a></li>
<li class="chapter" data-level="6.6" data-path="linearalgebra.html"><a href="linearalgebra.html#vectors-special-case-of-a-matrix"><i class="fa fa-check"></i><b>6.6</b> Vectors, special case of a matrix</a></li>
<li class="chapter" data-level="6.7" data-path="linearalgebra.html"><a href="linearalgebra.html#tensor-arithmetic"><i class="fa fa-check"></i><b>6.7</b> Tensor arithmetic</a></li>
<li class="chapter" data-level="6.8" data-path="linearalgebra.html"><a href="linearalgebra.html#add-a-scalar-to-a-tensor"><i class="fa fa-check"></i><b>6.8</b> Add a scalar to a tensor</a></li>
<li class="chapter" data-level="6.9" data-path="linearalgebra.html"><a href="linearalgebra.html#multiplying-tensors"><i class="fa fa-check"></i><b>6.9</b> Multiplying tensors</a></li>
<li class="chapter" data-level="6.10" data-path="linearalgebra.html"><a href="linearalgebra.html#dot-product-1"><i class="fa fa-check"></i><b>6.10</b> Dot product</a>
<ul>
<li class="chapter" data-level="6.10.1" data-path="linearalgebra.html"><a href="linearalgebra.html#dot-product-of-2d-array-using-python"><i class="fa fa-check"></i><b>6.10.1</b> Dot product of 2D array using Python</a></li>
<li class="chapter" data-level="6.10.2" data-path="linearalgebra.html"><a href="linearalgebra.html#dot-product-of-2d-array-using-r"><i class="fa fa-check"></i><b>6.10.2</b> Dot product of 2D array using R</a></li>
<li class="chapter" data-level="6.10.3" data-path="linearalgebra.html"><a href="linearalgebra.html#dot-product-with-mm-and-matmul-functions"><i class="fa fa-check"></i><b>6.10.3</b> Dot product with <code>mm</code> and <code>matmul</code> functions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="creating-pytorch-classes.html"><a href="creating-pytorch-classes.html"><i class="fa fa-check"></i><b>7</b> Creating PyTorch classes</a>
<ul>
<li class="chapter" data-level="7.1" data-path="creating-pytorch-classes.html"><a href="creating-pytorch-classes.html#build-a-pytorch-model-class"><i class="fa fa-check"></i><b>7.1</b> Build a PyTorch model class</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="creating-pytorch-classes.html"><a href="creating-pytorch-classes.html#example-1-a-neural-network-with-one-layer"><i class="fa fa-check"></i><b>7.1.1</b> Example 1: a neural network with one layer</a></li>
<li class="chapter" data-level="7.1.2" data-path="creating-pytorch-classes.html"><a href="creating-pytorch-classes.html#example-2-logistic-regression"><i class="fa fa-check"></i><b>7.1.2</b> Example 2: Logistic Regression</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Logistic Regression</b></span></li>
<li class="chapter" data-level="8" data-path="example-1-a-classification-problem-with-logistic-regression.html"><a href="example-1-a-classification-problem-with-logistic-regression.html"><i class="fa fa-check"></i><b>8</b> Example 1: A classification problem with Logistic Regression</a>
<ul>
<li class="chapter" data-level="8.1" data-path="example-1-a-classification-problem-with-logistic-regression.html"><a href="example-1-a-classification-problem-with-logistic-regression.html#code-in-python"><i class="fa fa-check"></i><b>8.1</b> Code in Python</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="mnistdigits.html"><a href="mnistdigits.html"><i class="fa fa-check"></i><b>9</b> Example 2: MNIST handwritten digits</a>
<ul>
<li class="chapter" data-level="9.1" data-path="mnistdigits.html"><a href="mnistdigits.html#code-in-r"><i class="fa fa-check"></i><b>9.1</b> Code in R</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="mnistdigits.html"><a href="mnistdigits.html#hyperparameters"><i class="fa fa-check"></i><b>9.1.1</b> Hyperparameters</a></li>
<li class="chapter" data-level="9.1.2" data-path="mnistdigits.html"><a href="mnistdigits.html#read-datasets"><i class="fa fa-check"></i><b>9.1.2</b> Read datasets</a></li>
<li class="chapter" data-level="9.1.3" data-path="mnistdigits.html"><a href="mnistdigits.html#define-the-model"><i class="fa fa-check"></i><b>9.1.3</b> Define the model</a></li>
<li class="chapter" data-level="9.1.4" data-path="mnistdigits.html"><a href="mnistdigits.html#training"><i class="fa fa-check"></i><b>9.1.4</b> Training</a></li>
<li class="chapter" data-level="9.1.5" data-path="mnistdigits.html"><a href="mnistdigits.html#prediction"><i class="fa fa-check"></i><b>9.1.5</b> Prediction</a></li>
<li class="chapter" data-level="9.1.6" data-path="mnistdigits.html"><a href="mnistdigits.html#save-the-model"><i class="fa fa-check"></i><b>9.1.6</b> Save the model</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="mnistdigits.html"><a href="mnistdigits.html#code-in-python-1"><i class="fa fa-check"></i><b>9.2</b> Code in Python</a></li>
</ul></li>
<li class="part"><span><b>IV Linear Regression</b></span></li>
<li class="chapter" data-level="10" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>10</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="10.1" data-path="linear-regression.html"><a href="linear-regression.html#introduction"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="linear-regression.html"><a href="linear-regression.html#generate-the-dataset"><i class="fa fa-check"></i><b>10.2</b> Generate the dataset</a></li>
<li class="chapter" data-level="10.3" data-path="linear-regression.html"><a href="linear-regression.html#convert-arrays-to-tensors"><i class="fa fa-check"></i><b>10.3</b> Convert arrays to tensors</a></li>
<li class="chapter" data-level="10.4" data-path="linear-regression.html"><a href="linear-regression.html#converting-from-numpy-to-tensor"><i class="fa fa-check"></i><b>10.4</b> Converting from numpy to tensor</a></li>
<li class="chapter" data-level="10.5" data-path="linear-regression.html"><a href="linear-regression.html#creating-the-network-model"><i class="fa fa-check"></i><b>10.5</b> Creating the network model</a></li>
<li class="chapter" data-level="10.6" data-path="linear-regression.html"><a href="linear-regression.html#optimizer-and-loss"><i class="fa fa-check"></i><b>10.6</b> Optimizer and Loss</a></li>
<li class="chapter" data-level="10.7" data-path="linear-regression.html"><a href="linear-regression.html#training-1"><i class="fa fa-check"></i><b>10.7</b> Training</a></li>
<li class="chapter" data-level="10.8" data-path="linear-regression.html"><a href="linear-regression.html#results"><i class="fa fa-check"></i><b>10.8</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="rainfall-prediction-with-linear-regression.html"><a href="rainfall-prediction-with-linear-regression.html"><i class="fa fa-check"></i><b>11</b> Rainfall prediction with Linear Regression</a>
<ul>
<li class="chapter" data-level="11.1" data-path="rainfall-prediction-with-linear-regression.html"><a href="rainfall-prediction-with-linear-regression.html#training-data"><i class="fa fa-check"></i><b>11.1</b> Training data</a></li>
<li class="chapter" data-level="11.2" data-path="rainfall-prediction-with-linear-regression.html"><a href="rainfall-prediction-with-linear-regression.html#convert-arrays-to-tensors-1"><i class="fa fa-check"></i><b>11.2</b> Convert arrays to tensors</a></li>
<li class="chapter" data-level="11.3" data-path="rainfall-prediction-with-linear-regression.html"><a href="rainfall-prediction-with-linear-regression.html#build-the-model"><i class="fa fa-check"></i><b>11.3</b> Build the model</a></li>
<li class="chapter" data-level="11.4" data-path="rainfall-prediction-with-linear-regression.html"><a href="rainfall-prediction-with-linear-regression.html#generate-predictions"><i class="fa fa-check"></i><b>11.4</b> Generate predictions</a></li>
<li class="chapter" data-level="11.5" data-path="rainfall-prediction-with-linear-regression.html"><a href="rainfall-prediction-with-linear-regression.html#loss-function"><i class="fa fa-check"></i><b>11.5</b> Loss Function</a></li>
<li class="chapter" data-level="11.6" data-path="rainfall-prediction-with-linear-regression.html"><a href="rainfall-prediction-with-linear-regression.html#step-by-step-process"><i class="fa fa-check"></i><b>11.6</b> Step by step process</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="rainfall-prediction-with-linear-regression.html"><a href="rainfall-prediction-with-linear-regression.html#compute-the-losses"><i class="fa fa-check"></i><b>11.6.1</b> Compute the losses</a></li>
<li class="chapter" data-level="11.6.2" data-path="rainfall-prediction-with-linear-regression.html"><a href="rainfall-prediction-with-linear-regression.html#compute-gradients"><i class="fa fa-check"></i><b>11.6.2</b> Compute Gradients</a></li>
<li class="chapter" data-level="11.6.3" data-path="rainfall-prediction-with-linear-regression.html"><a href="rainfall-prediction-with-linear-regression.html#reset-the-gradients"><i class="fa fa-check"></i><b>11.6.3</b> Reset the gradients</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="rainfall-prediction-with-linear-regression.html"><a href="rainfall-prediction-with-linear-regression.html#all-together-train-for-multiple-epochs"><i class="fa fa-check"></i><b>11.7</b> All together: train for multiple epochs</a></li>
</ul></li>
<li class="part"><span><b>V Neural Networks</b></span></li>
<li class="chapter" data-level="12" data-path="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html"><i class="fa fa-check"></i><b>12</b> Neural Networks using NumPy, r-base, rTorch and PyTorch</a>
<ul>
<li class="chapter" data-level="12.1" data-path="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#a-neural-network-with-numpy"><i class="fa fa-check"></i><b>12.1</b> A neural network with <code>numpy</code></a></li>
<li class="chapter" data-level="12.2" data-path="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#a-neural-network-with-r-base"><i class="fa fa-check"></i><b>12.2</b> A neural network with <code>r-base</code></a></li>
<li class="chapter" data-level="12.3" data-path="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#the-neural-network-written-in-pytorch"><i class="fa fa-check"></i><b>12.3</b> The neural network written in <code>PyTorch</code></a></li>
<li class="chapter" data-level="12.4" data-path="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#a-neural-network-written-in-rtorch"><i class="fa fa-check"></i><b>12.4</b> A neural network written in <code>rTorch</code></a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#load-the-libraries"><i class="fa fa-check"></i><b>12.4.1</b> Load the libraries</a></li>
<li class="chapter" data-level="12.4.2" data-path="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#dataset"><i class="fa fa-check"></i><b>12.4.2</b> Dataset</a></li>
<li class="chapter" data-level="12.4.3" data-path="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#initialize-the-weights"><i class="fa fa-check"></i><b>12.4.3</b> Initialize the weights</a></li>
<li class="chapter" data-level="12.4.4" data-path="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#iterate-through-the-dataset"><i class="fa fa-check"></i><b>12.4.4</b> Iterate through the dataset</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#complete-code-for-neural-network-in-rtorch"><i class="fa fa-check"></i><b>12.5</b> Complete code for neural network in rTorch</a></li>
<li class="chapter" data-level="12.6" data-path="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#exercise-2"><i class="fa fa-check"></i><b>12.6</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="a-step-by-step-neural-network-in-rtorch.html"><a href="a-step-by-step-neural-network-in-rtorch.html"><i class="fa fa-check"></i><b>13</b> A step-by-step neural network in rTorch</a>
<ul>
<li class="chapter" data-level="13.1" data-path="a-step-by-step-neural-network-in-rtorch.html"><a href="a-step-by-step-neural-network-in-rtorch.html#introduction-1"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="a-step-by-step-neural-network-in-rtorch.html"><a href="a-step-by-step-neural-network-in-rtorch.html#select-device"><i class="fa fa-check"></i><b>13.2</b> Select device</a></li>
<li class="chapter" data-level="13.3" data-path="a-step-by-step-neural-network-in-rtorch.html"><a href="a-step-by-step-neural-network-in-rtorch.html#create-the-dataset"><i class="fa fa-check"></i><b>13.3</b> Create the dataset</a></li>
<li class="chapter" data-level="13.4" data-path="a-step-by-step-neural-network-in-rtorch.html"><a href="a-step-by-step-neural-network-in-rtorch.html#define-the-model-1"><i class="fa fa-check"></i><b>13.4</b> Define the model</a></li>
<li class="chapter" data-level="13.5" data-path="a-step-by-step-neural-network-in-rtorch.html"><a href="a-step-by-step-neural-network-in-rtorch.html#the-loss-function"><i class="fa fa-check"></i><b>13.5</b> The Loss function</a></li>
<li class="chapter" data-level="13.6" data-path="a-step-by-step-neural-network-in-rtorch.html"><a href="a-step-by-step-neural-network-in-rtorch.html#iterate-through-the-dataset-1"><i class="fa fa-check"></i><b>13.6</b> Iterate through the dataset</a></li>
<li class="chapter" data-level="13.7" data-path="a-step-by-step-neural-network-in-rtorch.html"><a href="a-step-by-step-neural-network-in-rtorch.html#using-r-generics-to-simplify-tensor-operations"><i class="fa fa-check"></i><b>13.7</b> Using R generics to simplify tensor operations</a></li>
<li class="chapter" data-level="13.8" data-path="a-step-by-step-neural-network-in-rtorch.html"><a href="a-step-by-step-neural-network-in-rtorch.html#a-more-elegant-way-of-writing-the-neural-network"><i class="fa fa-check"></i><b>13.8</b> A more elegant way of writing the neural network</a></li>
<li class="chapter" data-level="13.9" data-path="a-step-by-step-neural-network-in-rtorch.html"><a href="a-step-by-step-neural-network-in-rtorch.html#create-a-browseable-dataframe"><i class="fa fa-check"></i><b>13.9</b> Create a browseable dataframe</a></li>
<li class="chapter" data-level="13.10" data-path="a-step-by-step-neural-network-in-rtorch.html"><a href="a-step-by-step-neural-network-in-rtorch.html#plot-the-loss-at-each-iteration"><i class="fa fa-check"></i><b>13.10</b> Plot the loss at each iteration</a></li>
</ul></li>
<li class="part"><span><b>VI PyTorch and R data structures</b></span></li>
<li class="chapter" data-level="14" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html"><i class="fa fa-check"></i><b>14</b> Working with data.frame</a>
<ul>
<li class="chapter" data-level="14.1" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html#load-pytorch-libraries"><i class="fa fa-check"></i><b>14.1</b> Load PyTorch libraries</a></li>
<li class="chapter" data-level="14.2" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html#load-dataset"><i class="fa fa-check"></i><b>14.2</b> Load dataset</a></li>
<li class="chapter" data-level="14.3" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html#summary-statistics-for-tensors"><i class="fa fa-check"></i><b>14.3</b> Summary statistics for tensors</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html#using-data.frame"><i class="fa fa-check"></i><b>14.3.1</b> using <code>data.frame</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="working-with-data-table.html"><a href="working-with-data-table.html"><i class="fa fa-check"></i><b>15</b> Working with data.table</a>
<ul>
<li class="chapter" data-level="15.1" data-path="working-with-data-table.html"><a href="working-with-data-table.html#load-pytorch-libraries-1"><i class="fa fa-check"></i><b>15.1</b> Load PyTorch libraries</a></li>
<li class="chapter" data-level="15.2" data-path="working-with-data-table.html"><a href="working-with-data-table.html#load-dataset-1"><i class="fa fa-check"></i><b>15.2</b> Load dataset</a></li>
<li class="chapter" data-level="15.3" data-path="working-with-data-table.html"><a href="working-with-data-table.html#read-the-datasets-without-normalization"><i class="fa fa-check"></i><b>15.3</b> Read the datasets without normalization</a></li>
<li class="chapter" data-level="15.4" data-path="working-with-data-table.html"><a href="working-with-data-table.html#using-data.table"><i class="fa fa-check"></i><b>15.4</b> Using <code>data.table</code></a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendixA.html"><a href="appendixA.html"><i class="fa fa-check"></i><b>A</b> Statistical Background</a>
<ul>
<li class="chapter" data-level="A.1" data-path="appendixA.html"><a href="appendixA.html#basic-statistical-terms"><i class="fa fa-check"></i><b>A.1</b> Basic statistical terms</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="appendixA.html"><a href="appendixA.html#five-number-summary"><i class="fa fa-check"></i><b>A.1.1</b> Five-number summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="appendixB.html"><a href="appendixB.html"><i class="fa fa-check"></i><b>B</b> Activation Functions</a>
<ul>
<li class="chapter" data-level="B.1" data-path="appendixB.html"><a href="appendixB.html#the-sigmoid-function"><i class="fa fa-check"></i><b>B.1</b> The Sigmoid function</a></li>
<li class="chapter" data-level="B.2" data-path="appendixB.html"><a href="appendixB.html#the-relu-function"><i class="fa fa-check"></i><b>B.2</b> The ReLU function</a></li>
<li class="chapter" data-level="B.3" data-path="appendixB.html"><a href="appendixB.html#the-tanh-function"><i class="fa fa-check"></i><b>B.3</b> The tanh function</a></li>
<li class="chapter" data-level="B.4" data-path="appendixB.html"><a href="appendixB.html#the-softmax-activation-function"><i class="fa fa-check"></i><b>B.4</b> The Softmax Activation function</a></li>
<li class="chapter" data-level="B.5" data-path="appendixB.html"><a href="appendixB.html#coding-your-own-activation-functions-in-python"><i class="fa fa-check"></i><b>B.5</b> Coding your own activation functions in Python</a>
<ul>
<li class="chapter" data-level="" data-path="appendixB.html"><a href="appendixB.html#linear-activation"><i class="fa fa-check"></i>Linear activation</a></li>
<li class="chapter" data-level="" data-path="appendixB.html"><a href="appendixB.html#sigmoid-activation"><i class="fa fa-check"></i>Sigmoid activation</a></li>
<li class="chapter" data-level="" data-path="appendixB.html"><a href="appendixB.html#hyperbolic-tangent-activation"><i class="fa fa-check"></i>Hyperbolic Tangent activation</a></li>
<li class="chapter" data-level="" data-path="appendixB.html"><a href="appendixB.html#rectifier-linear-unit-relu"><i class="fa fa-check"></i>Rectifier linear unit (ReLU)</a></li>
<li><a href="appendixB.html#visualization-with-matplotlib">Visualization with <code>matplotlib</code></a></li>
</ul></li>
<li class="chapter" data-level="B.6" data-path="appendixB.html"><a href="appendixB.html#softmax-in-python"><i class="fa fa-check"></i><b>B.6</b> Softmax in Python</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Minimal rTorch Book</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="neural-networks-using-numpy-r-base-rtorch-and-pytorch" class="section level1" number="12">
<h1><span class="header-section-number">Chapter 12</span> Neural Networks using NumPy, r-base, rTorch and PyTorch</h1>
<p>We will compare three neural networks:</p>
<ul>
<li><p>a neural network written in <code>numpy</code></p></li>
<li><p>a neural network written in <code>r-base</code></p></li>
<li><p>a neural network written in <code>PyTorch</code></p></li>
<li><p>a neural network written in <code>rTorch</code></p></li>
</ul>
<div id="a-neural-network-with-numpy" class="section level2" number="12.1">
<h2><span class="header-section-number">12.1</span> A neural network with <code>numpy</code></h2>
<p>We start the neural network by simply using <code>numpy</code>:</p>
<div class="sourceCode" id="cb512"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb512-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-1" aria-hidden="true" tabindex="-1"></a><span class="co"># A simple neural network using NumPy</span></span>
<span id="cb512-2"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Code in file tensor/two_layer_net_numpy.py</span></span>
<span id="cb512-3"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb512-4"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb512-5"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb512-6"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-6" aria-hidden="true" tabindex="-1"></a>tic <span class="op">=</span> time.process_time()</span>
<span id="cb512-7"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb512-8"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-8" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">123</span>)   <span class="co"># set a seed for reproducibility</span></span>
<span id="cb512-9"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-9" aria-hidden="true" tabindex="-1"></a><span class="co"># N is batch size; D_in is input dimension;</span></span>
<span id="cb512-10"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-10" aria-hidden="true" tabindex="-1"></a><span class="co"># H is hidden dimension; D_out is output dimension.</span></span>
<span id="cb512-11"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-11" aria-hidden="true" tabindex="-1"></a>N, D_in, H, D_out <span class="op">=</span> <span class="dv">64</span>, <span class="dv">1000</span>, <span class="dv">100</span>, <span class="dv">10</span></span>
<span id="cb512-12"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb512-13"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Create random input and output data</span></span>
<span id="cb512-14"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-14" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.randn(N, D_in)</span>
<span id="cb512-15"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-15" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.random.randn(N, D_out)</span>
<span id="cb512-16"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-16" aria-hidden="true" tabindex="-1"></a><span class="co"># print(x.shape)</span></span>
<span id="cb512-17"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-17" aria-hidden="true" tabindex="-1"></a><span class="co"># print(y.shape)</span></span>
<span id="cb512-18"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb512-19"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-19" aria-hidden="true" tabindex="-1"></a>w1 <span class="op">=</span> np.random.randn(D_in, H)</span>
<span id="cb512-20"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-20" aria-hidden="true" tabindex="-1"></a>w2 <span class="op">=</span> np.random.randn(H, D_out)</span>
<span id="cb512-21"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-21" aria-hidden="true" tabindex="-1"></a><span class="co"># print(w1.shape)</span></span>
<span id="cb512-22"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-22" aria-hidden="true" tabindex="-1"></a><span class="co"># print(w2.shape)</span></span>
<span id="cb512-23"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb512-24"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-24" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">1e-6</span></span>
<span id="cb512-25"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-25" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">500</span>):</span>
<span id="cb512-26"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-26" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Forward pass: compute predicted y</span></span>
<span id="cb512-27"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-27" aria-hidden="true" tabindex="-1"></a>  h <span class="op">=</span> x.dot(w1)</span>
<span id="cb512-28"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-28" aria-hidden="true" tabindex="-1"></a>  <span class="co"># print(t, h.max())</span></span>
<span id="cb512-29"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-29" aria-hidden="true" tabindex="-1"></a>  h_relu <span class="op">=</span> np.maximum(h, <span class="dv">0</span>)</span>
<span id="cb512-30"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-30" aria-hidden="true" tabindex="-1"></a>  y_pred <span class="op">=</span> h_relu.dot(w2)</span>
<span id="cb512-31"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-31" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb512-32"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-32" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compute and print loss</span></span>
<span id="cb512-33"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-33" aria-hidden="true" tabindex="-1"></a>  sq <span class="op">=</span> np.square(y_pred <span class="op">-</span> y)</span>
<span id="cb512-34"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-34" aria-hidden="true" tabindex="-1"></a>  loss <span class="op">=</span> sq.<span class="bu">sum</span>()</span>
<span id="cb512-35"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-35" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(t, loss)</span>
<span id="cb512-36"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-36" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb512-37"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-37" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span>
<span id="cb512-38"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-38" aria-hidden="true" tabindex="-1"></a>  grad_y_pred <span class="op">=</span> <span class="fl">2.0</span> <span class="op">*</span> (y_pred <span class="op">-</span> y)</span>
<span id="cb512-39"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-39" aria-hidden="true" tabindex="-1"></a>  grad_w2 <span class="op">=</span> h_relu.T.dot(grad_y_pred)</span>
<span id="cb512-40"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-40" aria-hidden="true" tabindex="-1"></a>  grad_h_relu <span class="op">=</span> grad_y_pred.dot(w2.T)</span>
<span id="cb512-41"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-41" aria-hidden="true" tabindex="-1"></a>  grad_h <span class="op">=</span> grad_h_relu.copy()</span>
<span id="cb512-42"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-42" aria-hidden="true" tabindex="-1"></a>  grad_h[h <span class="op">&lt;</span> <span class="dv">0</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb512-43"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-43" aria-hidden="true" tabindex="-1"></a>  grad_w1 <span class="op">=</span> x.T.dot(grad_h)</span>
<span id="cb512-44"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-44" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb512-45"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-45" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Update weights</span></span>
<span id="cb512-46"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-46" aria-hidden="true" tabindex="-1"></a>  w1 <span class="op">-=</span> learning_rate <span class="op">*</span> grad_w1</span>
<span id="cb512-47"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-47" aria-hidden="true" tabindex="-1"></a>  w2 <span class="op">-=</span> learning_rate <span class="op">*</span> grad_w2</span>
<span id="cb512-48"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb512-48" aria-hidden="true" tabindex="-1"></a><span class="co"># processing time  </span></span></code></pre></div>
<pre><code>#&gt; 0 28624200.800938517
#&gt; 1 24402861.381040636
#&gt; 2 23157437.29147552
#&gt; 3 21617191.63397175
#&gt; 4 18598190.361558598
#&gt; 5 14198211.419692844
#&gt; 6 9786244.45261814
#&gt; 7 6233451.217340663
#&gt; 8 3862647.267829599
#&gt; 9 2412366.632764836
#&gt; 10 1569915.4392193707
#&gt; 11 1078501.3381487518
#&gt; 12 785163.9233288621
#&gt; 13 601495.2825043725
#&gt; 14 479906.0403613456
#&gt; 15 394555.19331746205
#&gt; 16 331438.6987273826
#&gt; 17 282679.6687873873
#&gt; 18 243807.84432087594
#&gt; 19 211970.18110708205
#&gt; 20 185451.6861514274
#&gt; 21 163078.20881862927
#&gt; 22 144011.80160918707
#&gt; 23 127662.96132466741
#&gt; 24 113546.29175681781
#&gt; 25 101291.55288493488
#&gt; 26 90623.20833654879
#&gt; 27 81307.32590692889
#&gt; 28 73135.24710426925
#&gt; 29 65937.50294095621
#&gt; 30 59570.26425368039
#&gt; 31 53923.82804264227
#&gt; 32 48909.69273028215
#&gt; 33 44438.89933807681
#&gt; 34 40445.34031569733
#&gt; 35 36873.30041989413
#&gt; 36 33664.990437423825
#&gt; 37 30781.198962949587
#&gt; 38 28184.24227268406
#&gt; 39 25843.99793108194
#&gt; 40 23727.282448406426
#&gt; 41 21810.062067327668
#&gt; 42 20071.326437572196
#&gt; 43 18492.63752543329
#&gt; 44 17056.72779714255
#&gt; 45 15749.299484025236
#&gt; 46 14557.324481207237
#&gt; 47 13468.469764338035
#&gt; 48 12473.575866914027
#&gt; 49 11562.485809665774
#&gt; 50 10727.865926563407
#&gt; 51 9962.411372816146
#&gt; 52 9259.619803682268
#&gt; 53 8613.269071227103
#&gt; 54 8018.523834750763
#&gt; 55 7471.080819104451
#&gt; 56 6966.00651845651
#&gt; 57 6499.96685422581
#&gt; 58 6069.576425345411
#&gt; 59 5671.2821228408475
#&gt; 60 5302.644980086279
#&gt; 61 4961.339043761728
#&gt; 62 4645.02541423451
#&gt; 63 4351.473575805103
#&gt; 64 4079.2165446062972
#&gt; 65 3826.1480820887655
#&gt; 66 3590.887308956795
#&gt; 67 3372.0103280622666
#&gt; 68 3168.173408650748
#&gt; 69 2978.362100081684
#&gt; 70 2801.302649097963
#&gt; 71 2636.037950790892
#&gt; 72 2481.7354010452655
#&gt; 73 2337.6093944873246
#&gt; 74 2202.8250425683987
#&gt; 75 2076.8872560589616
#&gt; 76 1958.9976460120263
#&gt; 77 1848.5060338548483
#&gt; 78 1744.9993380824799
#&gt; 79 1647.9807349258715
#&gt; 80 1556.9947585282196
#&gt; 81 1471.7081797400347
#&gt; 82 1391.6136870762566
#&gt; 83 1316.3329239757227
#&gt; 84 1245.5902641069824
#&gt; 85 1179.0691783286234
#&gt; 86 1116.5095209528572
#&gt; 87 1057.6662051951396
#&gt; 88 1002.2519686823666
#&gt; 89 950.0167505993219
#&gt; 90 900.7916929993518
#&gt; 91 854.3816389576979
#&gt; 92 810.6277767708903
#&gt; 93 769.3592041348505
#&gt; 94 730.3836012940042
#&gt; 95 693.5644048073411
#&gt; 96 658.7807027999521
#&gt; 97 625.9238747325827
#&gt; 98 594.8758111695068
#&gt; 99 565.4973547949257
#&gt; 100 537.7012178149556
#&gt; 101 511.3901106843991
#&gt; 102 486.4837276215478
#&gt; 103 462.90746955458474
#&gt; 104 440.5787622887435
#&gt; 105 419.4121231392399
#&gt; 106 399.34612374957226
#&gt; 107 380.3221777272873
#&gt; 108 362.2821345456067
#&gt; 109 345.18049757120184
#&gt; 110 328.94028615976936
#&gt; 111 313.5191206271147
#&gt; 112 298.8754770672758
#&gt; 113 284.96926791620496
#&gt; 114 271.7642984526849
#&gt; 115 259.2246266311472
#&gt; 116 247.30122156531897
#&gt; 117 235.96203976771662
#&gt; 118 225.17874184522793
#&gt; 119 214.9253969806085
#&gt; 120 205.16916168826197
#&gt; 121 195.88920014324063
#&gt; 122 187.0522150132689
#&gt; 123 178.6428873875804
#&gt; 124 170.63479897325027
#&gt; 125 163.00806018890546
#&gt; 126 155.7440191346056
#&gt; 127 148.83352898111042
#&gt; 128 142.2496666996878
#&gt; 129 135.97509122834504
#&gt; 130 129.98982612428355
#&gt; 131 124.28418865778005
#&gt; 132 118.84482149781273
#&gt; 133 113.65645952102406
#&gt; 134 108.7054397008061
#&gt; 135 103.98144604072209
#&gt; 136 99.47512083365962
#&gt; 137 95.17318303450762
#&gt; 138 91.06775169947714
#&gt; 139 87.14952592945869
#&gt; 140 83.4075554849774
#&gt; 141 79.8333553283839
#&gt; 142 76.41993249926654
#&gt; 143 73.159531678603
#&gt; 144 70.04535899921396
#&gt; 145 67.0700037713867
#&gt; 146 64.22536514818646
#&gt; 147 61.50715956099643
#&gt; 148 58.90970110703718
#&gt; 149 56.42818157298958
#&gt; 150 54.053456343974474
#&gt; 151 51.78409899250521
#&gt; 152 49.613042222061935
#&gt; 153 47.537088681832714
#&gt; 154 45.55073951374691
#&gt; 155 43.651385230775375
#&gt; 156 41.8333828820336
#&gt; 157 40.0944925576898
#&gt; 158 38.4304655768987
#&gt; 159 36.83773398481151
#&gt; 160 35.313368600585044
#&gt; 161 33.85436928433868
#&gt; 162 32.457997092726586
#&gt; 163 31.120973836567913
#&gt; 164 29.841057186484246
#&gt; 165 28.61536631365921
#&gt; 166 27.441646501921213
#&gt; 167 26.31767712811449
#&gt; 168 25.241065734351473
#&gt; 169 24.210568668753154
#&gt; 170 23.223366825888164
#&gt; 171 22.27691447596546
#&gt; 172 21.370561777029383
#&gt; 173 20.502013041055037
#&gt; 174 19.669605151002397
#&gt; 175 18.872156637147214
#&gt; 176 18.107932697664136
#&gt; 177 17.375347093063624
#&gt; 178 16.67329705241241
#&gt; 179 16.000313127916616
#&gt; 180 15.355056259809643
#&gt; 181 14.736642044314163
#&gt; 182 14.143657665391123
#&gt; 183 13.575482981169435
#&gt; 184 13.03055792072713
#&gt; 185 12.507813624903267
#&gt; 186 12.00650847964371
#&gt; 187 11.525873890625666
#&gt; 188 11.064924569594556
#&gt; 189 10.622845128602144
#&gt; 190 10.199224278747348
#&gt; 191 9.79248532294249
#&gt; 192 9.40221537769526
#&gt; 193 9.027996925837858
#&gt; 194 8.668895520243254
#&gt; 195 8.324385761675554
#&gt; 196 7.99390867066041
#&gt; 197 7.676665609325665
#&gt; 198 7.3722991001285685
#&gt; 199 7.080233920966563
#&gt; 200 6.7999405980009
#&gt; 201 6.530984430178585
#&gt; 202 6.2728878687947365
#&gt; 203 6.025197539285438
#&gt; 204 5.787473375780924
#&gt; 205 5.559253501791474
#&gt; 206 5.340172472449113
#&gt; 207 5.129896948041436
#&gt; 208 4.928007606815918
#&gt; 209 4.734225282679221
#&gt; 210 4.548186858907342
#&gt; 211 4.369651328446663
#&gt; 212 4.198236457646962
#&gt; 213 4.033565011138579
#&gt; 214 3.8754625080281464
#&gt; 215 3.7236914115521316
#&gt; 216 3.5779627242857224
#&gt; 217 3.4379821914239286
#&gt; 218 3.303565587540205
#&gt; 219 3.174454405800678
#&gt; 220 3.0504743070396323
#&gt; 221 2.931383709316906
#&gt; 222 2.8170418304762785
#&gt; 223 2.7072412196038553
#&gt; 224 2.6017277000868093
#&gt; 225 2.50040409121904
#&gt; 226 2.403078781570677
#&gt; 227 2.309594481835507
#&gt; 228 2.219794799730801
#&gt; 229 2.133526678637347
#&gt; 230 2.0506760423604566
#&gt; 231 1.9710453639295484
#&gt; 232 1.894559024310974
#&gt; 233 1.8211210547720629
#&gt; 234 1.7505340383436803
#&gt; 235 1.6826932948721067
#&gt; 236 1.6175070289508109
#&gt; 237 1.5549072300348752
#&gt; 238 1.4947316986695944
#&gt; 239 1.436912502600996
#&gt; 240 1.381372987946563
#&gt; 241 1.3279854205041584
#&gt; 242 1.2766884038688984
#&gt; 243 1.2273848146334094
#&gt; 244 1.1800217450316255
#&gt; 245 1.1344919105891025
#&gt; 246 1.0907369940975837
#&gt; 247 1.0486826235693274
#&gt; 248 1.0082656206399931
#&gt; 249 0.9694282665755529
#&gt; 250 0.9320976601575675
#&gt; 251 0.8962339607475229
#&gt; 252 0.8617533865905884
#&gt; 253 0.8286151485833971
#&gt; 254 0.7967578289852474
#&gt; 255 0.7661404678425654
#&gt; 256 0.7367202044072118
#&gt; 257 0.708422713667491
#&gt; 258 0.6812311487720265
#&gt; 259 0.6550822696783506
#&gt; 260 0.6299469090210432
#&gt; 261 0.605786995355434
#&gt; 262 0.5825650778276774
#&gt; 263 0.5602382140936045
#&gt; 264 0.5387735503110371
#&gt; 265 0.5181403816556053
#&gt; 266 0.49830590931295304
#&gt; 267 0.47922937308117297
#&gt; 268 0.46088901492620127
#&gt; 269 0.44325464817119054
#&gt; 270 0.42630408406116316
#&gt; 271 0.41000543380657917
#&gt; 272 0.39433673295843236
#&gt; 273 0.37927114581493265
#&gt; 274 0.36478176529460243
#&gt; 275 0.35085044445134994
#&gt; 276 0.3374578361158044
#&gt; 277 0.32457682402453136
#&gt; 278 0.31219123729919207
#&gt; 279 0.300296586147234
#&gt; 280 0.28884848624094894
#&gt; 281 0.27783526470539743
#&gt; 282 0.26724487697010957
#&gt; 283 0.2570618106928273
#&gt; 284 0.2472693951468085
#&gt; 285 0.23785306876436113
#&gt; 286 0.22879648231270536
#&gt; 287 0.22008909643106767
#&gt; 288 0.21171318526106842
#&gt; 289 0.2036578219834066
#&gt; 290 0.19591133993811427
#&gt; 291 0.18846041746510728
#&gt; 292 0.18129477007162065
#&gt; 293 0.174405315161736
#&gt; 294 0.16777998120837712
#&gt; 295 0.16140610523836268
#&gt; 296 0.1552756501716649
#&gt; 297 0.14937904644542377
#&gt; 298 0.14370793039467633
#&gt; 299 0.13825290527822973
#&gt; 300 0.13300640130439656
#&gt; 301 0.12796012311324031
#&gt; 302 0.12310750541656884
#&gt; 303 0.11844182274749851
#&gt; 304 0.11395158652041627
#&gt; 305 0.10963187686672912
#&gt; 306 0.10547640155933785
#&gt; 307 0.10148022089409026
#&gt; 308 0.0976363799328684
#&gt; 309 0.09393976586801374
#&gt; 310 0.09038186218007657
#&gt; 311 0.08696004033318867
#&gt; 312 0.08366808215670352
#&gt; 313 0.08050159133387036
#&gt; 314 0.0774556507265311
#&gt; 315 0.07452541616811464
#&gt; 316 0.07170677388789805
#&gt; 317 0.06899492388917926
#&gt; 318 0.06638632065320674
#&gt; 319 0.06387707772657374
#&gt; 320 0.06146291085125196
#&gt; 321 0.0591402294396231
#&gt; 322 0.05690662209831464
#&gt; 323 0.05475707395743591
#&gt; 324 0.05268944906989688
#&gt; 325 0.05069984545069233
#&gt; 326 0.048785688597973095
#&gt; 327 0.046944795197577285
#&gt; 328 0.045173966618895535
#&gt; 329 0.043469382749897256
#&gt; 330 0.04182932192085659
#&gt; 331 0.04025154186795582
#&gt; 332 0.038733588417595735
#&gt; 333 0.03727299017402862
#&gt; 334 0.03586799441058297
#&gt; 335 0.03451589218265247
#&gt; 336 0.03321501089199479
#&gt; 337 0.03196371785309425
#&gt; 338 0.030759357425241718
#&gt; 339 0.029600888472444742
#&gt; 340 0.028485919148238392
#&gt; 341 0.02741317225069457
#&gt; 342 0.026380963792005673
#&gt; 343 0.025387828276963217
#&gt; 344 0.02443225636975702
#&gt; 345 0.02351279471955997
#&gt; 346 0.02262815392798661
#&gt; 347 0.02177684408442846
#&gt; 348 0.02095765200803268
#&gt; 349 0.02016947466161515
#&gt; 350 0.019410962895712616
#&gt; 351 0.018681045066734122
#&gt; 352 0.017978879513468316
#&gt; 353 0.017303468563130222
#&gt; 354 0.016653437842251186
#&gt; 355 0.01602766278432409
#&gt; 356 0.015425464893044428
#&gt; 357 0.01484594678906112
#&gt; 358 0.014288249850265784
#&gt; 359 0.01375163575426638
#&gt; 360 0.01323528665049373
#&gt; 361 0.012738339025978556
#&gt; 362 0.012260186918304262
#&gt; 363 0.011799970856220952
#&gt; 364 0.011357085981162363
#&gt; 365 0.010930950268775873
#&gt; 366 0.010520842685022909
#&gt; 367 0.010126145830079638
#&gt; 368 0.009746393154855839
#&gt; 369 0.009380889339520658
#&gt; 370 0.009029161386689313
#&gt; 371 0.00869059833698051
#&gt; 372 0.00836477207696539
#&gt; 373 0.008051209390678065
#&gt; 374 0.0077494325069793705
#&gt; 375 0.007459023266150334
#&gt; 376 0.007179590434333104
#&gt; 377 0.006910623445853765
#&gt; 378 0.006651749941578513
#&gt; 379 0.006402648026678379
#&gt; 380 0.006162978285307884
#&gt; 381 0.005932194796367616
#&gt; 382 0.005710085052295781
#&gt; 383 0.005496310244895275
#&gt; 384 0.0052906289241425215
#&gt; 385 0.0050926241688279104
#&gt; 386 0.004902076613033862
#&gt; 387 0.004718638851167859
#&gt; 388 0.004542078962047164
#&gt; 389 0.004372164586665975
#&gt; 390 0.004208618626839021
#&gt; 391 0.004051226677923414
#&gt; 392 0.0038997374494828298
#&gt; 393 0.003753918301513866
#&gt; 394 0.003613561837935153
#&gt; 395 0.0034784786917529164
#&gt; 396 0.003348462575629662
#&gt; 397 0.003223327362263324
#&gt; 398 0.0031028635490837437
#&gt; 399 0.002986912218213565
#&gt; 400 0.002875348146367024
#&gt; 401 0.0027679524720207994
#&gt; 402 0.0026645903412969877
#&gt; 403 0.00256506728009952
#&gt; 404 0.0024692701898842025
#&gt; 405 0.0023770671718814063
#&gt; 406 0.0022883091777422303
#&gt; 407 0.0022029269889801703
#&gt; 408 0.0021207379368966914
#&gt; 409 0.0020415781423120893
#&gt; 410 0.001965380838191689
#&gt; 411 0.0018920388674650765
#&gt; 412 0.0018214489876606395
#&gt; 413 0.0017534990549357195
#&gt; 414 0.0016880979054376358
#&gt; 415 0.0016251364192863505
#&gt; 416 0.0015645343026947606
#&gt; 417 0.0015062064772070694
#&gt; 418 0.0014500530088225327
#&gt; 419 0.0013959868097274688
#&gt; 420 0.001343946421404061
#&gt; 421 0.0012938496041169677
#&gt; 422 0.001245622397754905
#&gt; 423 0.0011992050880615885
#&gt; 424 0.0011545283489900085
#&gt; 425 0.0011115075856686302
#&gt; 426 0.001070100670544413
#&gt; 427 0.0010302364937566674
#&gt; 428 0.0009918591300819473
#&gt; 429 0.000954924393232083
#&gt; 430 0.0009193639132775486
#&gt; 431 0.0008851308467932729
#&gt; 432 0.0008521777959560448
#&gt; 433 0.0008204570911784497
#&gt; 434 0.0007899223397731109
#&gt; 435 0.0007605278374214596
#&gt; 436 0.0007322343466954752
#&gt; 437 0.0007049830914115257
#&gt; 438 0.0006787512341473519
#&gt; 439 0.00065350212037464
#&gt; 440 0.0006291921955255096
#&gt; 441 0.0006057856348208776
#&gt; 442 0.0005832525024800561
#&gt; 443 0.0005615598539424442
#&gt; 444 0.0005406761235200468
#&gt; 445 0.0005205750249286578
#&gt; 446 0.0005012184845940066
#&gt; 447 0.0004825848028301716
#&gt; 448 0.0004646447575300741
#&gt; 449 0.0004473739461918762
#&gt; 450 0.0004307513759213604
#&gt; 451 0.00041474810355609723
#&gt; 452 0.00039933580480713945
#&gt; 453 0.0003844970781264902
#&gt; 454 0.0003702109250696993
#&gt; 455 0.00035645948619340297
#&gt; 456 0.0003432213223641764
#&gt; 457 0.0003304723731848576
#&gt; 458 0.00031819830164465815
#&gt; 459 0.00030638121798918724
#&gt; 460 0.0002950045353519474
#&gt; 461 0.0002840533130499193
#&gt; 462 0.00027350873727298176
#&gt; 463 0.00026335657398426546
#&gt; 464 0.000253581258369829
#&gt; 465 0.00024416913722126747
#&gt; 466 0.0002351142689424904
#&gt; 467 0.0002263919313737711
#&gt; 468 0.00021799257674327073
#&gt; 469 0.00020990427540056088
#&gt; 470 0.0002021174506938248
#&gt; 471 0.00019462054044199915
#&gt; 472 0.00018740325426984858
#&gt; 473 0.00018045252249983815
#&gt; 474 0.000173759960543912
#&gt; 475 0.00016731630060690805
#&gt; 476 0.0001611122710715995
#&gt; 477 0.00015513993832625702
#&gt; 478 0.00014938925941558148
#&gt; 479 0.00014385207870578823
#&gt; 480 0.00013852014130375656
#&gt; 481 0.00013338601187671428
#&gt; 482 0.000128442793294424
#&gt; 483 0.0001236841045646944
#&gt; 484 0.00011910150087090696
#&gt; 485 0.00011468967274610794
#&gt; 486 0.00011044058002490428
#&gt; 487 0.00010634983745106246
#&gt; 488 0.00010241132940006558
#&gt; 489 9.861901302344988e-05
#&gt; 490 9.496682985475842e-05
#&gt; 491 9.144989845880715e-05
#&gt; 492 8.806354488018214e-05
#&gt; 493 8.480312707749194e-05
#&gt; 494 8.166404591653792e-05
#&gt; 495 7.864135637113095e-05
#&gt; 496 7.573027443124469e-05
#&gt; 497 7.292787602990206e-05
#&gt; 498 7.023030228370285e-05
#&gt; 499 6.763183953445079e-05</code></pre>
<div class="sourceCode" id="cb514"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb514-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-1" aria-hidden="true" tabindex="-1"></a>toc <span class="op">=</span> time.process_time()</span>
<span id="cb514-2"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(toc <span class="op">-</span> tic, <span class="st">&quot;seconds&quot;</span>)</span></code></pre></div>
<pre><code>#&gt; 9.821577708000007 seconds</code></pre>
</div>
<div id="a-neural-network-with-r-base" class="section level2" number="12.2">
<h2><span class="header-section-number">12.2</span> A neural network with <code>r-base</code></h2>
<p>It is the same algorithm above in <code>numpy</code> but written in R base.</p>
<div class="sourceCode" id="cb516"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb516-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tictoc)</span>
<span id="cb516-2"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb516-3"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-3" aria-hidden="true" tabindex="-1"></a><span class="fu">tic</span>()</span>
<span id="cb516-4"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb516-5"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-5" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">64</span>; D_in <span class="ot">&lt;-</span> <span class="dv">1000</span>; H <span class="ot">&lt;-</span> <span class="dv">100</span>; D_out <span class="ot">&lt;-</span> <span class="dv">10</span>;</span>
<span id="cb516-6"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create random input and output data</span></span>
<span id="cb516-7"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-7" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="fu">rnorm</span>(N <span class="sc">*</span> D_in),  <span class="at">dim =</span> <span class="fu">c</span>(N, D_in))</span>
<span id="cb516-8"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-8" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="fu">rnorm</span>(N <span class="sc">*</span> D_out), <span class="at">dim =</span> <span class="fu">c</span>(N, D_out))</span>
<span id="cb516-9"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Randomly initialize weights</span></span>
<span id="cb516-10"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-10" aria-hidden="true" tabindex="-1"></a>w1 <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="fu">rnorm</span>(D_in <span class="sc">*</span> H),  <span class="at">dim =</span> <span class="fu">c</span>(D_in, H))</span>
<span id="cb516-11"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-11" aria-hidden="true" tabindex="-1"></a>w2 <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="fu">rnorm</span>(H <span class="sc">*</span> D_out),  <span class="at">dim =</span> <span class="fu">c</span>(H, D_out))</span>
<span id="cb516-12"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-12" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="ot">&lt;-</span>  <span class="fl">1e-6</span></span>
<span id="cb516-13"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb516-14"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="fu">seq</span>(<span class="dv">1</span>, <span class="dv">500</span>)) {</span>
<span id="cb516-15"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Forward pass: compute predicted y</span></span>
<span id="cb516-16"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-16" aria-hidden="true" tabindex="-1"></a>  h <span class="ot">=</span> x <span class="sc">%*%</span> w1</span>
<span id="cb516-17"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-17" aria-hidden="true" tabindex="-1"></a>  h_relu <span class="ot">=</span> <span class="fu">pmax</span>(h, <span class="dv">0</span>)</span>
<span id="cb516-18"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-18" aria-hidden="true" tabindex="-1"></a>  y_pred <span class="ot">=</span> h_relu <span class="sc">%*%</span> w2</span>
<span id="cb516-19"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb516-20"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-20" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compute and print loss</span></span>
<span id="cb516-21"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-21" aria-hidden="true" tabindex="-1"></a>  sq <span class="ot">&lt;-</span> (y_pred <span class="sc">-</span> y)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb516-22"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-22" aria-hidden="true" tabindex="-1"></a>  loss <span class="ot">=</span> <span class="fu">sum</span>(sq)</span>
<span id="cb516-23"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(t, loss, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb516-24"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-24" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb516-25"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-25" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span>
<span id="cb516-26"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-26" aria-hidden="true" tabindex="-1"></a>  grad_y_pred <span class="ot">=</span> <span class="fl">2.0</span> <span class="sc">*</span> (y_pred <span class="sc">-</span> y)</span>
<span id="cb516-27"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-27" aria-hidden="true" tabindex="-1"></a>  grad_w2 <span class="ot">=</span> <span class="fu">t</span>(h_relu) <span class="sc">%*%</span> grad_y_pred</span>
<span id="cb516-28"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-28" aria-hidden="true" tabindex="-1"></a>  grad_h_relu <span class="ot">=</span> grad_y_pred <span class="sc">%*%</span> <span class="fu">t</span>(w2)</span>
<span id="cb516-29"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-29" aria-hidden="true" tabindex="-1"></a>  <span class="co"># grad_h &lt;- sapply(grad_h_relu, function(i) i, simplify = FALSE )   # grad_h = grad_h_relu.copy()</span></span>
<span id="cb516-30"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-30" aria-hidden="true" tabindex="-1"></a>  grad_h <span class="ot">&lt;-</span> rlang<span class="sc">::</span><span class="fu">duplicate</span>(grad_h_relu)</span>
<span id="cb516-31"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-31" aria-hidden="true" tabindex="-1"></a>  grad_h[h <span class="sc">&lt;</span> <span class="dv">0</span>] <span class="ot">&lt;-</span>  <span class="dv">0</span></span>
<span id="cb516-32"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-32" aria-hidden="true" tabindex="-1"></a>  grad_w1 <span class="ot">=</span> <span class="fu">t</span>(x) <span class="sc">%*%</span> grad_h</span>
<span id="cb516-33"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-33" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb516-34"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-34" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Update weights</span></span>
<span id="cb516-35"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-35" aria-hidden="true" tabindex="-1"></a>  w1 <span class="ot">=</span> w1 <span class="sc">-</span> learning_rate <span class="sc">*</span> grad_w1</span>
<span id="cb516-36"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-36" aria-hidden="true" tabindex="-1"></a>  w2 <span class="ot">=</span> w2 <span class="sc">-</span> learning_rate <span class="sc">*</span> grad_w2</span>
<span id="cb516-37"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-37" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb516-38"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-38" aria-hidden="true" tabindex="-1"></a><span class="fu">toc</span>()</span></code></pre></div>
<pre><code>#&gt; 1 2.8e+07 
#&gt; 2 25505803 
#&gt; 3 29441299 
#&gt; 4 35797650 
#&gt; 5 39517126 
#&gt; 6 34884942 
#&gt; 7 23333535 
#&gt; 8 11927525 
#&gt; 9 5352787 
#&gt; 10 2496984 
#&gt; 11 1379780 
#&gt; 12 918213 
#&gt; 13 695760 
#&gt; 14 564974 
#&gt; 15 474479 
#&gt; 16 405370 
#&gt; 17 349747 
#&gt; 18 303724 
#&gt; 19 265075 
#&gt; 20 232325 
#&gt; 21 204394 
#&gt; 22 180414 
#&gt; 23 159752 
#&gt; 24 141895 
#&gt; 25 126374 
#&gt; 26 112820 
#&gt; 27 100959 
#&gt; 28 90536 
#&gt; 29 81352 
#&gt; 30 73244 
#&gt; 31 66058 
#&gt; 32 59675 
#&gt; 33 53993 
#&gt; 34 48921 
#&gt; 35 44388 
#&gt; 36 40328 
#&gt; 37 36687 
#&gt; 38 33414 
#&gt; 39 30469 
#&gt; 40 27816 
#&gt; 41 25419 
#&gt; 42 23251 
#&gt; 43 21288 
#&gt; 44 19508 
#&gt; 45 17893 
#&gt; 46 16426 
#&gt; 47 15092 
#&gt; 48 13877 
#&gt; 49 12769 
#&gt; 50 11758 
#&gt; 51 10835 
#&gt; 52 9991 
#&gt; 53 9218 
#&gt; 54 8510 
#&gt; 55 7862 
#&gt; 56 7267 
#&gt; 57 6719 
#&gt; 58 6217 
#&gt; 59 5754 
#&gt; 60 5329 
#&gt; 61 4938 
#&gt; 62 4577 
#&gt; 63 4245 
#&gt; 64 3938 
#&gt; 65 3655 
#&gt; 66 3394 
#&gt; 67 3153 
#&gt; 68 2930 
#&gt; 69 2724 
#&gt; 70 2533 
#&gt; 71 2357 
#&gt; 72 2193 
#&gt; 73 2042 
#&gt; 74 1902 
#&gt; 75 1772 
#&gt; 76 1651 
#&gt; 77 1539 
#&gt; 78 1435 
#&gt; 79 1338 
#&gt; 80 1249 
#&gt; 81 1165 
#&gt; 82 1088 
#&gt; 83 1016 
#&gt; 84 949 
#&gt; 85 886 
#&gt; 86 828 
#&gt; 87 774 
#&gt; 88 724 
#&gt; 89 677 
#&gt; 90 633 
#&gt; 91 592 
#&gt; 92 554 
#&gt; 93 519 
#&gt; 94 486 
#&gt; 95 455 
#&gt; 96 426 
#&gt; 97 399 
#&gt; 98 374 
#&gt; 99 350 
#&gt; 100 328 
#&gt; 101 308 
#&gt; 102 289 
#&gt; 103 271 
#&gt; 104 254 
#&gt; 105 238 
#&gt; 106 224 
#&gt; 107 210 
#&gt; 108 197 
#&gt; 109 185 
#&gt; 110 174 
#&gt; 111 163 
#&gt; 112 153 
#&gt; 113 144 
#&gt; 114 135 
#&gt; 115 127 
#&gt; 116 119 
#&gt; 117 112 
#&gt; 118 106 
#&gt; 119 99.2 
#&gt; 120 93.3 
#&gt; 121 87.8 
#&gt; 122 82.6 
#&gt; 123 77.7 
#&gt; 124 73.1 
#&gt; 125 68.8 
#&gt; 126 64.7 
#&gt; 127 60.9 
#&gt; 128 57.4 
#&gt; 129 54 
#&gt; 130 50.9 
#&gt; 131 47.9 
#&gt; 132 45.1 
#&gt; 133 42.5 
#&gt; 134 40.1 
#&gt; 135 37.8 
#&gt; 136 35.6 
#&gt; 137 33.5 
#&gt; 138 31.6 
#&gt; 139 29.8 
#&gt; 140 28.1 
#&gt; 141 26.5 
#&gt; 142 25 
#&gt; 143 23.6 
#&gt; 144 22.2 
#&gt; 145 21 
#&gt; 146 19.8 
#&gt; 147 18.7 
#&gt; 148 17.6 
#&gt; 149 16.6 
#&gt; 150 15.7 
#&gt; 151 14.8 
#&gt; 152 14 
#&gt; 153 13.2 
#&gt; 154 12.5 
#&gt; 155 11.8 
#&gt; 156 11.1 
#&gt; 157 10.5 
#&gt; 158 9.94 
#&gt; 159 9.39 
#&gt; 160 8.87 
#&gt; 161 8.38 
#&gt; 162 7.92 
#&gt; 163 7.49 
#&gt; 164 7.08 
#&gt; 165 6.69 
#&gt; 166 6.32 
#&gt; 167 5.98 
#&gt; 168 5.65 
#&gt; 169 5.35 
#&gt; 170 5.06 
#&gt; 171 4.78 
#&gt; 172 4.52 
#&gt; 173 4.28 
#&gt; 174 4.05 
#&gt; 175 3.83 
#&gt; 176 3.62 
#&gt; 177 3.43 
#&gt; 178 3.25 
#&gt; 179 3.07 
#&gt; 180 2.91 
#&gt; 181 2.75 
#&gt; 182 2.6 
#&gt; 183 2.47 
#&gt; 184 2.33 
#&gt; 185 2.21 
#&gt; 186 2.09 
#&gt; 187 1.98 
#&gt; 188 1.88 
#&gt; 189 1.78 
#&gt; 190 1.68 
#&gt; 191 1.6 
#&gt; 192 1.51 
#&gt; 193 1.43 
#&gt; 194 1.36 
#&gt; 195 1.29 
#&gt; 196 1.22 
#&gt; 197 1.15 
#&gt; 198 1.09 
#&gt; 199 1.04 
#&gt; 200 0.983 
#&gt; 201 0.932 
#&gt; 202 0.883 
#&gt; 203 0.837 
#&gt; 204 0.794 
#&gt; 205 0.753 
#&gt; 206 0.714 
#&gt; 207 0.677 
#&gt; 208 0.642 
#&gt; 209 0.609 
#&gt; 210 0.577 
#&gt; 211 0.548 
#&gt; 212 0.519 
#&gt; 213 0.493 
#&gt; 214 0.467 
#&gt; 215 0.443 
#&gt; 216 0.421 
#&gt; 217 0.399 
#&gt; 218 0.379 
#&gt; 219 0.359 
#&gt; 220 0.341 
#&gt; 221 0.324 
#&gt; 222 0.307 
#&gt; 223 0.292 
#&gt; 224 0.277 
#&gt; 225 0.263 
#&gt; 226 0.249 
#&gt; 227 0.237 
#&gt; 228 0.225 
#&gt; 229 0.213 
#&gt; 230 0.203 
#&gt; 231 0.192 
#&gt; 232 0.183 
#&gt; 233 0.173 
#&gt; 234 0.165 
#&gt; 235 0.156 
#&gt; 236 0.149 
#&gt; 237 0.141 
#&gt; 238 0.134 
#&gt; 239 0.127 
#&gt; 240 0.121 
#&gt; 241 0.115 
#&gt; 242 0.109 
#&gt; 243 0.104 
#&gt; 244 0.0985 
#&gt; 245 0.0936 
#&gt; 246 0.0889 
#&gt; 247 0.0845 
#&gt; 248 0.0803 
#&gt; 249 0.0763 
#&gt; 250 0.0725 
#&gt; 251 0.0689 
#&gt; 252 0.0655 
#&gt; 253 0.0623 
#&gt; 254 0.0592 
#&gt; 255 0.0563 
#&gt; 256 0.0535 
#&gt; 257 0.0508 
#&gt; 258 0.0483 
#&gt; 259 0.0459 
#&gt; 260 0.0437 
#&gt; 261 0.0415 
#&gt; 262 0.0395 
#&gt; 263 0.0375 
#&gt; 264 0.0357 
#&gt; 265 0.0339 
#&gt; 266 0.0323 
#&gt; 267 0.0307 
#&gt; 268 0.0292 
#&gt; 269 0.0278 
#&gt; 270 0.0264 
#&gt; 271 0.0251 
#&gt; 272 0.0239 
#&gt; 273 0.0227 
#&gt; 274 0.0216 
#&gt; 275 0.0206 
#&gt; 276 0.0196 
#&gt; 277 0.0186 
#&gt; 278 0.0177 
#&gt; 279 0.0168 
#&gt; 280 0.016 
#&gt; 281 0.0152 
#&gt; 282 0.0145 
#&gt; 283 0.0138 
#&gt; 284 0.0131 
#&gt; 285 0.0125 
#&gt; 286 0.0119 
#&gt; 287 0.0113 
#&gt; 288 0.0108 
#&gt; 289 0.0102 
#&gt; 290 0.00975 
#&gt; 291 0.00927 
#&gt; 292 0.00883 
#&gt; 293 0.0084 
#&gt; 294 0.008 
#&gt; 295 0.00761 
#&gt; 296 0.00724 
#&gt; 297 0.0069 
#&gt; 298 0.00656 
#&gt; 299 0.00625 
#&gt; 300 0.00595 
#&gt; 301 0.00566 
#&gt; 302 0.00539 
#&gt; 303 0.00513 
#&gt; 304 0.00489 
#&gt; 305 0.00465 
#&gt; 306 0.00443 
#&gt; 307 0.00422 
#&gt; 308 0.00401 
#&gt; 309 0.00382 
#&gt; 310 0.00364 
#&gt; 311 0.00347 
#&gt; 312 0.0033 
#&gt; 313 0.00314 
#&gt; 314 0.00299 
#&gt; 315 0.00285 
#&gt; 316 0.00271 
#&gt; 317 0.00259 
#&gt; 318 0.00246 
#&gt; 319 0.00234 
#&gt; 320 0.00223 
#&gt; 321 0.00213 
#&gt; 322 0.00203 
#&gt; 323 0.00193 
#&gt; 324 0.00184 
#&gt; 325 0.00175 
#&gt; 326 0.00167 
#&gt; 327 0.00159 
#&gt; 328 0.00151 
#&gt; 329 0.00144 
#&gt; 330 0.00137 
#&gt; 331 0.00131 
#&gt; 332 0.00125 
#&gt; 333 0.00119 
#&gt; 334 0.00113 
#&gt; 335 0.00108 
#&gt; 336 0.00103 
#&gt; 337 0.000979 
#&gt; 338 0.000932 
#&gt; 339 0.000888 
#&gt; 340 0.000846 
#&gt; 341 0.000807 
#&gt; 342 0.000768 
#&gt; 343 0.000732 
#&gt; 344 0.000698 
#&gt; 345 0.000665 
#&gt; 346 0.000634 
#&gt; 347 0.000604 
#&gt; 348 0.000575 
#&gt; 349 0.000548 
#&gt; 350 0.000523 
#&gt; 351 0.000498 
#&gt; 352 0.000475 
#&gt; 353 0.000452 
#&gt; 354 0.000431 
#&gt; 355 0.000411 
#&gt; 356 0.000392 
#&gt; 357 0.000373 
#&gt; 358 0.000356 
#&gt; 359 0.000339 
#&gt; 360 0.000323 
#&gt; 361 0.000308 
#&gt; 362 0.000294 
#&gt; 363 0.00028 
#&gt; 364 0.000267 
#&gt; 365 0.000254 
#&gt; 366 0.000243 
#&gt; 367 0.000231 
#&gt; 368 0.00022 
#&gt; 369 0.00021 
#&gt; 370 2e-04 
#&gt; 371 0.000191 
#&gt; 372 0.000182 
#&gt; 373 0.000174 
#&gt; 374 0.000165 
#&gt; 375 0.000158 
#&gt; 376 0.00015 
#&gt; 377 0.000143 
#&gt; 378 0.000137 
#&gt; 379 0.00013 
#&gt; 380 0.000124 
#&gt; 381 0.000119 
#&gt; 382 0.000113 
#&gt; 383 0.000108 
#&gt; 384 0.000103 
#&gt; 385 9.8e-05 
#&gt; 386 9.34e-05 
#&gt; 387 8.91e-05 
#&gt; 388 8.49e-05 
#&gt; 389 8.1e-05 
#&gt; 390 7.72e-05 
#&gt; 391 7.37e-05 
#&gt; 392 7.02e-05 
#&gt; 393 6.7e-05 
#&gt; 394 6.39e-05 
#&gt; 395 6.09e-05 
#&gt; 396 5.81e-05 
#&gt; 397 5.54e-05 
#&gt; 398 5.28e-05 
#&gt; 399 5.04e-05 
#&gt; 400 4.81e-05 
#&gt; 401 4.58e-05 
#&gt; 402 4.37e-05 
#&gt; 403 4.17e-05 
#&gt; 404 3.98e-05 
#&gt; 405 3.79e-05 
#&gt; 406 3.62e-05 
#&gt; 407 3.45e-05 
#&gt; 408 3.29e-05 
#&gt; 409 3.14e-05 
#&gt; 410 2.99e-05 
#&gt; 411 2.86e-05 
#&gt; 412 2.72e-05 
#&gt; 413 2.6e-05 
#&gt; 414 2.48e-05 
#&gt; 415 2.36e-05 
#&gt; 416 2.25e-05 
#&gt; 417 2.15e-05 
#&gt; 418 2.05e-05 
#&gt; 419 1.96e-05 
#&gt; 420 1.87e-05 
#&gt; 421 1.78e-05 
#&gt; 422 1.7e-05 
#&gt; 423 1.62e-05 
#&gt; 424 1.55e-05 
#&gt; 425 1.48e-05 
#&gt; 426 1.41e-05 
#&gt; 427 1.34e-05 
#&gt; 428 1.28e-05 
#&gt; 429 1.22e-05 
#&gt; 430 1.17e-05 
#&gt; 431 1.11e-05 
#&gt; 432 1.06e-05 
#&gt; 433 1.01e-05 
#&gt; 434 9.66e-06 
#&gt; 435 9.22e-06 
#&gt; 436 8.79e-06 
#&gt; 437 8.39e-06 
#&gt; 438 8e-06 
#&gt; 439 7.64e-06 
#&gt; 440 7.29e-06 
#&gt; 441 6.95e-06 
#&gt; 442 6.63e-06 
#&gt; 443 6.33e-06 
#&gt; 444 6.04e-06 
#&gt; 445 5.76e-06 
#&gt; 446 5.5e-06 
#&gt; 447 5.25e-06 
#&gt; 448 5.01e-06 
#&gt; 449 4.78e-06 
#&gt; 450 4.56e-06 
#&gt; 451 4.35e-06 
#&gt; 452 4.15e-06 
#&gt; 453 3.96e-06 
#&gt; 454 3.78e-06 
#&gt; 455 3.61e-06 
#&gt; 456 3.44e-06 
#&gt; 457 3.28e-06 
#&gt; 458 3.13e-06 
#&gt; 459 2.99e-06 
#&gt; 460 2.85e-06 
#&gt; 461 2.72e-06 
#&gt; 462 2.6e-06 
#&gt; 463 2.48e-06 
#&gt; 464 2.37e-06 
#&gt; 465 2.26e-06 
#&gt; 466 2.15e-06 
#&gt; 467 2.06e-06 
#&gt; 468 1.96e-06 
#&gt; 469 1.87e-06 
#&gt; 470 1.79e-06 
#&gt; 471 1.71e-06 
#&gt; 472 1.63e-06 
#&gt; 473 1.55e-06 
#&gt; 474 1.48e-06 
#&gt; 475 1.42e-06 
#&gt; 476 1.35e-06 
#&gt; 477 1.29e-06 
#&gt; 478 1.23e-06 
#&gt; 479 1.17e-06 
#&gt; 480 1.12e-06 
#&gt; 481 1.07e-06 
#&gt; 482 1.02e-06 
#&gt; 483 9.74e-07 
#&gt; 484 9.3e-07 
#&gt; 485 8.88e-07 
#&gt; 486 8.47e-07 
#&gt; 487 8.09e-07 
#&gt; 488 7.72e-07 
#&gt; 489 7.37e-07 
#&gt; 490 7.03e-07 
#&gt; 491 6.71e-07 
#&gt; 492 6.41e-07 
#&gt; 493 6.12e-07 
#&gt; 494 5.84e-07 
#&gt; 495 5.57e-07 
#&gt; 496 5.32e-07 
#&gt; 497 5.08e-07 
#&gt; 498 4.85e-07 
#&gt; 499 4.63e-07 
#&gt; 500 4.42e-07 
#&gt; 2.739 sec elapsed</code></pre>
</div>
<div id="the-neural-network-written-in-pytorch" class="section level2" number="12.3">
<h2><span class="header-section-number">12.3</span> The neural network written in <code>PyTorch</code></h2>
<p>Here is the same example we have used above but written in PyTorch. Notice the following differences with the <code>numpy</code> code:</p>
<ul>
<li><p>we select the computation device which could be <code>cpu</code> or <code>gpu</code></p></li>
<li><p>when building or creating the tensors, we specify which device we want to use</p></li>
<li><p>the tensors have <code>torch</code> methods and properties. Example: <code>mm()</code>, <code>clamp()</code>, <code>sum()</code>, <code>clone()</code>, and <code>t()</code>,</p></li>
<li><p>also notice the use some <code>torch</code> functions: <code>device()</code>, <code>randn()</code></p></li>
</ul>
<div class="sourceCode" id="cb518"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb518-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb518-1" aria-hidden="true" tabindex="-1"></a>reticulate<span class="sc">::</span><span class="fu">use_condaenv</span>(<span class="st">&quot;r-torch&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb519"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb519-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Code in file tensor/two_layer_net_tensor.py</span></span>
<span id="cb519-2"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb519-3"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb519-4"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb519-5"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-5" aria-hidden="true" tabindex="-1"></a>ms <span class="op">=</span> torch.manual_seed(<span class="dv">0</span>)</span>
<span id="cb519-6"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-6" aria-hidden="true" tabindex="-1"></a>tic <span class="op">=</span> time.process_time()</span>
<span id="cb519-7"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-7" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">&#39;cpu&#39;</span>)</span>
<span id="cb519-8"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-8" aria-hidden="true" tabindex="-1"></a><span class="co"># device = torch.device(&#39;cuda&#39;)  # Uncomment this to run on GPU</span></span>
<span id="cb519-9"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb519-10"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-10" aria-hidden="true" tabindex="-1"></a><span class="co"># N is batch size; D_in is input dimension;</span></span>
<span id="cb519-11"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-11" aria-hidden="true" tabindex="-1"></a><span class="co"># H is hidden dimension; D_out is output dimension.</span></span>
<span id="cb519-12"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-12" aria-hidden="true" tabindex="-1"></a>N, D_in, H, D_out <span class="op">=</span> <span class="dv">64</span>, <span class="dv">1000</span>, <span class="dv">100</span>, <span class="dv">10</span></span>
<span id="cb519-13"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb519-14"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Create random input and output data</span></span>
<span id="cb519-15"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-15" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(N, D_in, device<span class="op">=</span>device)</span>
<span id="cb519-16"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-16" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.randn(N, D_out, device<span class="op">=</span>device)</span>
<span id="cb519-17"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb519-18"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Randomly initialize weights</span></span>
<span id="cb519-19"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-19" aria-hidden="true" tabindex="-1"></a>w1 <span class="op">=</span> torch.randn(D_in, H, device<span class="op">=</span>device)</span>
<span id="cb519-20"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-20" aria-hidden="true" tabindex="-1"></a>w2 <span class="op">=</span> torch.randn(H, D_out, device<span class="op">=</span>device)</span>
<span id="cb519-21"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb519-22"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-22" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">1e-6</span></span>
<span id="cb519-23"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">500</span>):</span>
<span id="cb519-24"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-24" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Forward pass: compute predicted y</span></span>
<span id="cb519-25"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-25" aria-hidden="true" tabindex="-1"></a>  h <span class="op">=</span> x.mm(w1)</span>
<span id="cb519-26"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-26" aria-hidden="true" tabindex="-1"></a>  h_relu <span class="op">=</span> h.clamp(<span class="bu">min</span><span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb519-27"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-27" aria-hidden="true" tabindex="-1"></a>  y_pred <span class="op">=</span> h_relu.mm(w2)</span>
<span id="cb519-28"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb519-29"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-29" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor</span></span>
<span id="cb519-30"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-30" aria-hidden="true" tabindex="-1"></a>  <span class="co"># of shape (); we can get its value as a Python number with loss.item().</span></span>
<span id="cb519-31"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-31" aria-hidden="true" tabindex="-1"></a>  loss <span class="op">=</span> (y_pred <span class="op">-</span> y).<span class="bu">pow</span>(<span class="dv">2</span>).<span class="bu">sum</span>()</span>
<span id="cb519-32"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-32" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(t, loss.item())</span>
<span id="cb519-33"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb519-34"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-34" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span>
<span id="cb519-35"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-35" aria-hidden="true" tabindex="-1"></a>  grad_y_pred <span class="op">=</span> <span class="fl">2.0</span> <span class="op">*</span> (y_pred <span class="op">-</span> y)</span>
<span id="cb519-36"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-36" aria-hidden="true" tabindex="-1"></a>  grad_w2 <span class="op">=</span> h_relu.t().mm(grad_y_pred)</span>
<span id="cb519-37"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-37" aria-hidden="true" tabindex="-1"></a>  grad_h_relu <span class="op">=</span> grad_y_pred.mm(w2.t())</span>
<span id="cb519-38"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-38" aria-hidden="true" tabindex="-1"></a>  grad_h <span class="op">=</span> grad_h_relu.clone()</span>
<span id="cb519-39"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-39" aria-hidden="true" tabindex="-1"></a>  grad_h[h <span class="op">&lt;</span> <span class="dv">0</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb519-40"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-40" aria-hidden="true" tabindex="-1"></a>  grad_w1 <span class="op">=</span> x.t().mm(grad_h)</span>
<span id="cb519-41"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb519-42"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-42" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Update weights using gradient descent</span></span>
<span id="cb519-43"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-43" aria-hidden="true" tabindex="-1"></a>  w1 <span class="op">-=</span> learning_rate <span class="op">*</span> grad_w1</span>
<span id="cb519-44"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-44" aria-hidden="true" tabindex="-1"></a>  w2 <span class="op">-=</span> learning_rate <span class="op">*</span> grad_w2</span></code></pre></div>
<pre><code>#&gt; 0 28304951.678202875
#&gt; 1 24927328.6639303
#&gt; 2 28155855.770583864
#&gt; 3 34180357.7194307
#&gt; 4 37707746.88778937
#&gt; 5 33620991.02849947
#&gt; 6 22446553.707722448
#&gt; 7 11618280.121770753
#&gt; 8 5223431.478905054
#&gt; 9 2461507.5224659983
#&gt; 10 1379540.5581786444
#&gt; 11 932839.5714197145
#&gt; 12 715554.0194977626
#&gt; 13 585999.3830106169
#&gt; 14 495224.1190599698
#&gt; 15 425139.8269296172
#&gt; 16 368213.26971499954
#&gt; 17 320861.3685088707
#&gt; 18 280855.466445944
#&gt; 19 246786.88499174578
#&gt; 20 217605.04415179993
#&gt; 21 192469.10582625493
#&gt; 22 170743.82020579866
#&gt; 23 151851.94095434327
#&gt; 24 135381.0968247089
#&gt; 25 120989.87510382358
#&gt; 26 108377.08193979257
#&gt; 27 97286.4646626631
#&gt; 28 87516.74262182195
#&gt; 29 78871.40974253972
#&gt; 30 71194.55619418157
#&gt; 31 64368.698068814076
#&gt; 32 58290.88589673794
#&gt; 33 52862.53039132398
#&gt; 34 48006.486289653774
#&gt; 35 43648.17638706032
#&gt; 36 39733.12183144025
#&gt; 37 36212.83667739258
#&gt; 38 33041.34233720849
#&gt; 39 30180.24917305512
#&gt; 40 27596.03102989422
#&gt; 41 25257.718969527337
#&gt; 42 23140.47748479181
#&gt; 43 21219.215332345848
#&gt; 44 19477.346011970196
#&gt; 45 17898.94249245009
#&gt; 46 16461.18614409637
#&gt; 47 15150.468945112083
#&gt; 48 13954.327647924765
#&gt; 49 12862.049952206966
#&gt; 50 11863.846542866391
#&gt; 51 10950.190923462626
#&gt; 52 10112.944267863539
#&gt; 53 9346.057705583647
#&gt; 54 8643.227456215867
#&gt; 55 7998.013709383482
#&gt; 56 7404.314143395923
#&gt; 57 6858.3403423632835
#&gt; 58 6355.823822117154
#&gt; 59 5893.272718725857
#&gt; 60 5467.590824799892
#&gt; 61 5075.314900617862
#&gt; 62 4713.257357968131
#&gt; 63 4378.928521878634
#&gt; 64 4070.089909903081
#&gt; 65 3784.681538189908
#&gt; 66 3520.7321920656836
#&gt; 67 3276.437457528057
#&gt; 68 3050.1573582667497
#&gt; 69 2840.7435683049644
#&gt; 70 2646.8554702835486
#&gt; 71 2467.1301658974385
#&gt; 72 2300.4931160132837
#&gt; 73 2145.7384163982692
#&gt; 74 2002.064527849923
#&gt; 75 1868.6258523243748
#&gt; 76 1744.5305134660675
#&gt; 77 1629.2088434191028
#&gt; 78 1521.921263744062
#&gt; 79 1422.0702028813182
#&gt; 80 1329.1314050876515
#&gt; 81 1242.5784391571635
#&gt; 82 1161.9741527801023
#&gt; 83 1086.8592749421489
#&gt; 84 1016.8590222122994
#&gt; 85 951.5662507540096
#&gt; 86 890.6602544816327
#&gt; 87 833.8443278601538
#&gt; 88 780.8578032641328
#&gt; 89 731.3973374911395
#&gt; 90 685.1947512043153
#&gt; 91 642.0442981541431
#&gt; 92 601.7328625418835
#&gt; 93 564.0571957786616
#&gt; 94 528.8261654125921
#&gt; 95 495.902553693872
#&gt; 96 465.09740183896065
#&gt; 97 436.29130708841797
#&gt; 98 409.31979498318674
#&gt; 99 384.0735885326195
#&gt; 100 360.44638041822594
#&gt; 101 338.3392483462657
#&gt; 102 317.6202138550324
#&gt; 103 298.21139417294785
#&gt; 104 280.03437380191264
#&gt; 105 263.00447665526605
#&gt; 106 247.03986887884167
#&gt; 107 232.07821803150665
#&gt; 108 218.04881824787782
#&gt; 109 204.90061200353932
#&gt; 110 192.56394398444377
#&gt; 111 180.98923443491825
#&gt; 112 170.13203285049113
#&gt; 113 159.9505069457677
#&gt; 114 150.3897248578236
#&gt; 115 141.4173419841134
#&gt; 116 132.9961149736754
#&gt; 117 125.09156325963654
#&gt; 118 117.66642940606961
#&gt; 119 110.69533918173482
#&gt; 120 104.14845396396551
#&gt; 121 97.99893007959595
#&gt; 122 92.22011750510347
#&gt; 123 86.78965563408651
#&gt; 124 81.68832953028868
#&gt; 125 76.89369485137074
#&gt; 126 72.38701330673487
#&gt; 127 68.15098341186066
#&gt; 128 64.17155344502414
#&gt; 129 60.42803847692409
#&gt; 130 56.90822735092523
#&gt; 131 53.59735929955392
#&gt; 132 50.482912635102096
#&gt; 133 47.55356853186724
#&gt; 134 44.79743700983772
#&gt; 135 42.204954739023094
#&gt; 136 39.76511807450798
#&gt; 137 37.468859040583865
#&gt; 138 35.30845310161282
#&gt; 139 33.27467715145973
#&gt; 140 31.360220767969174
#&gt; 141 29.558058573196686
#&gt; 142 27.861685433714456
#&gt; 143 26.264124116161796
#&gt; 144 24.75962375030956
#&gt; 145 23.343125632673342
#&gt; 146 22.008981468973907
#&gt; 147 20.752466464076186
#&gt; 148 19.568797416670904
#&gt; 149 18.453977382983343
#&gt; 150 17.403372852348166
#&gt; 151 16.413591679611937
#&gt; 152 15.481311105285194
#&gt; 153 14.602503969148433
#&gt; 154 13.774233452362386
#&gt; 155 12.994002407102446
#&gt; 156 12.258561227892864
#&gt; 157 11.565443792085238
#&gt; 158 10.912019785854863
#&gt; 159 10.29598609513392
#&gt; 160 9.715282302954897
#&gt; 161 9.167793733318923
#&gt; 162 8.651771772978766
#&gt; 163 8.164942470537255
#&gt; 164 7.705943656617592
#&gt; 165 7.273075881961047
#&gt; 166 6.864819666905014
#&gt; 167 6.479820163401724
#&gt; 168 6.1167081584500504
#&gt; 169 5.774100177817066
#&gt; 170 5.450995917450388
#&gt; 171 5.14617953857055
#&gt; 172 4.858643025682133
#&gt; 173 4.587382709817419
#&gt; 174 4.331387287369351
#&gt; 175 4.0899014249256584
#&gt; 176 3.862103131723888
#&gt; 177 3.6470527511392836
#&gt; 178 3.4441067125944795
#&gt; 179 3.252609836897492
#&gt; 180 3.071816765868509
#&gt; 181 2.9012185410680806
#&gt; 182 2.7402092504331836
#&gt; 183 2.5882196806285753
#&gt; 184 2.444769343935472
#&gt; 185 2.3093519385929504
#&gt; 186 2.181504010372785
#&gt; 187 2.0608004075817794
#&gt; 188 1.94686287787116
#&gt; 189 1.8395089185871978
#&gt; 190 1.7383087355434326
#&gt; 191 1.642721224723191
#&gt; 192 1.5524241674871404
#&gt; 193 1.4671676414789498
#&gt; 194 1.3866326949058312
#&gt; 195 1.3105816336367513
#&gt; 196 1.238728798737455
#&gt; 197 1.1708405590524702
#&gt; 198 1.1067184254730693
#&gt; 199 1.0461214720168293
#&gt; 200 0.988882118783601
#&gt; 201 0.9348112964047799
#&gt; 202 0.8837227088872526
#&gt; 203 0.835467413742493
#&gt; 204 0.7898553794337078
#&gt; 205 0.7467719311431874
#&gt; 206 0.706057495497072
#&gt; 207 0.667565811137808
#&gt; 208 0.6311950860161368
#&gt; 209 0.5968226032998389
#&gt; 210 0.5643375968525862
#&gt; 211 0.5336355741319234
#&gt; 212 0.504610088158823
#&gt; 213 0.47718566009270746
#&gt; 214 0.45125778948937145
#&gt; 215 0.42675382897281866
#&gt; 216 0.4035914490050285
#&gt; 217 0.38169380063839276
#&gt; 218 0.3609943366232995
#&gt; 219 0.34142901569763473
#&gt; 220 0.3229336644020611
#&gt; 221 0.30544089540015784
#&gt; 222 0.28890489048702905
#&gt; 223 0.273269578663846
#&gt; 224 0.25848651426797276
#&gt; 225 0.24450984538843007
#&gt; 226 0.23129297732745033
#&gt; 227 0.2187967010829131
#&gt; 228 0.20697931842607983
#&gt; 229 0.19580671378874537
#&gt; 230 0.18523834142731682
#&gt; 231 0.17524520436757623
#&gt; 232 0.1657943113710092
#&gt; 233 0.15685667535611536
#&gt; 234 0.14840827333322165
#&gt; 235 0.1404142723759103
#&gt; 236 0.13285399914584847
#&gt; 237 0.1257020226899864
#&gt; 238 0.11893866535669374
#&gt; 239 0.11254012548427543
#&gt; 240 0.10648973715952825
#&gt; 241 0.10076465672099208
#&gt; 242 0.09535057382494336
#&gt; 243 0.09022846810149934
#&gt; 244 0.08538314823665655
#&gt; 245 0.0807998885782199
#&gt; 246 0.07646497849047959
#&gt; 247 0.07236264442670526
#&gt; 248 0.06848434833114786
#&gt; 249 0.06481281290919032
#&gt; 250 0.061339266016361305
#&gt; 251 0.058053076624774276
#&gt; 252 0.05494384702981967
#&gt; 253 0.05200185854877895
#&gt; 254 0.04921917115147216
#&gt; 255 0.04658596833483621
#&gt; 256 0.04409498516075393
#&gt; 257 0.04173722942941192
#&gt; 258 0.03950646096310502
#&gt; 259 0.03739526392881331
#&gt; 260 0.035397675983245905
#&gt; 261 0.03350714940338512
#&gt; 262 0.03171914800558596
#&gt; 263 0.030026565964001137
#&gt; 264 0.028424539700355397
#&gt; 265 0.02690826459692102
#&gt; 266 0.0254734519722664
#&gt; 267 0.024115391447933534
#&gt; 268 0.022829917524534244
#&gt; 269 0.02161353412653501
#&gt; 270 0.02046211324589218
#&gt; 271 0.019372541709399185
#&gt; 272 0.018341010110597494
#&gt; 273 0.017364750002398423
#&gt; 274 0.016440801352053572
#&gt; 275 0.015566351570904894
#&gt; 276 0.014738758943595356
#&gt; 277 0.013955031983031344
#&gt; 278 0.013213137527317422
#&gt; 279 0.012510963462657058
#&gt; 280 0.01184618659504817
#&gt; 281 0.011216909862542226
#&gt; 282 0.010621170683666852
#&gt; 283 0.010057232252600974
#&gt; 284 0.009523402267278717
#&gt; 285 0.009017963443058108
#&gt; 286 0.00853956066110637
#&gt; 287 0.008086641395192046
#&gt; 288 0.0076577986044574515
#&gt; 289 0.007251985398647458
#&gt; 290 0.006867552434207753
#&gt; 291 0.0065036150242445
#&gt; 292 0.0061589977274546836
#&gt; 293 0.005832728877811346
#&gt; 294 0.0055238333065745285
#&gt; 295 0.0052313168873346135
#&gt; 296 0.004954423113002782
#&gt; 297 0.004692217253596893
#&gt; 298 0.0044439183256077056
#&gt; 299 0.004208844585203585
#&gt; 300 0.0039862405530345394
#&gt; 301 0.0037754867189063604
#&gt; 302 0.0035759236905880025
#&gt; 303 0.003386943755817545
#&gt; 304 0.0032079271944867598
#&gt; 305 0.0030384316988391583
#&gt; 306 0.0028779268787451255
#&gt; 307 0.00272593446766093
#&gt; 308 0.0025819845895133416
#&gt; 309 0.0024456583936131064
#&gt; 310 0.0023165757211595604
#&gt; 311 0.0021943198339292253
#&gt; 312 0.0020785379932759178
#&gt; 313 0.0019688908467641575
#&gt; 314 0.0018650391381504062
#&gt; 315 0.0017667028380151024
#&gt; 316 0.001673582502888648
#&gt; 317 0.0015853676791664365
#&gt; 318 0.0015018105395260203
#&gt; 319 0.0014226731177160083
#&gt; 320 0.001347714346774477
#&gt; 321 0.001276722959315653
#&gt; 322 0.0012094776044946054
#&gt; 323 0.0011457905013085136
#&gt; 324 0.0010854694552154343
#&gt; 325 0.00102832969376531
#&gt; 326 0.000974209625839674
#&gt; 327 0.0009229458544159802
#&gt; 328 0.0008743961838085411
#&gt; 329 0.0008284222289505557
#&gt; 330 0.0007848532048264199
#&gt; 331 0.0007435814161699444
#&gt; 332 0.0007044863185096952
#&gt; 333 0.0006674517964981812
#&gt; 334 0.0006323708525592692
#&gt; 335 0.0005991396002037806
#&gt; 336 0.000567659127372756
#&gt; 337 0.000537841006879714
#&gt; 338 0.0005095932062070654
#&gt; 339 0.00048283262452850794
#&gt; 340 0.0004574801373032235
#&gt; 341 0.0004334642559669667
#&gt; 342 0.00041071676854426804
#&gt; 343 0.0003891669811254254
#&gt; 344 0.00036874509104527534
#&gt; 345 0.00034939800233551536
#&gt; 346 0.00033106827511562186
#&gt; 347 0.00031370294821781266
#&gt; 348 0.00029725119212988167
#&gt; 349 0.00028166684242615344
#&gt; 350 0.0002669014349453837
#&gt; 351 0.0002529104551679453
#&gt; 352 0.00023965499091091642
#&gt; 353 0.00022709499915437483
#&gt; 354 0.00021519647430201294
#&gt; 355 0.00020392409418245903
#&gt; 356 0.00019324565889762736
#&gt; 357 0.00018312352945852278
#&gt; 358 0.00017353441805992892
#&gt; 359 0.00016444931721080637
#&gt; 360 0.00015584082439144625
#&gt; 361 0.0001476829549914828
#&gt; 362 0.00013995332436004464
#&gt; 363 0.0001326295572686851
#&gt; 364 0.00012569091305484416
#&gt; 365 0.00011911540231082307
#&gt; 366 0.00011288450618005281
#&gt; 367 0.00010698023609264636
#&gt; 368 0.00010138656406386237
#&gt; 369 9.608703941146006e-05
#&gt; 370 9.10641618909672e-05
#&gt; 371 8.63036350072799e-05
#&gt; 372 8.179250816467423e-05
#&gt; 373 7.751771557662506e-05
#&gt; 374 7.346701689471864e-05
#&gt; 375 6.962838417913235e-05
#&gt; 376 6.599115070650522e-05
#&gt; 377 6.254418117397552e-05
#&gt; 378 5.9277521713027985e-05
#&gt; 379 5.618173072483101e-05
#&gt; 380 5.3248296817493444e-05
#&gt; 381 5.0468440037584684e-05
#&gt; 382 4.783531311488376e-05
#&gt; 383 4.533824050989163e-05
#&gt; 384 4.29719258994691e-05
#&gt; 385 4.072931820039442e-05
#&gt; 386 3.860419712198903e-05
#&gt; 387 3.6589984097044954e-05
#&gt; 388 3.468116364059317e-05
#&gt; 389 3.287222611907134e-05
#&gt; 390 3.115784475053244e-05
#&gt; 391 2.9533143774866968e-05
#&gt; 392 2.7993239384037584e-05
#&gt; 393 2.6533713458016734e-05
#&gt; 394 2.5150503707662906e-05
#&gt; 395 2.3840054483217527e-05
#&gt; 396 2.2597735768618626e-05
#&gt; 397 2.1420010465553325e-05
#&gt; 398 2.030382796515953e-05
#&gt; 399 1.9245876020804877e-05
#&gt; 400 1.8243200207829568e-05
#&gt; 401 1.72928590104265e-05
#&gt; 402 1.639224233300692e-05
#&gt; 403 1.553860787455827e-05
#&gt; 404 1.4729443662689417e-05
#&gt; 405 1.3962452336983257e-05
#&gt; 406 1.3235478070943392e-05
#&gt; 407 1.2546412926345343e-05
#&gt; 408 1.189360422859807e-05
#&gt; 409 1.1274605652117058e-05
#&gt; 410 1.0687845492426086e-05
#&gt; 411 1.0131680171403766e-05
#&gt; 412 9.604526090890798e-06
#&gt; 413 9.104866882213997e-06
#&gt; 414 8.631198696567078e-06
#&gt; 415 8.182218044070821e-06
#&gt; 416 7.75664115683507e-06
#&gt; 417 7.353270255865123e-06
#&gt; 418 6.970852595729622e-06
#&gt; 419 6.608387207165208e-06
#&gt; 420 6.264784392323233e-06
#&gt; 421 5.939231288374619e-06
#&gt; 422 5.630510532790305e-06
#&gt; 423 5.337868490745223e-06
#&gt; 424 5.060468769654502e-06
#&gt; 425 4.79747919731141e-06
#&gt; 426 4.548175418631486e-06
#&gt; 427 4.311842569116853e-06
#&gt; 428 4.0878168953101445e-06
#&gt; 429 3.875456427475025e-06
#&gt; 430 3.6741748595036232e-06
#&gt; 431 3.4833269248703717e-06
#&gt; 432 3.3024133002363433e-06
#&gt; 433 3.1309131719210234e-06
#&gt; 434 2.96842021991241e-06
#&gt; 435 2.8143063952853263e-06
#&gt; 436 2.6681942944162764e-06
#&gt; 437 2.5296788307168387e-06
#&gt; 438 2.398359475120639e-06
#&gt; 439 2.2738738447735176e-06
#&gt; 440 2.1558529903028955e-06
#&gt; 441 2.043973414925204e-06
#&gt; 442 1.937914955976e-06
#&gt; 443 1.8373813025117565e-06
#&gt; 444 1.7420525189323081e-06
#&gt; 445 1.6516818382471362e-06
#&gt; 446 1.5660026988565995e-06
#&gt; 447 1.4848126887379396e-06
#&gt; 448 1.4078052437347216e-06
#&gt; 449 1.3347904752897985e-06
#&gt; 450 1.2655713812394453e-06
#&gt; 451 1.1999412751104083e-06
#&gt; 452 1.1377221542094639e-06
#&gt; 453 1.0787354003046668e-06
#&gt; 454 1.0228096438668164e-06
#&gt; 455 9.697877298065688e-07
#&gt; 456 9.19523236016184e-07
#&gt; 457 8.718667280516637e-07
#&gt; 458 8.26680714333657e-07
#&gt; 459 7.838400421017189e-07
#&gt; 460 7.432422667091295e-07
#&gt; 461 7.047295295500828e-07
#&gt; 462 6.682172782081674e-07
#&gt; 463 6.335979167725098e-07
#&gt; 464 6.00773919679542e-07
#&gt; 465 5.696536284140131e-07
#&gt; 466 5.40148377027734e-07
#&gt; 467 5.121740256365324e-07
#&gt; 468 4.856503636162229e-07
#&gt; 469 4.60499483657311e-07
#&gt; 470 4.366530917811009e-07
#&gt; 471 4.140445328299982e-07
#&gt; 472 3.926067623835811e-07
#&gt; 473 3.7229033787640473e-07
#&gt; 474 3.530199060266021e-07
#&gt; 475 3.347453714494452e-07
#&gt; 476 3.1741785274561153e-07
#&gt; 477 3.0098863896686543e-07
#&gt; 478 2.8541218477373855e-07
#&gt; 479 2.706418664275911e-07
#&gt; 480 2.566371859854609e-07
#&gt; 481 2.433570345044821e-07
#&gt; 482 2.3076549095254704e-07
#&gt; 483 2.1882618343061597e-07
#&gt; 484 2.075048994651545e-07
#&gt; 485 1.9677098639464595e-07
#&gt; 486 1.8659788170175725e-07
#&gt; 487 1.769471938085824e-07
#&gt; 488 1.6779487752836963e-07
#&gt; 489 1.591173465194994e-07
#&gt; 490 1.5088941983564955e-07
#&gt; 491 1.4308626190217763e-07
#&gt; 492 1.3568760341969896e-07
#&gt; 493 1.2867142459194279e-07
#&gt; 494 1.2201888483467056e-07
#&gt; 495 1.1571047963285856e-07
#&gt; 496 1.0972832085184751e-07
#&gt; 497 1.0405612689291217e-07
#&gt; 498 9.867753049705583e-08
#&gt; 499 9.357999193873026e-08</code></pre>
<div class="sourceCode" id="cb521"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb521-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb521-1" aria-hidden="true" tabindex="-1"></a>toc <span class="op">=</span> time.process_time()</span>
<span id="cb521-2"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb521-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(toc <span class="op">-</span> tic, <span class="st">&quot;seconds&quot;</span>)</span></code></pre></div>
<pre><code>#&gt; 96.11892338700002 seconds</code></pre>
</div>
<div id="a-neural-network-written-in-rtorch" class="section level2" number="12.4">
<h2><span class="header-section-number">12.4</span> A neural network written in <code>rTorch</code></h2>
<p>The example shows the long and manual way of calculating the forward and backward passes but using <code>rTorch</code>. The objective is getting familiarized with the rTorch tensor operations.</p>
<p>The following example was converted from <strong>PyTorch</strong> to <strong>rTorch</strong> to show differences and similarities of both approaches. The original source can be found here: <a href="https://github.com/jcjohnson/pytorch-examples#pytorch-tensors" class="uri">Source</a>.</p>
<div id="load-the-libraries" class="section level3" number="12.4.1">
<h3><span class="header-section-number">12.4.1</span> Load the libraries</h3>
<div class="sourceCode" id="cb523"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb523-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rTorch)</span>
<span id="cb523-2"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb523-3"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb523-4"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-4" aria-hidden="true" tabindex="-1"></a>device <span class="ot">=</span> torch<span class="sc">$</span><span class="fu">device</span>(<span class="st">&#39;cpu&#39;</span>)</span>
<span id="cb523-5"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-5" aria-hidden="true" tabindex="-1"></a><span class="co"># device = torch.device(&#39;cuda&#39;)  # Uncomment this to run on GPU</span></span>
<span id="cb523-6"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-6" aria-hidden="true" tabindex="-1"></a><span class="fu">invisible</span>(torch<span class="sc">$</span><span class="fu">manual_seed</span>(<span class="dv">0</span>))</span></code></pre></div>
<ul>
<li><code>N</code> is batch size;</li>
<li><code>D_in</code> is input dimension;</li>
<li><code>H</code> is hidden dimension;</li>
<li><code>D_out</code> is output dimension.</li>
</ul>
</div>
<div id="dataset" class="section level3" number="12.4.2">
<h3><span class="header-section-number">12.4.2</span> Dataset</h3>
<p>We will create a random dataset for a <strong>two layer neural network</strong>.</p>
<div class="sourceCode" id="cb524"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb524-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb524-1" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> 64L; D_in <span class="ot">&lt;-</span> 1000L; H <span class="ot">&lt;-</span> 100L; D_out <span class="ot">&lt;-</span> 10L</span>
<span id="cb524-2"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb524-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb524-3"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb524-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create random Tensors to hold inputs and outputs</span></span>
<span id="cb524-4"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb524-4" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">randn</span>(N, D_in, <span class="at">device=</span>device)</span>
<span id="cb524-5"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb524-5" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">randn</span>(N, D_out, <span class="at">device=</span>device)</span>
<span id="cb524-6"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb524-6" aria-hidden="true" tabindex="-1"></a><span class="co"># dimensions of both tensors</span></span>
<span id="cb524-7"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb524-7" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(x)</span>
<span id="cb524-8"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb524-8" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(y)</span></code></pre></div>
<pre><code>#&gt; [1]   64 1000
#&gt; [1] 64 10</code></pre>
</div>
<div id="initialize-the-weights" class="section level3" number="12.4.3">
<h3><span class="header-section-number">12.4.3</span> Initialize the weights</h3>
<div class="sourceCode" id="cb526"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb526-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb526-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Randomly initialize weights</span></span>
<span id="cb526-2"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb526-2" aria-hidden="true" tabindex="-1"></a>w1 <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">randn</span>(D_in, H, <span class="at">device=</span>device)   <span class="co"># layer 1</span></span>
<span id="cb526-3"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb526-3" aria-hidden="true" tabindex="-1"></a>w2 <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">randn</span>(H, D_out, <span class="at">device=</span>device)  <span class="co"># layer 2</span></span>
<span id="cb526-4"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb526-4" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(w1)</span>
<span id="cb526-5"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb526-5" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(w2)</span></code></pre></div>
<pre><code>#&gt; [1] 1000  100
#&gt; [1] 100  10</code></pre>
</div>
<div id="iterate-through-the-dataset" class="section level3" number="12.4.4">
<h3><span class="header-section-number">12.4.4</span> Iterate through the dataset</h3>
<p>Now, we are going to train our neural network on the <code>training</code> dataset. The equestion is: <em>“how many times do we have to expose the training data to the algorithm?”</em> By looking at the graph of the loss we may get an idea when we should stop.</p>
<div id="iterate-50-times" class="section level4" number="12.4.4.1">
<h4><span class="header-section-number">12.4.4.1</span> Iterate 50 times</h4>
<p>Let’s say that for the sake of time we select to run only 50 iterations of the loop doing the training.</p>
<div class="sourceCode" id="cb528"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb528-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-1" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="ot">=</span> <span class="fl">1e-6</span></span>
<span id="cb528-2"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb528-3"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-3" aria-hidden="true" tabindex="-1"></a><span class="co"># loop</span></span>
<span id="cb528-4"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>) {</span>
<span id="cb528-5"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Forward pass: compute predicted y, y_pred</span></span>
<span id="cb528-6"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-6" aria-hidden="true" tabindex="-1"></a>  h <span class="ot">&lt;-</span> x<span class="sc">$</span><span class="fu">mm</span>(w1)              <span class="co"># matrix multiplication, x*w1</span></span>
<span id="cb528-7"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-7" aria-hidden="true" tabindex="-1"></a>  h_relu <span class="ot">&lt;-</span> h<span class="sc">$</span><span class="fu">clamp</span>(<span class="at">min=</span><span class="dv">0</span>)   <span class="co"># make elements greater than zero</span></span>
<span id="cb528-8"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-8" aria-hidden="true" tabindex="-1"></a>  y_pred <span class="ot">&lt;-</span> h_relu<span class="sc">$</span><span class="fu">mm</span>(w2)    <span class="co"># matrix multiplication, h_relu*w2</span></span>
<span id="cb528-9"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb528-10"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor</span></span>
<span id="cb528-11"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># of shape (); we can get its value as a Python number with loss.item().</span></span>
<span id="cb528-12"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-12" aria-hidden="true" tabindex="-1"></a>  loss <span class="ot">&lt;-</span> (torch<span class="sc">$</span><span class="fu">sub</span>(y_pred, y))<span class="sc">$</span><span class="fu">pow</span>(<span class="dv">2</span>)<span class="sc">$</span><span class="fu">sum</span>()   <span class="co"># sum((y_pred-y)^2)</span></span>
<span id="cb528-13"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-13" aria-hidden="true" tabindex="-1"></a>  <span class="co"># cat(t, &quot;\t&quot;)</span></span>
<span id="cb528-14"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># cat(loss$item(), &quot;\n&quot;)</span></span>
<span id="cb528-15"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb528-16"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span>
<span id="cb528-17"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-17" aria-hidden="true" tabindex="-1"></a>  grad_y_pred <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">mul</span>(torch<span class="sc">$</span><span class="fu">scalar_tensor</span>(<span class="fl">2.0</span>), torch<span class="sc">$</span><span class="fu">sub</span>(y_pred, y))</span>
<span id="cb528-18"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-18" aria-hidden="true" tabindex="-1"></a>  grad_w2 <span class="ot">&lt;-</span> h_relu<span class="sc">$</span><span class="fu">t</span>()<span class="sc">$</span><span class="fu">mm</span>(grad_y_pred)        <span class="co"># compute gradient of w2</span></span>
<span id="cb528-19"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-19" aria-hidden="true" tabindex="-1"></a>  grad_h_relu <span class="ot">&lt;-</span> grad_y_pred<span class="sc">$</span><span class="fu">mm</span>(w2<span class="sc">$</span><span class="fu">t</span>())</span>
<span id="cb528-20"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-20" aria-hidden="true" tabindex="-1"></a>  grad_h <span class="ot">&lt;-</span> grad_h_relu<span class="sc">$</span><span class="fu">clone</span>()</span>
<span id="cb528-21"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-21" aria-hidden="true" tabindex="-1"></a>  mask <span class="ot">&lt;-</span> grad_h<span class="sc">$</span><span class="fu">lt</span>(<span class="dv">0</span>)                         <span class="co"># filter values lower than zero </span></span>
<span id="cb528-22"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-22" aria-hidden="true" tabindex="-1"></a>  torch<span class="sc">$</span><span class="fu">masked_select</span>(grad_h, mask)<span class="sc">$</span><span class="fu">fill_</span>(<span class="fl">0.0</span>) <span class="co"># make them equal to zero</span></span>
<span id="cb528-23"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-23" aria-hidden="true" tabindex="-1"></a>  grad_w1 <span class="ot">&lt;-</span> x<span class="sc">$</span><span class="fu">t</span>()<span class="sc">$</span><span class="fu">mm</span>(grad_h)                  <span class="co"># compute gradient of w1</span></span>
<span id="cb528-24"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-24" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb528-25"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-25" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Update weights using gradient descent</span></span>
<span id="cb528-26"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-26" aria-hidden="true" tabindex="-1"></a>  w1 <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">sub</span>(w1, torch<span class="sc">$</span><span class="fu">mul</span>(learning_rate, grad_w1))</span>
<span id="cb528-27"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-27" aria-hidden="true" tabindex="-1"></a>  w2 <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">sub</span>(w2, torch<span class="sc">$</span><span class="fu">mul</span>(learning_rate, grad_w2))</span>
<span id="cb528-28"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-28" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb528-29"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-29" aria-hidden="true" tabindex="-1"></a><span class="co"># y vs predicted y</span></span>
<span id="cb528-30"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-30" aria-hidden="true" tabindex="-1"></a>df_50 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">y =</span> y<span class="sc">$</span><span class="fu">flatten</span>()<span class="sc">$</span><span class="fu">numpy</span>(), </span>
<span id="cb528-31"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-31" aria-hidden="true" tabindex="-1"></a>                    <span class="at">y_pred =</span> y_pred<span class="sc">$</span><span class="fu">flatten</span>()<span class="sc">$</span><span class="fu">numpy</span>(), <span class="at">iter =</span> <span class="dv">50</span>)</span>
<span id="cb528-32"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb528-33"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-33" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df_50, <span class="fu">aes</span>(<span class="at">x =</span> y, <span class="at">y =</span> y_pred)) <span class="sc">+</span></span>
<span id="cb528-34"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-34" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="rtorch-minimal-book_files/figure-html/run-model-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We see a lot of dispersion between the predicted values, <span class="math inline">\(y_{pred}\)</span> and the real values, <span class="math inline">\(y\)</span>. We are far from our goal.</p>
<p>Let’s take a look at the dataframe:</p>
<div class="sourceCode" id="cb529"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb529-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&#39;DT&#39;</span>)</span>
<span id="cb529-2"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-2" aria-hidden="true" tabindex="-1"></a><span class="fu">datatable</span>(df_50, <span class="at">options =</span> <span class="fu">list</span>(<span class="at">pageLength =</span> <span class="dv">10</span>))</span></code></pre></div>
<div id="htmlwidget-1202f61defa40973a1ae" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-1202f61defa40973a1ae">{"x":{"filter":"none","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153","154","155","156","157","158","159","160","161","162","163","164","165","166","167","168","169","170","171","172","173","174","175","176","177","178","179","180","181","182","183","184","185","186","187","188","189","190","191","192","193","194","195","196","197","198","199","200","201","202","203","204","205","206","207","208","209","210","211","212","213","214","215","216","217","218","219","220","221","222","223","224","225","226","227","228","229","230","231","232","233","234","235","236","237","238","239","240","241","242","243","244","245","246","247","248","249","250","251","252","253","254","255","256","257","258","259","260","261","262","263","264","265","266","267","268","269","270","271","272","273","274","275","276","277","278","279","280","281","282","283","284","285","286","287","288","289","290","291","292","293","294","295","296","297","298","299","300","301","302","303","304","305","306","307","308","309","310","311","312","313","314","315","316","317","318","319","320","321","322","323","324","325","326","327","328","329","330","331","332","333","334","335","336","337","338","339","340","341","342","343","344","345","346","347","348","349","350","351","352","353","354","355","356","357","358","359","360","361","362","363","364","365","366","367","368","369","370","371","372","373","374","375","376","377","378","379","380","381","382","383","384","385","386","387","388","389","390","391","392","393","394","395","396","397","398","399","400","401","402","403","404","405","406","407","408","409","410","411","412","413","414","415","416","417","418","419","420","421","422","423","424","425","426","427","428","429","430","431","432","433","434","435","436","437","438","439","440","441","442","443","444","445","446","447","448","449","450","451","452","453","454","455","456","457","458","459","460","461","462","463","464","465","466","467","468","469","470","471","472","473","474","475","476","477","478","479","480","481","482","483","484","485","486","487","488","489","490","491","492","493","494","495","496","497","498","499","500","501","502","503","504","505","506","507","508","509","510","511","512","513","514","515","516","517","518","519","520","521","522","523","524","525","526","527","528","529","530","531","532","533","534","535","536","537","538","539","540","541","542","543","544","545","546","547","548","549","550","551","552","553","554","555","556","557","558","559","560","561","562","563","564","565","566","567","568","569","570","571","572","573","574","575","576","577","578","579","580","581","582","583","584","585","586","587","588","589","590","591","592","593","594","595","596","597","598","599","600","601","602","603","604","605","606","607","608","609","610","611","612","613","614","615","616","617","618","619","620","621","622","623","624","625","626","627","628","629","630","631","632","633","634","635","636","637","638","639","640"],[0.83994026831249,-0.00261469246496478,1.17331508976272,0.113064550776486,0.0103083101232198,0.741725300575136,0.757130556967624,-1.19798639091765,0.977469466953348,0.958784411472249,0.0208831428895932,-1.64816158891565,-0.0500028243495128,0.733813527145886,1.58085339989783,1.37525327872787,-0.386739371622718,-0.360212335648884,0.757263768677633,-2.33097748052543,-0.280231384151112,-0.958825812111196,-0.0248397757648495,1.0914318283759,-0.458192050288552,1.88579895675686,1.8374918101401,-0.102590503582433,0.158376834397865,-1.69505820821086,0.0630930916536182,1.3418620933296,0.173417858163622,-0.972149178523607,-0.312413823093578,2.16446071258402,-1.08569798526669,-0.570029361135791,0.180937256764405,-1.26229551939497,-1.36950465873129,-1.15493391741737,1.40785825831085,0.781023621423221,0.0780186960888742,2.11273169799433,-0.382805598728771,-0.332249209115966,0.833881240069712,0.59587340242551,-1.10900749622674,0.768900089169295,-1.35001396009045,0.483618658476077,-0.286807646207213,0.141828938278373,-0.374918514638461,-0.231434565088581,-0.00992419102991136,-2.11812756250465,-0.523762933650836,-1.32458158157352,0.431681037219444,0.925057143386304,0.93777525245556,0.546488768273407,1.11596946687424,0.570940155822875,-1.05140854030057,-0.318810634218592,-0.289448897409276,-0.418406412091964,0.781141791150043,-0.114891413907268,-0.845416526254121,-0.91845936893655,1.20789854386747,0.741312355103571,0.634427251618938,2.20585574787314,0.356161490453256,0.267242739519324,0.484831528360182,-0.165738361704601,0.189930678043718,-0.93651989042175,-1.45410391289995,0.233684800337273,0.502157215971716,0.28920149072855,-1.22551276737874,-0.385846452553303,1.42231442741027,-1.7116822994054,0.155053435413657,1.75487195590539,-0.480695737266194,-0.744335584259619,-2.13245053322139,-0.364322343399947,-0.233973774563064,0.111681118770621,-0.88914370304214,-0.722085375742162,-2.32991274785241,-0.479068930295669,0.789931238294979,1.06380136135981,-0.0798134682221333,-0.374168001227348,0.122704935131071,-1.43690716269235,-0.853944502445023,0.456289787735743,-0.0142516296486563,0.816751922214445,0.0849881110641102,0.446428132382165,2.42338976222122,-0.246995607668513,0.276176450241468,0.930116509469416,-1.02544718711831,0.104887979909664,-0.0142328430202254,2.03937753638296,0.373422932290684,1.43684984339667,0.948776626114924,-1.31827952307809,0.292905851185605,0.147308597580878,-1.03373092754513,0.419607277988786,-0.261092633509276,1.6071495149865,-0.647906145700861,0.657156199635267,-0.57098265859795,-0.818600322951857,1.31069089175351,1.81375842080092,-0.943433957369921,0.857269379202826,-0.263283789177259,1.31296346802011,-0.955590491841724,-1.56443546642085,-2.17226562414852,1.45591099037169,1.03814194129823,-0.197772383407226,-0.650297223745001,-1.33005360476313,0.921942504799078,0.752209085763665,-0.0361295533712819,2.63443405115608,0.287654631188146,0.655186314323695,0.400648171873719,-0.307311020383442,-0.0880147158656964,-1.41821667150147,-0.313515957169452,1.02977246630598,1.63557919370632,0.471406593501004,0.996474278664091,0.682501811766753,-0.971734007624617,-0.654540506542262,1.99500285811451,-1.64399263553111,-0.663557865235524,-1.68572596581282,-0.00487618116822535,0.458801236543731,0.5824022309138,-1.36287855122539,-1.27186820648675,0.714902868782501,-1.0165666698022,-0.0235973711331876,-0.0710369673541219,0.142253138818817,-0.675694865931503,-0.256829936236055,-0.101224481444322,-2.15395839517849,-0.964817551233336,-0.58900570028774,-1.77786400328839,1.14147095872602,-0.32772078240833,-1.35670287649227,0.415691882771873,-0.925932387905733,-1.61418322141891,-0.0867796005444939,0.601846649732015,1.12863379512717,0.184978687339329,0.156848912253547,-0.484058595314498,1.12522934475228,0.243459583230952,-0.634978878232694,-1.25090253023824,-1.43563909830578,1.13586385748573,0.215411392403429,1.86239774513426,1.11499996333691,-0.674626192034018,-0.434131329001274,1.03699553226858,-0.0366741372856878,-0.402271366973104,2.10246137335328,0.459018936796291,-0.25503953202401,-0.672610058718924,1.09980827026172,-0.410925835278326,-0.896790320270794,-1.03109344053961,-0.262748015975908,-0.333069074621615,0.977294210489194,0.678715733607743,1.25873526914719,0.42400531072313,1.79340828861104,0.294284737340495,-1.51557096175814,-0.566556739572426,0.0695601296483909,1.20367672097015,-0.549833339055226,0.120064475138402,0.307536795611604,1.21556005041361,0.12602924786391,-1.47214013475447,-0.318687822897892,0.279546357353257,2.70523892501022,1.68587064051207,1.1239941108417,0.424088292374247,0.104431077381569,-1.18021245837732,-0.916706926774142,-1.56221497392067,-0.851852947077733,-1.11863086095433,0.148312190622533,0.502631432428189,1.58774800017961,-1.10166271639973,-0.777449867610656,0.942870568479463,-1.9339574431339,0.00698050744702491,0.673684269774716,-1.39732869525216,0.283501034389694,0.874755970337202,-0.417645129576752,-1.36874292450096,-0.0747812970509281,0.0105724695173432,-0.0456309112554566,-1.47339841259021,1.33700556433092,1.94209421947211,0.284457070328279,-1.98482664413377,-0.50080737939018,-0.274676881900125,0.710938213804541,2.25851536328219,0.618237686663071,0.230927765626917,1.10122470908834,-0.572603422146998,-0.53302471140162,-0.717679874784871,-0.877038791682033,-0.748485888179382,0.184771684002625,-0.22231743531996,-1.00929695552901,-1.37047063999704,-1.07025776684948,-0.0066305826821924,0.00463777722911298,0.200669539977365,0.809383668396162,0.287278665935646,-0.303985246895298,0.139672450093511,0.127722862850529,-0.95565825905712,-0.799909006767028,-0.202940490973971,-2.41551300196281,0.3750531988884,2.0938362702894,0.605430623566889,-0.816263936823711,1.4935718662945,2.15379218446882,1.28063423391778,-1.16859150067533,-0.495931078057793,-1.12572347427127,-0.0289018780637711,-0.491217346285856,0.582465462265955,-1.73320910486715,-0.250616430567373,1.37773646638411,0.505183750595998,1.03329001588251,-0.148129803407604,-1.02024054280369,-1.16747702799092,-0.698262649012394,-0.29088420462642,-0.0873481684419898,2.28367061778488,0.799274120229441,0.496859660507907,-1.16771232655097,0.0560084744790094,0.0697664131694416,-0.63172486335937,-1.61579250963407,0.822541288820032,-0.585096534350738,-0.316981493648855,-0.445729843878749,-2.50838624530302,-1.39899463359675,-1.31102892051884,0.750402144358772,0.703404121659897,-0.25083983221813,-0.25095095379119,-0.872342684631607,0.628192536989848,-0.936217675649455,-0.550371466231215,-0.26970983693769,-0.688604841006888,-0.390605966140334,1.06988684039523,0.498849800595456,0.333419121051892,0.910556713584374,-1.27892253213922,-1.84182185333004,-0.512035156980371,-0.364119099300724,0.459286557995962,-0.0437559190006266,-0.453340737020902,2.02055656551793,1.47116567306761,-0.0115038122635589,0.131722917926454,-2.28429446135232,0.669356739346067,-1.14593193034066,-1.91291968267857,-1.67212741707599,0.54073309384294,-0.256724509418031,-1.89134380794797,0.161915204911539,-0.458809181262073,-0.194078077908355,0.296793193074685,-0.0250807732722297,-1.05794861180844,0.24484382118802,-0.648883034347149,0.618876699768757,1.44663045545316,1.76518871577172,-1.92334252201028,0.202212741132606,-0.342486128124399,-0.119772048017177,0.266562321266478,0.722029306985646,0.403631540478367,-1.9875531289521,-0.781171921052648,0.987488573315664,0.308690446320804,0.106148749253679,0.185085607568157,-0.390160876288962,1.25429513802065,-1.35013386465017,-0.342974766571647,-0.232894537629491,0.980332519980097,-2.11322741735,0.23024995771627,-1.45705269393795,1.38625292848739,1.6457152752694,0.886358263746316,0.296335125360452,-0.442944664529715,0.274627958703436,1.37404495680148,-1.61754075111507,-1.01356309656133,0.34151499813101,0.60561927302074,0.466358958410998,1.18788000370773,-0.998222009747409,0.472411048704897,0.802363134564957,0.778863518671328,1.33312992214265,-0.11483784787192,-1.24014458172761,1.68097845650274,-0.156295579276488,-0.36167688476951,1.12990338986994,0.544741774675208,-0.739519141431302,0.348344512782951,-0.10457968657704,-1.54862790298088,0.991859873430099,-0.712951101315977,-0.688713554688238,-0.019645395217261,-0.4276668324237,0.485554217337533,-1.22769785113341,-0.0839624525226322,0.929646496319533,-1.04399669558851,1.5862211735349,-0.762919277781499,-0.0482214954780706,0.482863741201961,0.272647805689435,-0.529870425645623,-2.32824908375099,0.0361073596021704,-0.451810115323141,-0.282254053923175,1.54911580277808,0.253374436546217,2.1590870111341,1.00793354498743,1.2238963859792,-0.262656713944417,0.371188568837448,1.84975076615656,-0.32057748245843,-0.484757190293574,0.119115296354525,0.311425735299618,2.86340591290978,0.561681683625529,-1.36773128004884,-0.521412125803238,1.8438836271771,0.15291957051073,-1.32938108961612,1.12345625118363,-0.0988379062841592,-0.781398448382246,1.13052436749178,0.767681931298354,0.525356584083132,0.582232899579348,0.333066984200484,-0.806258638926797,0.556552894435339,-1.30665318755998,0.785186326876701,-0.394106214647196,0.550730858543516,0.774230360594257,1.25782444024924,-1.59645579014647,0.726215347670778,-0.63694860577194,1.27493610125473,-0.0352439186929309,2.89168261447142,1.37501515427315,-0.0491456451207457,-0.0546139127595145,0.260510410587632,0.981880273079814,0.14277683851885,-0.254490023068397,-1.08396906594294,0.271948629915079,0.377547175371029,0.981224651122629,1.80754059091588,-1.1073823023655,0.482139414365283,1.8188538137154,-0.58749687491936,1.1093609884837,-0.297608176884006,2.57784620418014,2.52204599693865,-0.384840198277502,-0.855562334499144,0.938512269689899,-0.468196363866475,0.260008292451711,-0.462982716485726,-0.886275855262118,1.12411879590505,-0.909935517062699,-0.751334836930073,0.514366756631972,0.195860128347231,1.55471787782953,-0.253467892483979,-0.944270533093175,0.0655037556538774,0.669046506691992,-0.329793629158233,1.30135750702321,1.40114405750616,0.796269800703752,-0.10853585230126,1.29063556898416,-0.693149828347284,1.15340154198528,-1.67650319533252,-1.49429593747768,-1.273469908539,1.31161289723836,-1.31181443286064,-0.950226359402931,0.144663430311268,-1.37563897316271,-0.229895425421889,0.419692024617746,0.516239563506011,0.740669495728343,-0.361952001881317,-1.60995844987436,0.327757025857471,1.6432950871785,1.20535256388868,-0.353510280844418,1.21473389654699,-1.53240226188522,1.92233286176372,-1.09253954985412,-1.59184155325035,-0.64646780319668,-0.426883748541837,0.209569862271875,0.0415756254512372,1.33238165750511,-0.681027400604152,0.620571329940411,-0.900045782913094,0.309651318876625,0.118175407084332,0.227812270228395,-1.18930438569969,0.0837636991730383,0.588009163095429,-1.5092271780105,0.995898635439402,-0.00639701815793872,-0.3502219106773,1.4288096764155,-0.372525850460849,-1.86657115947184,1.73533502830238,1.4419101995773,-0.0083570684921407,1.3092622868551,0.000392255402139403,-1.45683485419048,-0.844996213277658,-0.60477119564568,-0.0517937465881769,0.821273104631407,1.50318458611856,0.26033919453097,0.521628843921831,-0.525259906808531,-1.07777898882352,-0.340595705086819,-0.0688224810312909,0.024696280077251,1.76093398424226,0.175448866859079,0.138406693673293,-0.206063763800319,-0.435211393481228,0.529893499013784,0.967447102676697,-2.43106301578848,-0.0424933305199116,-1.99868317055989,-2.47557651859656,-0.965041073936391,-0.435050040400478,-0.507170912986636,-1.37604070291559,0.641342899780833,0.15067215935219,0.647532140383024,-1.21460078326391,1.46931171546606,-1.78034791140857,0.39149639637274,0.953942227218803,1.27980367071278,0.263325295304111,-1.03877191179501,-0.962563912956559,0.895371173457089,0.905913690706474],[-10.5647346894392,2.14915615578878,-8.13878442490383,-4.29308261348075,18.7224487868664,-0.849544993526873,2.80821304968644,-5.75869401904614,-15.5218068640173,3.03253908015009,0.12208291407669,-0.682878496541912,1.47662687970814,2.02538209822391,-1.42475662127592,-3.64222107442094,-0.0479609782905204,3.82343746416699,0.331072344632494,-7.41052874763599,4.50057912243374,0.133744550210169,-7.29968383960053,9.26743294691356,6.2409251121433,12.2589044561664,7.74282427061568,-3.96793463676747,5.19211860796664,-3.9678154464228,-7.97965002478975,3.36402792540625,0.884761925216946,-5.40058636901499,-0.0479572253917322,-11.856648839236,-4.51544671436274,6.70621346037413,-7.31497735022544,-4.39119607543416,-3.1822296959924,-3.40679178148787,-0.0294178323845893,4.49708976822961,6.33326553759514,0.254081703518634,-1.32148076120274,-1.20271979838311,-0.420883919363573,-3.76567644525061,3.55745621404241,4.79904769555979,0.983768487876174,11.0026902089051,-0.390108288909807,4.81433416707626,3.22117255366378,2.76960676328512,6.84981910148885,-0.388398738546325,-1.9737229365466,0.865936092025589,-1.8735789817845,-3.55140389294761,-4.84289985786919,-3.44725694199528,-2.1388491783638,4.42555744960982,-5.07797657137648,0.98518811278661,-3.52038121367892,-3.10394789006459,-2.77798211871035,-2.48415213217625,3.85623290247929,-4.62318696085968,4.59707710105077,0.343802613160257,-6.07772649610575,-0.697945527634279,2.81813612035605,-0.175952372494794,6.60179181865755,0.549551618986554,-3.94670039989044,-2.48891427937561,-1.3639280875938,1.87370351572564,4.1374809129657,-0.58805841557395,3.14754737951854,0.355022965514352,2.56658801513892,-0.133226063851472,-2.63413903493152,2.60140491856357,-0.0166972456695336,0.190204239561668,1.28258135744867,-1.80749571012066,-0.23760578352901,-1.48399400310458,-7.74427376379467,4.2296515157751,-6.79185151630956,-0.888192281547331,2.75388214576451,3.20565688576109,1.82611606646609,-8.87640523350379,0.914811852030563,0.615504051122911,-3.21354354196445,1.21178678107376,2.24570921065792,5.47434741525281,0.274403423960898,-2.87590404098156,6.52334607198622,1.97668092081629,-4.32846075182534,1.82717418196645,-6.75576525721646,-1.22151168179277,4.83285464724795,5.57784285991976,2.62419181930333,-1.12196838121008,-3.77886753275603,-1.78234690001134,0.409638174199825,1.62896278228013,5.92507717373842,-4.52698431976125,-5.68188942123089,-1.13551998338499,-0.931547945076743,2.5413964681327,-5.95814314205395,2.74887086907446,-2.43890337283552,4.13479808509122,-0.634958561371338,-3.49197690853556,-5.55880328058889,-0.902115018759614,-4.17678880498892,0.549504035957216,-3.66754812149755,6.56959791964703,1.57769996861389,-2.45006272741546,-1.78110033990458,-3.07943363162383,0.578443635277252,3.60043936925105,2.51052145214168,-0.837160496163038,-0.0811165176798593,-0.75451534243691,1.58830709710476,-0.504811152644071,-0.973361540982849,1.33341212306965,-0.569854206579797,3.96363740878082,-0.981422860074988,-0.76055951411096,4.27793204685182,-0.983127710396999,-1.00104787799583,-2.37589082267636,3.10981769825673,-3.65200035173796,-0.386185229199626,-5.37817164054695,-1.99042917871823,0.251143427522729,0.903769470383397,-0.885559456761257,0.52588460474927,-4.97205336888223,1.74066926801559,-0.601951957502115,6.8059730318033,1.6209063464245,0.77292200259544,-3.02586723532086,3.01522759616782,-2.63034673340535,-1.01465711612143,-1.62037261798777,5.41952599917787,5.2055820087725,-3.49743062967806,-8.27725109277562,-0.555283399521489,2.77718234285967,-0.329297476235054,-1.05584791224422,-2.4638528494512,3.93042450816321,-2.01211303356732,0.659077491264246,8.94285129267826,7.94719742351824,0.120119002324576,-7.59400275355757,-0.918623068573618,7.19247720295039,-3.3322020089621,-1.18517306594297,-10.9575240416156,3.42572208373327,2.21842112728344,-2.64859859884604,2.99611186275182,1.24150732708718,-5.60310772912981,-4.02950620482672,-1.68263789331664,-3.24994254592961,7.0004386958002,-7.35454941990313,-1.15399524804533,-4.8575302070098,-13.6549646631358,0.414775154899286,-1.04969492887527,21.7153171933749,-4.88290809624897,0.0759665127996446,5.80174844139885,-4.54364028006906,-6.64873020409231,-13.2067813359279,-1.21176178820895,2.40210018268533,-2.86992822833956,-8.9759894914075,-0.889525326468447,0.164773275392312,3.84657764376355,-4.54672533726899,-5.7905691581801,3.10085622862953,-3.39866743910152,1.28349436046648,2.09424105712014,4.02623340927795,8.10159951146792,2.28107293057077,-0.9858417351586,3.45510314053269,-6.38717501488343,3.49004534297851,-3.28367375627249,-3.22035982563879,7.41923952015791,-4.8005218620006,-0.877032891175935,-2.32033020025328,0.533026666298909,-5.22162160062949,-2.44067366671025,0.384039326880277,-1.10360136532637,-1.57017690290853,4.0745310172783,1.84262576619828,-1.15855594100589,4.48023054586641,18.1977139381723,-1.00185720652124,-12.6945310125567,2.07707114881308,-0.875595330005115,3.15164887454054,2.88660848009031,8.1938706299576,-4.37297923315364,2.82175835387865,1.13078399602532,-11.4375273536284,6.70878489214175,-0.483111294345787,0.738043003873835,-3.06616101303153,-14.7326721596788,-1.56112793472298,0.45628901818855,2.53354525436803,0.235940355583324,-1.02794545821845,-0.0855839650579699,7.22388363476023,-1.5906161757559,-2.95916105165468,4.38960876128725,10.2421798603603,0.622702942101624,-0.784336754841809,-3.77915787593709,0.61767707845341,4.63279154356186,8.60025835161539,1.11249295859062,-6.53231082616366,-0.15195644296281,7.77389656639367,0.86700072081244,-1.22335441627333,0.257466546413184,1.27319286462793,-0.888491950654449,0.504213863017712,-2.45298272474374,-0.159555002744842,0.81761570698593,2.0373494832247,0.320679832856785,0.482221586793945,7.51274712936285,8.16851627730282,5.54871855718085,-1.59853231039589,2.55837367203844,0.224364552929637,-0.879919748317902,-5.81594461603329,1.64399861772356,-3.88086499314091,0.978070124623124,4.69301546980802,4.84739779040023,-0.762798977048185,2.56885317255673,-2.89103719854485,0.818448396301838,-6.02822660816831,-0.787592782402622,-3.57644326961283,-1.76792716583951,-7.16400876418419,-1.56950182556556,0.32714102087355,-0.845397469057363,-0.322068670634321,-2.85305739162323,2.24055975074787,5.78503828200623,1.33939871937795,-1.04017531000221,8.58107880936378,-1.34299166081895,0.881461153519747,-3.17723213900734,3.16711881563158,10.2188789968477,-3.93632258826478,2.94906323245679,-1.03459880624077,4.21688678202778,-3.93875942437207,-3.10375756443253,-1.03470696147215,1.13176758992514,-1.25301544253043,0.59033844266844,2.3284475743433,2.38638033653474,-3.60184951871633,-3.01346559475036,-0.787742542125834,5.62476908877586,2.54072674316038,-0.741754649005681,-5.41974101112274,2.19803532039809,-0.318933079402901,-6.58448188587101,8.13901240188152,-1.27371300704183,-4.15045139533834,9.16260315678036,1.17535286432748,0.794699389118416,1.13876656192517,-11.7669550241596,9.07034805021099,0.767704972406227,2.03723958696924,-12.1924501463971,3.39097065207973,8.7152128303918,4.01848704599316,1.481121154432,-1.17783104031195,-3.11132426039528,-2.41808527213207,-4.58339910452202,-2.53626153614468,2.18120189205285,-5.43038098670462,-1.4205063040007,-9.13309833021966,0.874916603116266,-0.110764133933982,-7.28822708621123,-7.28563050370704,-1.36658321486234,-4.49985611509412,-4.47924660149268,-1.20056385295784,-4.05667098788998,-2.81449841268563,5.27565340249848,-0.721796814797841,-1.95446111571467,-15.6985294926973,1.11604384441125,0.682012790799597,3.58890947841159,-4.63705615828189,7.9344258539902,3.71588957709049,4.12855963665035,-4.90937566598984,-3.01263353230257,10.1801201364225,2.23771097623495,-2.2039905933656,-3.33282081758109,-0.479262581536191,8.04524639241576,3.19648118178926,-3.84424902966615,-2.21623689292077,2.5455238688531,3.80296980478867,1.39526298387243,-1.1477162855127,-7.34828624672576,2.88946313064319,-4.00488697057184,1.23668815024415,0.217704739968317,-1.07169437871016,4.60424598036802,-5.92693280290257,-4.79531554774754,1.7566472042615,-9.94305246379388,1.93673218801167,-0.58637348097869,-1.13857815973362,-1.49101931955842,1.33690631538981,-4.09541426151166,-5.01026898060986,-5.43661749056704,-1.03969019951846,9.86320701484043,-2.94119658214153,-2.99404967242441,2.54437623209152,-0.791897734033398,1.31890206921746,-2.12680936533798,11.2047052016521,3.64689080222484,-4.66292696129557,4.09181565277506,-6.61423071664982,-0.921603876063874,2.67451883351161,4.16361608385895,-4.26846402071202,-1.49242454501366,5.33375343560457,4.9874164023585,-1.73711842077085,6.41963324336516,0.986813333473786,-5.1510712150176,4.44930761499274,-0.134971968597764,2.38021565324473,8.08187169367892,4.65424262141463,0.582824082555065,3.79599344780312,1.39766888203411,3.13313885753111,-7.34430870016098,-6.16037552674793,-2.3653101596525,7.46601288580542,-0.724742625069621,-3.95130507919675,-2.08592361873716,-2.26794401350618,-1.43116135019092,2.81342857309378,5.49378243133167,3.64665959197612,-2.74441427723778,-3.05722972077734,-3.42344479352914,2.69937472237573,1.90712357999944,0.335149041838759,3.30124907575723,0.963576868746347,-1.53054810596661,1.88641730792098,2.71583312984361,1.60552037532099,0.921300149179597,-3.06383379687749,3.88897487784444,2.4998756482758,-2.4739205559921,4.21777776945804,-4.69615220572379,3.7978201907412,1.42970766696383,-1.75523772809252,3.82890101642392,-2.10939939874903,-1.48852025720068,-0.754166735891441,5.595840258927,-7.54812733029472,-7.98203917285935,-7.709961024943,-1.75391446732352,2.85261662824854,0.69379515359469,-2.86657477684302,0.00528322474133768,0.540281353378496,4.49560390039255,3.36356376015965,-0.415645888371135,1.2437843144467,-2.61434390679435,-0.810739008343646,1.99343767843715,2.56789592693397,0.648464740497748,-0.609329204802553,-0.584699288813544,3.2796089286661,-0.405557211149326,-0.0367839747937615,-3.84015657089592,2.97608831345094,3.84188911737506,3.72918618025742,1.37149998731959,-0.81462666600823,-2.8635838375993,2.46887365644719,3.48832610459379,4.11074109702152,1.54064235191556,0.242194508600522,-3.84780184737965,0.899217294263611,2.54422998277999,-4.8120951186465,-1.81430200872243,4.70462108222735,0.147240294028547,3.79132183902008,3.04769990554746,-1.39992715282931,7.39250748383584,0.146438988699361,0.194322660054559,0.513096720145646,1.62233359664949,-0.60577795132522,0.891003093554393,2.33239551081241,-0.484728627625303,-1.08497701252786,0.64829199621887,1.76121589728927,-0.636029319320797,-5.2191910346412,-7.67208444617577,2.9128743825879,9.61518276707234,-2.32743536038674,1.8843326217148,-1.25478097789868,-5.50258513815263,-6.1059133730463,1.05560041282271,2.91855497800767,-5.95523215016053,8.61172991130797,-0.792804641443542,2.13504168706003,-0.28318722662086,1.04758022729354,4.06266529501999,-4.84142830710625,6.93054770421274,3.27858089307198,8.59646781758217,-3.51902570767891,-8.15109988115041,-6.21900715881126,4.77336080053183,4.04152801708858,0.218553463104701,-0.808888814760882,3.62707111715758,7.07899453740502,1.46273016425255,9.94527785752359,-11.3970722666504,2.91701155593918,0.326476875594684,6.77990222601823,14.3634908511184,2.15265044103146,-1.46370632292113,-1.54965456505616,-3.06496741913042,-1.85931115890063,2.48229955728174,-4.55548811739351,-1.04130411945145,2.94409719818542,-4.06281046113364,-2.10779797497321],[50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>y<\/th>\n      <th>y_pred<\/th>\n      <th>iter<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"columnDefs":[{"className":"dt-right","targets":[1,2,3]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
</div>
<div id="a-function-to-train-the-neural-network" class="section level4" number="12.4.4.2">
<h4><span class="header-section-number">12.4.4.2</span> A function to train the neural network</h4>
<p>Now, we convert the script above to a function, so we could reuse it several times. We want to study the effect of the iteration on the performance of the algorithm.</p>
<p>This time we create a function <code>train</code> to input the number of iterations that we want to run:</p>
<div class="sourceCode" id="cb530"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb530-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-1" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="cf">function</span>(iterations) {</span>
<span id="cb530-2"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Randomly initialize weights</span></span>
<span id="cb530-3"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-3" aria-hidden="true" tabindex="-1"></a>    w1 <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">randn</span>(D_in, H, <span class="at">device=</span>device)   <span class="co"># layer 1</span></span>
<span id="cb530-4"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-4" aria-hidden="true" tabindex="-1"></a>    w2 <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">randn</span>(H, D_out, <span class="at">device=</span>device)  <span class="co"># layer 2</span></span>
<span id="cb530-5"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb530-6"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-6" aria-hidden="true" tabindex="-1"></a>    learning_rate <span class="ot">=</span> <span class="fl">1e-6</span></span>
<span id="cb530-7"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># loop</span></span>
<span id="cb530-8"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>iterations) {</span>
<span id="cb530-9"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-9" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Forward pass: compute predicted y</span></span>
<span id="cb530-10"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-10" aria-hidden="true" tabindex="-1"></a>      h <span class="ot">&lt;-</span> x<span class="sc">$</span><span class="fu">mm</span>(w1)</span>
<span id="cb530-11"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-11" aria-hidden="true" tabindex="-1"></a>      h_relu <span class="ot">&lt;-</span> h<span class="sc">$</span><span class="fu">clamp</span>(<span class="at">min=</span><span class="dv">0</span>)</span>
<span id="cb530-12"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-12" aria-hidden="true" tabindex="-1"></a>      y_pred <span class="ot">&lt;-</span> h_relu<span class="sc">$</span><span class="fu">mm</span>(w2)</span>
<span id="cb530-13"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb530-14"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-14" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Compute and print loss; loss is a scalar stored in a PyTorch Tensor</span></span>
<span id="cb530-15"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-15" aria-hidden="true" tabindex="-1"></a>      <span class="co"># of shape (); we can get its value as a Python number with loss.item().</span></span>
<span id="cb530-16"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-16" aria-hidden="true" tabindex="-1"></a>      loss <span class="ot">&lt;-</span> (torch<span class="sc">$</span><span class="fu">sub</span>(y_pred, y))<span class="sc">$</span><span class="fu">pow</span>(<span class="dv">2</span>)<span class="sc">$</span><span class="fu">sum</span>()</span>
<span id="cb530-17"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-17" aria-hidden="true" tabindex="-1"></a>      <span class="co"># cat(t, &quot;\t&quot;); cat(loss$item(), &quot;\n&quot;)</span></span>
<span id="cb530-18"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb530-19"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-19" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span>
<span id="cb530-20"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-20" aria-hidden="true" tabindex="-1"></a>      grad_y_pred <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">mul</span>(torch<span class="sc">$</span><span class="fu">scalar_tensor</span>(<span class="fl">2.0</span>), torch<span class="sc">$</span><span class="fu">sub</span>(y_pred, y))</span>
<span id="cb530-21"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-21" aria-hidden="true" tabindex="-1"></a>      grad_w2 <span class="ot">&lt;-</span> h_relu<span class="sc">$</span><span class="fu">t</span>()<span class="sc">$</span><span class="fu">mm</span>(grad_y_pred)</span>
<span id="cb530-22"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-22" aria-hidden="true" tabindex="-1"></a>      grad_h_relu <span class="ot">&lt;-</span> grad_y_pred<span class="sc">$</span><span class="fu">mm</span>(w2<span class="sc">$</span><span class="fu">t</span>())</span>
<span id="cb530-23"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-23" aria-hidden="true" tabindex="-1"></a>      grad_h <span class="ot">&lt;-</span> grad_h_relu<span class="sc">$</span><span class="fu">clone</span>()</span>
<span id="cb530-24"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-24" aria-hidden="true" tabindex="-1"></a>      mask <span class="ot">&lt;-</span> grad_h<span class="sc">$</span><span class="fu">lt</span>(<span class="dv">0</span>)</span>
<span id="cb530-25"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-25" aria-hidden="true" tabindex="-1"></a>      torch<span class="sc">$</span><span class="fu">masked_select</span>(grad_h, mask)<span class="sc">$</span><span class="fu">fill_</span>(<span class="fl">0.0</span>)</span>
<span id="cb530-26"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-26" aria-hidden="true" tabindex="-1"></a>      grad_w1 <span class="ot">&lt;-</span> x<span class="sc">$</span><span class="fu">t</span>()<span class="sc">$</span><span class="fu">mm</span>(grad_h)</span>
<span id="cb530-27"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-27" aria-hidden="true" tabindex="-1"></a>       </span>
<span id="cb530-28"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-28" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Update weights using gradient descent</span></span>
<span id="cb530-29"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-29" aria-hidden="true" tabindex="-1"></a>      w1 <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">sub</span>(w1, torch<span class="sc">$</span><span class="fu">mul</span>(learning_rate, grad_w1))</span>
<span id="cb530-30"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-30" aria-hidden="true" tabindex="-1"></a>      w2 <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">sub</span>(w2, torch<span class="sc">$</span><span class="fu">mul</span>(learning_rate, grad_w2))</span>
<span id="cb530-31"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-31" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb530-32"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-32" aria-hidden="true" tabindex="-1"></a>    <span class="fu">data.frame</span>(<span class="at">y =</span> y<span class="sc">$</span><span class="fu">flatten</span>()<span class="sc">$</span><span class="fu">numpy</span>(), </span>
<span id="cb530-33"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-33" aria-hidden="true" tabindex="-1"></a>                        <span class="at">y_pred =</span> y_pred<span class="sc">$</span><span class="fu">flatten</span>()<span class="sc">$</span><span class="fu">numpy</span>(), <span class="at">iter =</span> iterations)</span>
<span id="cb530-34"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb530-34" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
</div>
<div id="run-it-at-100-iterations" class="section level4" number="12.4.4.3">
<h4><span class="header-section-number">12.4.4.3</span> Run it at 100 iterations</h4>
<div class="sourceCode" id="cb531"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb531-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb531-1" aria-hidden="true" tabindex="-1"></a><span class="co"># retrieve the results and store them in a dataframe</span></span>
<span id="cb531-2"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb531-2" aria-hidden="true" tabindex="-1"></a>df_100 <span class="ot">&lt;-</span> <span class="fu">train</span>(<span class="at">iterations =</span> <span class="dv">100</span>)</span>
<span id="cb531-3"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb531-3" aria-hidden="true" tabindex="-1"></a><span class="fu">datatable</span>(df_100, <span class="at">options =</span> <span class="fu">list</span>(<span class="at">pageLength =</span> <span class="dv">10</span>))</span>
<span id="cb531-4"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb531-4" aria-hidden="true" tabindex="-1"></a><span class="co"># plot</span></span>
<span id="cb531-5"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb531-5" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df_100, <span class="fu">aes</span>(<span class="at">x =</span> y_pred, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb531-6"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb531-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>()</span></code></pre></div>
<p><div id="htmlwidget-a5e68a2cd2425dc65a1f" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-a5e68a2cd2425dc65a1f">{"x":{"filter":"none","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153","154","155","156","157","158","159","160","161","162","163","164","165","166","167","168","169","170","171","172","173","174","175","176","177","178","179","180","181","182","183","184","185","186","187","188","189","190","191","192","193","194","195","196","197","198","199","200","201","202","203","204","205","206","207","208","209","210","211","212","213","214","215","216","217","218","219","220","221","222","223","224","225","226","227","228","229","230","231","232","233","234","235","236","237","238","239","240","241","242","243","244","245","246","247","248","249","250","251","252","253","254","255","256","257","258","259","260","261","262","263","264","265","266","267","268","269","270","271","272","273","274","275","276","277","278","279","280","281","282","283","284","285","286","287","288","289","290","291","292","293","294","295","296","297","298","299","300","301","302","303","304","305","306","307","308","309","310","311","312","313","314","315","316","317","318","319","320","321","322","323","324","325","326","327","328","329","330","331","332","333","334","335","336","337","338","339","340","341","342","343","344","345","346","347","348","349","350","351","352","353","354","355","356","357","358","359","360","361","362","363","364","365","366","367","368","369","370","371","372","373","374","375","376","377","378","379","380","381","382","383","384","385","386","387","388","389","390","391","392","393","394","395","396","397","398","399","400","401","402","403","404","405","406","407","408","409","410","411","412","413","414","415","416","417","418","419","420","421","422","423","424","425","426","427","428","429","430","431","432","433","434","435","436","437","438","439","440","441","442","443","444","445","446","447","448","449","450","451","452","453","454","455","456","457","458","459","460","461","462","463","464","465","466","467","468","469","470","471","472","473","474","475","476","477","478","479","480","481","482","483","484","485","486","487","488","489","490","491","492","493","494","495","496","497","498","499","500","501","502","503","504","505","506","507","508","509","510","511","512","513","514","515","516","517","518","519","520","521","522","523","524","525","526","527","528","529","530","531","532","533","534","535","536","537","538","539","540","541","542","543","544","545","546","547","548","549","550","551","552","553","554","555","556","557","558","559","560","561","562","563","564","565","566","567","568","569","570","571","572","573","574","575","576","577","578","579","580","581","582","583","584","585","586","587","588","589","590","591","592","593","594","595","596","597","598","599","600","601","602","603","604","605","606","607","608","609","610","611","612","613","614","615","616","617","618","619","620","621","622","623","624","625","626","627","628","629","630","631","632","633","634","635","636","637","638","639","640"],[0.83994026831249,-0.00261469246496478,1.17331508976272,0.113064550776486,0.0103083101232198,0.741725300575136,0.757130556967624,-1.19798639091765,0.977469466953348,0.958784411472249,0.0208831428895932,-1.64816158891565,-0.0500028243495128,0.733813527145886,1.58085339989783,1.37525327872787,-0.386739371622718,-0.360212335648884,0.757263768677633,-2.33097748052543,-0.280231384151112,-0.958825812111196,-0.0248397757648495,1.0914318283759,-0.458192050288552,1.88579895675686,1.8374918101401,-0.102590503582433,0.158376834397865,-1.69505820821086,0.0630930916536182,1.3418620933296,0.173417858163622,-0.972149178523607,-0.312413823093578,2.16446071258402,-1.08569798526669,-0.570029361135791,0.180937256764405,-1.26229551939497,-1.36950465873129,-1.15493391741737,1.40785825831085,0.781023621423221,0.0780186960888742,2.11273169799433,-0.382805598728771,-0.332249209115966,0.833881240069712,0.59587340242551,-1.10900749622674,0.768900089169295,-1.35001396009045,0.483618658476077,-0.286807646207213,0.141828938278373,-0.374918514638461,-0.231434565088581,-0.00992419102991136,-2.11812756250465,-0.523762933650836,-1.32458158157352,0.431681037219444,0.925057143386304,0.93777525245556,0.546488768273407,1.11596946687424,0.570940155822875,-1.05140854030057,-0.318810634218592,-0.289448897409276,-0.418406412091964,0.781141791150043,-0.114891413907268,-0.845416526254121,-0.91845936893655,1.20789854386747,0.741312355103571,0.634427251618938,2.20585574787314,0.356161490453256,0.267242739519324,0.484831528360182,-0.165738361704601,0.189930678043718,-0.93651989042175,-1.45410391289995,0.233684800337273,0.502157215971716,0.28920149072855,-1.22551276737874,-0.385846452553303,1.42231442741027,-1.7116822994054,0.155053435413657,1.75487195590539,-0.480695737266194,-0.744335584259619,-2.13245053322139,-0.364322343399947,-0.233973774563064,0.111681118770621,-0.88914370304214,-0.722085375742162,-2.32991274785241,-0.479068930295669,0.789931238294979,1.06380136135981,-0.0798134682221333,-0.374168001227348,0.122704935131071,-1.43690716269235,-0.853944502445023,0.456289787735743,-0.0142516296486563,0.816751922214445,0.0849881110641102,0.446428132382165,2.42338976222122,-0.246995607668513,0.276176450241468,0.930116509469416,-1.02544718711831,0.104887979909664,-0.0142328430202254,2.03937753638296,0.373422932290684,1.43684984339667,0.948776626114924,-1.31827952307809,0.292905851185605,0.147308597580878,-1.03373092754513,0.419607277988786,-0.261092633509276,1.6071495149865,-0.647906145700861,0.657156199635267,-0.57098265859795,-0.818600322951857,1.31069089175351,1.81375842080092,-0.943433957369921,0.857269379202826,-0.263283789177259,1.31296346802011,-0.955590491841724,-1.56443546642085,-2.17226562414852,1.45591099037169,1.03814194129823,-0.197772383407226,-0.650297223745001,-1.33005360476313,0.921942504799078,0.752209085763665,-0.0361295533712819,2.63443405115608,0.287654631188146,0.655186314323695,0.400648171873719,-0.307311020383442,-0.0880147158656964,-1.41821667150147,-0.313515957169452,1.02977246630598,1.63557919370632,0.471406593501004,0.996474278664091,0.682501811766753,-0.971734007624617,-0.654540506542262,1.99500285811451,-1.64399263553111,-0.663557865235524,-1.68572596581282,-0.00487618116822535,0.458801236543731,0.5824022309138,-1.36287855122539,-1.27186820648675,0.714902868782501,-1.0165666698022,-0.0235973711331876,-0.0710369673541219,0.142253138818817,-0.675694865931503,-0.256829936236055,-0.101224481444322,-2.15395839517849,-0.964817551233336,-0.58900570028774,-1.77786400328839,1.14147095872602,-0.32772078240833,-1.35670287649227,0.415691882771873,-0.925932387905733,-1.61418322141891,-0.0867796005444939,0.601846649732015,1.12863379512717,0.184978687339329,0.156848912253547,-0.484058595314498,1.12522934475228,0.243459583230952,-0.634978878232694,-1.25090253023824,-1.43563909830578,1.13586385748573,0.215411392403429,1.86239774513426,1.11499996333691,-0.674626192034018,-0.434131329001274,1.03699553226858,-0.0366741372856878,-0.402271366973104,2.10246137335328,0.459018936796291,-0.25503953202401,-0.672610058718924,1.09980827026172,-0.410925835278326,-0.896790320270794,-1.03109344053961,-0.262748015975908,-0.333069074621615,0.977294210489194,0.678715733607743,1.25873526914719,0.42400531072313,1.79340828861104,0.294284737340495,-1.51557096175814,-0.566556739572426,0.0695601296483909,1.20367672097015,-0.549833339055226,0.120064475138402,0.307536795611604,1.21556005041361,0.12602924786391,-1.47214013475447,-0.318687822897892,0.279546357353257,2.70523892501022,1.68587064051207,1.1239941108417,0.424088292374247,0.104431077381569,-1.18021245837732,-0.916706926774142,-1.56221497392067,-0.851852947077733,-1.11863086095433,0.148312190622533,0.502631432428189,1.58774800017961,-1.10166271639973,-0.777449867610656,0.942870568479463,-1.9339574431339,0.00698050744702491,0.673684269774716,-1.39732869525216,0.283501034389694,0.874755970337202,-0.417645129576752,-1.36874292450096,-0.0747812970509281,0.0105724695173432,-0.0456309112554566,-1.47339841259021,1.33700556433092,1.94209421947211,0.284457070328279,-1.98482664413377,-0.50080737939018,-0.274676881900125,0.710938213804541,2.25851536328219,0.618237686663071,0.230927765626917,1.10122470908834,-0.572603422146998,-0.53302471140162,-0.717679874784871,-0.877038791682033,-0.748485888179382,0.184771684002625,-0.22231743531996,-1.00929695552901,-1.37047063999704,-1.07025776684948,-0.0066305826821924,0.00463777722911298,0.200669539977365,0.809383668396162,0.287278665935646,-0.303985246895298,0.139672450093511,0.127722862850529,-0.95565825905712,-0.799909006767028,-0.202940490973971,-2.41551300196281,0.3750531988884,2.0938362702894,0.605430623566889,-0.816263936823711,1.4935718662945,2.15379218446882,1.28063423391778,-1.16859150067533,-0.495931078057793,-1.12572347427127,-0.0289018780637711,-0.491217346285856,0.582465462265955,-1.73320910486715,-0.250616430567373,1.37773646638411,0.505183750595998,1.03329001588251,-0.148129803407604,-1.02024054280369,-1.16747702799092,-0.698262649012394,-0.29088420462642,-0.0873481684419898,2.28367061778488,0.799274120229441,0.496859660507907,-1.16771232655097,0.0560084744790094,0.0697664131694416,-0.63172486335937,-1.61579250963407,0.822541288820032,-0.585096534350738,-0.316981493648855,-0.445729843878749,-2.50838624530302,-1.39899463359675,-1.31102892051884,0.750402144358772,0.703404121659897,-0.25083983221813,-0.25095095379119,-0.872342684631607,0.628192536989848,-0.936217675649455,-0.550371466231215,-0.26970983693769,-0.688604841006888,-0.390605966140334,1.06988684039523,0.498849800595456,0.333419121051892,0.910556713584374,-1.27892253213922,-1.84182185333004,-0.512035156980371,-0.364119099300724,0.459286557995962,-0.0437559190006266,-0.453340737020902,2.02055656551793,1.47116567306761,-0.0115038122635589,0.131722917926454,-2.28429446135232,0.669356739346067,-1.14593193034066,-1.91291968267857,-1.67212741707599,0.54073309384294,-0.256724509418031,-1.89134380794797,0.161915204911539,-0.458809181262073,-0.194078077908355,0.296793193074685,-0.0250807732722297,-1.05794861180844,0.24484382118802,-0.648883034347149,0.618876699768757,1.44663045545316,1.76518871577172,-1.92334252201028,0.202212741132606,-0.342486128124399,-0.119772048017177,0.266562321266478,0.722029306985646,0.403631540478367,-1.9875531289521,-0.781171921052648,0.987488573315664,0.308690446320804,0.106148749253679,0.185085607568157,-0.390160876288962,1.25429513802065,-1.35013386465017,-0.342974766571647,-0.232894537629491,0.980332519980097,-2.11322741735,0.23024995771627,-1.45705269393795,1.38625292848739,1.6457152752694,0.886358263746316,0.296335125360452,-0.442944664529715,0.274627958703436,1.37404495680148,-1.61754075111507,-1.01356309656133,0.34151499813101,0.60561927302074,0.466358958410998,1.18788000370773,-0.998222009747409,0.472411048704897,0.802363134564957,0.778863518671328,1.33312992214265,-0.11483784787192,-1.24014458172761,1.68097845650274,-0.156295579276488,-0.36167688476951,1.12990338986994,0.544741774675208,-0.739519141431302,0.348344512782951,-0.10457968657704,-1.54862790298088,0.991859873430099,-0.712951101315977,-0.688713554688238,-0.019645395217261,-0.4276668324237,0.485554217337533,-1.22769785113341,-0.0839624525226322,0.929646496319533,-1.04399669558851,1.5862211735349,-0.762919277781499,-0.0482214954780706,0.482863741201961,0.272647805689435,-0.529870425645623,-2.32824908375099,0.0361073596021704,-0.451810115323141,-0.282254053923175,1.54911580277808,0.253374436546217,2.1590870111341,1.00793354498743,1.2238963859792,-0.262656713944417,0.371188568837448,1.84975076615656,-0.32057748245843,-0.484757190293574,0.119115296354525,0.311425735299618,2.86340591290978,0.561681683625529,-1.36773128004884,-0.521412125803238,1.8438836271771,0.15291957051073,-1.32938108961612,1.12345625118363,-0.0988379062841592,-0.781398448382246,1.13052436749178,0.767681931298354,0.525356584083132,0.582232899579348,0.333066984200484,-0.806258638926797,0.556552894435339,-1.30665318755998,0.785186326876701,-0.394106214647196,0.550730858543516,0.774230360594257,1.25782444024924,-1.59645579014647,0.726215347670778,-0.63694860577194,1.27493610125473,-0.0352439186929309,2.89168261447142,1.37501515427315,-0.0491456451207457,-0.0546139127595145,0.260510410587632,0.981880273079814,0.14277683851885,-0.254490023068397,-1.08396906594294,0.271948629915079,0.377547175371029,0.981224651122629,1.80754059091588,-1.1073823023655,0.482139414365283,1.8188538137154,-0.58749687491936,1.1093609884837,-0.297608176884006,2.57784620418014,2.52204599693865,-0.384840198277502,-0.855562334499144,0.938512269689899,-0.468196363866475,0.260008292451711,-0.462982716485726,-0.886275855262118,1.12411879590505,-0.909935517062699,-0.751334836930073,0.514366756631972,0.195860128347231,1.55471787782953,-0.253467892483979,-0.944270533093175,0.0655037556538774,0.669046506691992,-0.329793629158233,1.30135750702321,1.40114405750616,0.796269800703752,-0.10853585230126,1.29063556898416,-0.693149828347284,1.15340154198528,-1.67650319533252,-1.49429593747768,-1.273469908539,1.31161289723836,-1.31181443286064,-0.950226359402931,0.144663430311268,-1.37563897316271,-0.229895425421889,0.419692024617746,0.516239563506011,0.740669495728343,-0.361952001881317,-1.60995844987436,0.327757025857471,1.6432950871785,1.20535256388868,-0.353510280844418,1.21473389654699,-1.53240226188522,1.92233286176372,-1.09253954985412,-1.59184155325035,-0.64646780319668,-0.426883748541837,0.209569862271875,0.0415756254512372,1.33238165750511,-0.681027400604152,0.620571329940411,-0.900045782913094,0.309651318876625,0.118175407084332,0.227812270228395,-1.18930438569969,0.0837636991730383,0.588009163095429,-1.5092271780105,0.995898635439402,-0.00639701815793872,-0.3502219106773,1.4288096764155,-0.372525850460849,-1.86657115947184,1.73533502830238,1.4419101995773,-0.0083570684921407,1.3092622868551,0.000392255402139403,-1.45683485419048,-0.844996213277658,-0.60477119564568,-0.0517937465881769,0.821273104631407,1.50318458611856,0.26033919453097,0.521628843921831,-0.525259906808531,-1.07777898882352,-0.340595705086819,-0.0688224810312909,0.024696280077251,1.76093398424226,0.175448866859079,0.138406693673293,-0.206063763800319,-0.435211393481228,0.529893499013784,0.967447102676697,-2.43106301578848,-0.0424933305199116,-1.99868317055989,-2.47557651859656,-0.965041073936391,-0.435050040400478,-0.507170912986636,-1.37604070291559,0.641342899780833,0.15067215935219,0.647532140383024,-1.21460078326391,1.46931171546606,-1.78034791140857,0.39149639637274,0.953942227218803,1.27980367071278,0.263325295304111,-1.03877191179501,-0.962563912956559,0.895371173457089,0.905913690706474],[1.45455392903023,0.127979886148149,2.11576802619236,0.721590332592378,0.718727176390164,0.803945842125534,0.59782346134677,-0.906152421533778,0.88132970687965,1.01558387659001,0.501415672715825,-1.77181442248488,-2.17330423610865,0.53116442563413,1.32996798083008,1.12126640454506,-0.817779552351742,-0.475105646588491,0.483436672821363,-2.5689626513899,0.317092358915433,-2.73138669617782,1.37759381450205,1.25689413289036,-0.35325977822214,0.863237379295043,2.00769895338101,-1.36479900824117,0.885083779192587,-1.45357689323861,-0.0549658183244174,0.651636871017632,0.492698843424174,-0.478886130712406,0.181665004812463,1.92884553005489,-1.30723002058808,-1.18049689058764,0.289360563463143,-0.833819138141956,-0.74993148364448,-1.5419469842697,1.24376330960582,1.04893997630657,0.86015418492544,1.73411218509684,-0.611696074239411,-0.346032671845892,0.850541980347479,0.96865433825319,-0.65214608794643,-0.795253385033044,0.0645270063019449,2.07323829593554,0.800007614995525,-0.641451027758909,0.0307467462614326,-1.73207068466649,0.91349982011753,-1.75470463366377,-1.00171898465568,0.488653821720506,0.341772257061285,-0.0191690432347055,0.168221574568466,1.93677353424073,1.52478452124677,1.70711699931766,-1.21379895471538,-0.579687709546852,1.08176331486219,-2.04849541171673,2.31484013187928,0.583789305251403,0.212293255523722,-1.46726595789437,2.14968307907626,-0.429631173327844,1.01015924271902,2.81228395233797,-1.67041252198154,2.02265997192769,3.14453514348789,0.272801658808313,1.11925472235902,-1.18092141588517,-2.24649601764876,0.211977866013016,-0.114010406794195,-0.373567670849043,-1.42324822194076,0.903949132049712,-0.107839925428649,-3.51575943775935,-1.34177456131285,2.53613269956311,0.0830842520008451,0.423020598577437,-2.91457644972374,-1.07695804092998,-0.408504292314111,0.275481290725075,-1.03467107620113,-0.780661908801804,-2.58845821883281,-0.497057679417127,0.679043470307913,1.00939587674757,-0.233394365500929,-0.535670660164188,0.799305122095471,-1.64527934018255,-0.18188190729423,0.876129109537827,0.601309604984977,0.173340568439929,-0.343956933998136,0.543563152475191,2.52803452499337,-0.118459711932523,2.3224351537464,-0.0293466551202781,-4.49055909475246,-1.84227509395362,-1.00190726774954,1.56068978336834,0.747921442159367,1.69971029037947,0.772172301978555,-1.41331907870862,0.533558691832012,0.342638451384771,-1.88817354239872,0.201734093551981,-1.29866349567292,1.98826124584664,-0.389845075695757,0.681704581822748,-0.635053550372043,-1.14187024975318,-0.0295016596043615,2.89691548422317,0.909776629752408,1.44510775292438,-0.194461974747158,1.6306975540672,-1.33918535897049,-1.39061859749541,-2.13192118086277,1.1367858953242,0.652947681553984,-0.585548446252756,0.161569261096494,-0.861866675962779,1.21048323698701,0.67484687971892,-0.0428775308318919,2.09717722239257,0.424909359777721,0.96890214468733,0.720648389503118,0.338956573481393,-3.79896497297377,-2.89481154898875,-1.83730957559228,0.767066388013485,1.12529337195696,1.90865934638147,-0.00934110260541683,0.0241562430868818,-2.50421354345041,0.241307626630586,4.81456610162892,-0.536173434118735,-0.348237401670472,-0.364574779887286,-0.248153578822729,0.219908049768439,1.05008298690315,-2.1827943345806,-1.16107569537391,-0.349225490518604,-1.09601040931303,0.413091574945561,0.314851237634054,-0.595926499161015,-0.974984776581642,-0.723730784957616,0.094730932131706,-1.86336720469773,-1.98556616046281,1.76749958910276,-1.66106034852546,0.887749621177237,-1.73687535462034,0.785532456145969,1.13801632985159,0.415236190863475,-1.51036257259158,-2.10222190817463,1.19545563290203,0.884275256003694,0.261501403287509,0.380178597714206,0.372055616828559,0.68533908009399,0.0154980926015381,-0.473013049535596,-1.15844711794671,-0.731533158823147,0.0982469612562809,0.538262146139488,4.10958694104826,1.53491312612813,-0.140240593355677,-0.181076455899422,0.393692328701841,-0.25851253796866,-0.278839196975291,1.96728059785303,1.26718946002199,-1.18932931116725,-1.4150644677335,1.56564741934346,-0.191624299597958,-0.88711065816804,-1.22602745291905,-0.282281827229268,-0.164211825870661,1.74985807680971,-0.115580533186955,1.49362246759935,1.77858941393839,2.89373907728399,0.830891209413776,-1.505043820662,-1.01150158127975,0.300426257443122,1.54446061356592,-0.214719834979651,-4.09948263758051,3.26016855273166,3.20708042457384,1.98475596005349,-1.67419300199372,0.907736080076751,-1.55698251828915,4.28431201979026,1.3801146305025,0.717444239964544,2.8770701725963,-1.62999689002386,-3.14909123556638,-0.876991296786752,0.584516240230853,-3.29004776983306,-0.693737679417125,-2.09837321699303,0.834985905330198,2.73881473941997,-0.76351413344542,-0.956473611468944,0.227499512869276,-2.40064381397708,-0.344326683351491,0.620190330234918,-0.915252864804511,0.129227172399695,0.815139684591633,-0.661560912297849,-0.638585006359612,-0.645483846394461,-1.6674291756124,-0.86685566155851,-1.89701242725788,0.900688814417758,2.27471261649368,-0.00214390383259238,-2.35315324485578,-0.43360072827215,-1.19405178237176,2.29626674253042,4.43487216639775,0.681710701915436,-0.300485831740576,2.19198870250146,-0.00220785701150898,-0.163547702042767,-0.650277521706642,-1.45041401902182,1.2190826093343,-0.223586434197921,-3.44183843448305,-3.72624183500941,-2.42977014635486,-1.19857353592352,0.527858325814631,0.0905821559382178,-0.415973609380463,0.375218540333945,1.52391430619389,-1.78336572638296,-1.45481773949924,0.206169053099722,-0.38680010073611,-2.09723686967577,-0.142843009464547,-2.96566427471391,0.794275460621815,2.59625203240987,0.482252436633083,-0.78472064716517,1.57038775879243,2.17170726395753,1.17291993020338,-0.999568131866249,-0.12930272108434,-1.16342328775642,0.0243019729866625,-0.356863839457366,0.138204614080159,-2.62181799626068,1.64013909572135,3.07155874612302,1.67377416699458,0.492394737183418,-0.745320929216197,-1.75137110253145,-0.529111401941947,0.126975656902085,-1.49486860049762,1.10372145153654,4.63069446895088,1.26390429598724,0.561560584245613,-0.449559254059866,-0.255761499677258,0.40896368645298,-0.622867006536829,-1.90262025556079,1.00713858724779,-0.604883686281754,-1.83973191348273,-0.853855309695597,-2.86573285671032,-1.82240757965384,-1.24797095603525,1.15127244197033,0.432004132079164,0.130236362987383,0.319992469896284,-0.775551080535279,-1.56979998341251,-2.59794594646738,-1.47923054092289,0.131541236467871,-0.0417306289955851,0.166039690906464,0.675080522805901,0.352458861800138,-0.628918675128145,1.35826785972955,0.409451590302719,-0.549846718180653,-0.212768151768142,-0.138415522213517,0.394440584490804,0.110802190981229,-0.0292900077336594,2.31933419150032,2.31614280334728,-1.05347670593277,0.143648912722544,-1.27836808988421,1.79000580579163,-2.3313954375485,-2.6679882597542,-1.58056818226108,0.553766185402604,0.992420070337123,-2.87590010010752,1.31224693935914,1.06636122126486,0.240472130173398,0.603525789113647,0.630511249917346,-0.947996001186137,0.474034921568318,-0.371864801261806,0.177580435769444,1.57538126714253,2.33290987889646,-2.20001580420075,-0.408190793581529,-0.618930368177453,-0.159840744358006,0.0417192575837648,1.45203748443925,0.00241078125304973,-2.05292213844454,0.320641156128965,-0.350133576403851,-0.949457969300998,-0.237461614904916,0.283470835577729,-1.36849060405771,1.5748643251333,-2.38700299403057,-0.689571185212685,0.206991745324159,0.496021588232189,-2.2713388998301,0.240172936158086,-1.74374183787724,1.1915675010373,1.52894377916179,0.722441743400448,0.380572349630593,-0.397124998263868,0.446177543910852,2.3072175508423,-2.60730360023716,-0.274966720829447,0.576495626031179,1.73883465600728,-0.281731180738563,1.26966273935992,-1.81465370996872,1.03504240186856,1.55808908828967,-0.314541666801479,2.53654633667702,-1.19360888543309,-1.95575776327428,0.380293921772089,0.883024267143154,-0.333975690256655,1.84000568194377,-0.143271545103529,-1.22147602952063,1.28440120358825,-1.75540337357658,-2.68176433579527,0.729280864852588,-0.906549175756805,-1.34329143605199,0.123089731333984,-1.54280026706529,0.582232567049726,-1.31218691044863,-0.420630093317562,1.27283892499407,-1.30740926390171,1.1578041493855,-0.969906283286766,-0.114758343783288,0.24964401258364,0.478666067188773,-0.759522837390877,-2.41137942969542,-0.074524902131765,-0.7428150772544,-0.614445891129085,0.997923179020188,0.100083644735227,1.9759934357765,0.942661552610041,1.01379485170852,-0.498172929921127,0.849627483684419,3.61512216837149,-1.9779065843683,-2.23063491164516,-0.621891715604371,-0.0822411984593873,2.35350505966315,0.824667213800672,-1.87483323087216,-0.48720398010831,2.38577502475488,-0.523199638894036,-0.726507414404578,2.45298614370966,0.860396900571693,-0.110055407855709,1.28588225032213,0.341057202846186,0.605377593150398,0.72136731396169,0.547794397746293,-1.01421454710758,2.5092777289014,-0.0526863973410327,0.00405369128707626,-0.0718443107415384,1.07237846841921,1.14129713910923,2.31009413407549,-2.09400137470898,0.456663798419397,-2.05975288684778,1.8758150362362,2.59853847379207,4.60136078507925,2.21698131858023,0.726115209763544,-0.599810130945621,0.354311501784252,1.87948293047567,0.238369682204631,-0.713775183119107,-0.604800438491915,1.41015571001845,0.585320930719922,0.871318647182723,2.20488299671375,-0.905842377789842,0.3688983850767,1.95550307555848,-0.852661457313588,2.16609494241586,-2.40814694374333,1.66722484427458,3.95022102628246,-0.224828330587484,-1.4505431533536,0.673972954807466,-1.15694441492131,1.21741454828699,0.539789278198326,-1.66442938337564,1.62032494568496,0.312940739364977,-0.284046938397323,0.71603303964642,0.240209777587362,1.47233769756634,-0.194790937304227,-0.861548194979865,-0.28030576562191,-0.40078391115229,-0.0549159865722765,2.24952703129328,2.26871913956642,0.935238743598456,0.221751661562736,0.836068369093162,-0.968649584523319,1.35489158742591,-1.93408643880338,-1.46613404227331,-0.0520950140496412,2.54842161913673,-0.821571654629921,-0.969645038716868,1.45741046120888,-1.02059475017999,0.369639267299918,0.320503524070014,-0.021998818857629,-0.966019536349913,0.44231725957982,0.205616202338776,1.23819690775915,1.86837067274718,2.09506832443385,-0.801652955390395,1.40664441930225,-1.45812241776314,1.74825092341394,0.0483990439655684,-3.57321874602449,-2.24003034446233,0.089889112525863,0.551344440953765,-0.32040700190579,1.50560335886048,-1.33189253414665,0.972670768806416,-0.212886158213597,-0.523813652756746,0.22046971572144,1.12597543686769,-0.329113110118854,0.00299606810824287,1.05097137088726,-1.52434677327573,0.96240948289536,0.364286359190311,-0.398763687573481,1.22172508835383,-1.10005583179796,-1.12944949344219,2.44620890006649,1.76391759810652,-0.266254565831541,1.09376037814418,-0.565286286921906,-1.04893457766946,-0.898431187485613,0.0962036420032532,-0.283099620545747,-0.0122664644880822,0.930619262132225,-0.166737790575519,0.352367380067922,-0.187621917616955,-0.75154805049614,-0.39470254839513,-0.384307588424691,1.88359886684115,1.11680786047837,-2.33830334608029,-1.32145777297899,-0.868277734200725,-0.958066533569076,1.57101119098721,0.74883219266784,-3.12325778802289,0.160689808840553,-1.86207835076371,-2.75837542867784,-2.81670532848697,-1.75357085148453,-1.51555079796211,-1.60236484047339,0.83426645544337,0.138222031389298,0.369755084286559,-1.54496025448757,4.68317450320011,-2.38573023625451,-7.44924962851386,-3.70339096894293,-0.192019527367885,-0.689648536676905,-0.166685459988536,0.384542797501622,-0.804133539516764,0.165326348979191],[100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>y<\/th>\n      <th>y_pred<\/th>\n      <th>iter<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"columnDefs":[{"className":"dt-right","targets":[1,2,3]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script><img src="rtorch-minimal-book_files/figure-html/dt-100-iters-2.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="iterations" class="section level4" number="12.4.4.4">
<h4><span class="header-section-number">12.4.4.4</span> 250 iterations</h4>
<p>Still there are differences between the value and the prediction. Let’s try with more iterations, like <strong>250</strong>:</p>
<div class="sourceCode" id="cb532"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb532-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb532-1" aria-hidden="true" tabindex="-1"></a>df_250 <span class="ot">&lt;-</span> <span class="fu">train</span>(<span class="at">iterations =</span> <span class="dv">200</span>)</span>
<span id="cb532-2"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb532-2" aria-hidden="true" tabindex="-1"></a><span class="fu">datatable</span>(df_250, <span class="at">options =</span> <span class="fu">list</span>(<span class="at">pageLength =</span> <span class="dv">25</span>))</span>
<span id="cb532-3"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb532-3" aria-hidden="true" tabindex="-1"></a><span class="co"># plot</span></span>
<span id="cb532-4"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb532-4" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df_250, <span class="fu">aes</span>(<span class="at">x =</span> y_pred, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb532-5"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb532-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>()</span></code></pre></div>
<p><div id="htmlwidget-e1231a09600ae0410e72" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-e1231a09600ae0410e72">{"x":{"filter":"none","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153","154","155","156","157","158","159","160","161","162","163","164","165","166","167","168","169","170","171","172","173","174","175","176","177","178","179","180","181","182","183","184","185","186","187","188","189","190","191","192","193","194","195","196","197","198","199","200","201","202","203","204","205","206","207","208","209","210","211","212","213","214","215","216","217","218","219","220","221","222","223","224","225","226","227","228","229","230","231","232","233","234","235","236","237","238","239","240","241","242","243","244","245","246","247","248","249","250","251","252","253","254","255","256","257","258","259","260","261","262","263","264","265","266","267","268","269","270","271","272","273","274","275","276","277","278","279","280","281","282","283","284","285","286","287","288","289","290","291","292","293","294","295","296","297","298","299","300","301","302","303","304","305","306","307","308","309","310","311","312","313","314","315","316","317","318","319","320","321","322","323","324","325","326","327","328","329","330","331","332","333","334","335","336","337","338","339","340","341","342","343","344","345","346","347","348","349","350","351","352","353","354","355","356","357","358","359","360","361","362","363","364","365","366","367","368","369","370","371","372","373","374","375","376","377","378","379","380","381","382","383","384","385","386","387","388","389","390","391","392","393","394","395","396","397","398","399","400","401","402","403","404","405","406","407","408","409","410","411","412","413","414","415","416","417","418","419","420","421","422","423","424","425","426","427","428","429","430","431","432","433","434","435","436","437","438","439","440","441","442","443","444","445","446","447","448","449","450","451","452","453","454","455","456","457","458","459","460","461","462","463","464","465","466","467","468","469","470","471","472","473","474","475","476","477","478","479","480","481","482","483","484","485","486","487","488","489","490","491","492","493","494","495","496","497","498","499","500","501","502","503","504","505","506","507","508","509","510","511","512","513","514","515","516","517","518","519","520","521","522","523","524","525","526","527","528","529","530","531","532","533","534","535","536","537","538","539","540","541","542","543","544","545","546","547","548","549","550","551","552","553","554","555","556","557","558","559","560","561","562","563","564","565","566","567","568","569","570","571","572","573","574","575","576","577","578","579","580","581","582","583","584","585","586","587","588","589","590","591","592","593","594","595","596","597","598","599","600","601","602","603","604","605","606","607","608","609","610","611","612","613","614","615","616","617","618","619","620","621","622","623","624","625","626","627","628","629","630","631","632","633","634","635","636","637","638","639","640"],[0.83994026831249,-0.00261469246496478,1.17331508976272,0.113064550776486,0.0103083101232198,0.741725300575136,0.757130556967624,-1.19798639091765,0.977469466953348,0.958784411472249,0.0208831428895932,-1.64816158891565,-0.0500028243495128,0.733813527145886,1.58085339989783,1.37525327872787,-0.386739371622718,-0.360212335648884,0.757263768677633,-2.33097748052543,-0.280231384151112,-0.958825812111196,-0.0248397757648495,1.0914318283759,-0.458192050288552,1.88579895675686,1.8374918101401,-0.102590503582433,0.158376834397865,-1.69505820821086,0.0630930916536182,1.3418620933296,0.173417858163622,-0.972149178523607,-0.312413823093578,2.16446071258402,-1.08569798526669,-0.570029361135791,0.180937256764405,-1.26229551939497,-1.36950465873129,-1.15493391741737,1.40785825831085,0.781023621423221,0.0780186960888742,2.11273169799433,-0.382805598728771,-0.332249209115966,0.833881240069712,0.59587340242551,-1.10900749622674,0.768900089169295,-1.35001396009045,0.483618658476077,-0.286807646207213,0.141828938278373,-0.374918514638461,-0.231434565088581,-0.00992419102991136,-2.11812756250465,-0.523762933650836,-1.32458158157352,0.431681037219444,0.925057143386304,0.93777525245556,0.546488768273407,1.11596946687424,0.570940155822875,-1.05140854030057,-0.318810634218592,-0.289448897409276,-0.418406412091964,0.781141791150043,-0.114891413907268,-0.845416526254121,-0.91845936893655,1.20789854386747,0.741312355103571,0.634427251618938,2.20585574787314,0.356161490453256,0.267242739519324,0.484831528360182,-0.165738361704601,0.189930678043718,-0.93651989042175,-1.45410391289995,0.233684800337273,0.502157215971716,0.28920149072855,-1.22551276737874,-0.385846452553303,1.42231442741027,-1.7116822994054,0.155053435413657,1.75487195590539,-0.480695737266194,-0.744335584259619,-2.13245053322139,-0.364322343399947,-0.233973774563064,0.111681118770621,-0.88914370304214,-0.722085375742162,-2.32991274785241,-0.479068930295669,0.789931238294979,1.06380136135981,-0.0798134682221333,-0.374168001227348,0.122704935131071,-1.43690716269235,-0.853944502445023,0.456289787735743,-0.0142516296486563,0.816751922214445,0.0849881110641102,0.446428132382165,2.42338976222122,-0.246995607668513,0.276176450241468,0.930116509469416,-1.02544718711831,0.104887979909664,-0.0142328430202254,2.03937753638296,0.373422932290684,1.43684984339667,0.948776626114924,-1.31827952307809,0.292905851185605,0.147308597580878,-1.03373092754513,0.419607277988786,-0.261092633509276,1.6071495149865,-0.647906145700861,0.657156199635267,-0.57098265859795,-0.818600322951857,1.31069089175351,1.81375842080092,-0.943433957369921,0.857269379202826,-0.263283789177259,1.31296346802011,-0.955590491841724,-1.56443546642085,-2.17226562414852,1.45591099037169,1.03814194129823,-0.197772383407226,-0.650297223745001,-1.33005360476313,0.921942504799078,0.752209085763665,-0.0361295533712819,2.63443405115608,0.287654631188146,0.655186314323695,0.400648171873719,-0.307311020383442,-0.0880147158656964,-1.41821667150147,-0.313515957169452,1.02977246630598,1.63557919370632,0.471406593501004,0.996474278664091,0.682501811766753,-0.971734007624617,-0.654540506542262,1.99500285811451,-1.64399263553111,-0.663557865235524,-1.68572596581282,-0.00487618116822535,0.458801236543731,0.5824022309138,-1.36287855122539,-1.27186820648675,0.714902868782501,-1.0165666698022,-0.0235973711331876,-0.0710369673541219,0.142253138818817,-0.675694865931503,-0.256829936236055,-0.101224481444322,-2.15395839517849,-0.964817551233336,-0.58900570028774,-1.77786400328839,1.14147095872602,-0.32772078240833,-1.35670287649227,0.415691882771873,-0.925932387905733,-1.61418322141891,-0.0867796005444939,0.601846649732015,1.12863379512717,0.184978687339329,0.156848912253547,-0.484058595314498,1.12522934475228,0.243459583230952,-0.634978878232694,-1.25090253023824,-1.43563909830578,1.13586385748573,0.215411392403429,1.86239774513426,1.11499996333691,-0.674626192034018,-0.434131329001274,1.03699553226858,-0.0366741372856878,-0.402271366973104,2.10246137335328,0.459018936796291,-0.25503953202401,-0.672610058718924,1.09980827026172,-0.410925835278326,-0.896790320270794,-1.03109344053961,-0.262748015975908,-0.333069074621615,0.977294210489194,0.678715733607743,1.25873526914719,0.42400531072313,1.79340828861104,0.294284737340495,-1.51557096175814,-0.566556739572426,0.0695601296483909,1.20367672097015,-0.549833339055226,0.120064475138402,0.307536795611604,1.21556005041361,0.12602924786391,-1.47214013475447,-0.318687822897892,0.279546357353257,2.70523892501022,1.68587064051207,1.1239941108417,0.424088292374247,0.104431077381569,-1.18021245837732,-0.916706926774142,-1.56221497392067,-0.851852947077733,-1.11863086095433,0.148312190622533,0.502631432428189,1.58774800017961,-1.10166271639973,-0.777449867610656,0.942870568479463,-1.9339574431339,0.00698050744702491,0.673684269774716,-1.39732869525216,0.283501034389694,0.874755970337202,-0.417645129576752,-1.36874292450096,-0.0747812970509281,0.0105724695173432,-0.0456309112554566,-1.47339841259021,1.33700556433092,1.94209421947211,0.284457070328279,-1.98482664413377,-0.50080737939018,-0.274676881900125,0.710938213804541,2.25851536328219,0.618237686663071,0.230927765626917,1.10122470908834,-0.572603422146998,-0.53302471140162,-0.717679874784871,-0.877038791682033,-0.748485888179382,0.184771684002625,-0.22231743531996,-1.00929695552901,-1.37047063999704,-1.07025776684948,-0.0066305826821924,0.00463777722911298,0.200669539977365,0.809383668396162,0.287278665935646,-0.303985246895298,0.139672450093511,0.127722862850529,-0.95565825905712,-0.799909006767028,-0.202940490973971,-2.41551300196281,0.3750531988884,2.0938362702894,0.605430623566889,-0.816263936823711,1.4935718662945,2.15379218446882,1.28063423391778,-1.16859150067533,-0.495931078057793,-1.12572347427127,-0.0289018780637711,-0.491217346285856,0.582465462265955,-1.73320910486715,-0.250616430567373,1.37773646638411,0.505183750595998,1.03329001588251,-0.148129803407604,-1.02024054280369,-1.16747702799092,-0.698262649012394,-0.29088420462642,-0.0873481684419898,2.28367061778488,0.799274120229441,0.496859660507907,-1.16771232655097,0.0560084744790094,0.0697664131694416,-0.63172486335937,-1.61579250963407,0.822541288820032,-0.585096534350738,-0.316981493648855,-0.445729843878749,-2.50838624530302,-1.39899463359675,-1.31102892051884,0.750402144358772,0.703404121659897,-0.25083983221813,-0.25095095379119,-0.872342684631607,0.628192536989848,-0.936217675649455,-0.550371466231215,-0.26970983693769,-0.688604841006888,-0.390605966140334,1.06988684039523,0.498849800595456,0.333419121051892,0.910556713584374,-1.27892253213922,-1.84182185333004,-0.512035156980371,-0.364119099300724,0.459286557995962,-0.0437559190006266,-0.453340737020902,2.02055656551793,1.47116567306761,-0.0115038122635589,0.131722917926454,-2.28429446135232,0.669356739346067,-1.14593193034066,-1.91291968267857,-1.67212741707599,0.54073309384294,-0.256724509418031,-1.89134380794797,0.161915204911539,-0.458809181262073,-0.194078077908355,0.296793193074685,-0.0250807732722297,-1.05794861180844,0.24484382118802,-0.648883034347149,0.618876699768757,1.44663045545316,1.76518871577172,-1.92334252201028,0.202212741132606,-0.342486128124399,-0.119772048017177,0.266562321266478,0.722029306985646,0.403631540478367,-1.9875531289521,-0.781171921052648,0.987488573315664,0.308690446320804,0.106148749253679,0.185085607568157,-0.390160876288962,1.25429513802065,-1.35013386465017,-0.342974766571647,-0.232894537629491,0.980332519980097,-2.11322741735,0.23024995771627,-1.45705269393795,1.38625292848739,1.6457152752694,0.886358263746316,0.296335125360452,-0.442944664529715,0.274627958703436,1.37404495680148,-1.61754075111507,-1.01356309656133,0.34151499813101,0.60561927302074,0.466358958410998,1.18788000370773,-0.998222009747409,0.472411048704897,0.802363134564957,0.778863518671328,1.33312992214265,-0.11483784787192,-1.24014458172761,1.68097845650274,-0.156295579276488,-0.36167688476951,1.12990338986994,0.544741774675208,-0.739519141431302,0.348344512782951,-0.10457968657704,-1.54862790298088,0.991859873430099,-0.712951101315977,-0.688713554688238,-0.019645395217261,-0.4276668324237,0.485554217337533,-1.22769785113341,-0.0839624525226322,0.929646496319533,-1.04399669558851,1.5862211735349,-0.762919277781499,-0.0482214954780706,0.482863741201961,0.272647805689435,-0.529870425645623,-2.32824908375099,0.0361073596021704,-0.451810115323141,-0.282254053923175,1.54911580277808,0.253374436546217,2.1590870111341,1.00793354498743,1.2238963859792,-0.262656713944417,0.371188568837448,1.84975076615656,-0.32057748245843,-0.484757190293574,0.119115296354525,0.311425735299618,2.86340591290978,0.561681683625529,-1.36773128004884,-0.521412125803238,1.8438836271771,0.15291957051073,-1.32938108961612,1.12345625118363,-0.0988379062841592,-0.781398448382246,1.13052436749178,0.767681931298354,0.525356584083132,0.582232899579348,0.333066984200484,-0.806258638926797,0.556552894435339,-1.30665318755998,0.785186326876701,-0.394106214647196,0.550730858543516,0.774230360594257,1.25782444024924,-1.59645579014647,0.726215347670778,-0.63694860577194,1.27493610125473,-0.0352439186929309,2.89168261447142,1.37501515427315,-0.0491456451207457,-0.0546139127595145,0.260510410587632,0.981880273079814,0.14277683851885,-0.254490023068397,-1.08396906594294,0.271948629915079,0.377547175371029,0.981224651122629,1.80754059091588,-1.1073823023655,0.482139414365283,1.8188538137154,-0.58749687491936,1.1093609884837,-0.297608176884006,2.57784620418014,2.52204599693865,-0.384840198277502,-0.855562334499144,0.938512269689899,-0.468196363866475,0.260008292451711,-0.462982716485726,-0.886275855262118,1.12411879590505,-0.909935517062699,-0.751334836930073,0.514366756631972,0.195860128347231,1.55471787782953,-0.253467892483979,-0.944270533093175,0.0655037556538774,0.669046506691992,-0.329793629158233,1.30135750702321,1.40114405750616,0.796269800703752,-0.10853585230126,1.29063556898416,-0.693149828347284,1.15340154198528,-1.67650319533252,-1.49429593747768,-1.273469908539,1.31161289723836,-1.31181443286064,-0.950226359402931,0.144663430311268,-1.37563897316271,-0.229895425421889,0.419692024617746,0.516239563506011,0.740669495728343,-0.361952001881317,-1.60995844987436,0.327757025857471,1.6432950871785,1.20535256388868,-0.353510280844418,1.21473389654699,-1.53240226188522,1.92233286176372,-1.09253954985412,-1.59184155325035,-0.64646780319668,-0.426883748541837,0.209569862271875,0.0415756254512372,1.33238165750511,-0.681027400604152,0.620571329940411,-0.900045782913094,0.309651318876625,0.118175407084332,0.227812270228395,-1.18930438569969,0.0837636991730383,0.588009163095429,-1.5092271780105,0.995898635439402,-0.00639701815793872,-0.3502219106773,1.4288096764155,-0.372525850460849,-1.86657115947184,1.73533502830238,1.4419101995773,-0.0083570684921407,1.3092622868551,0.000392255402139403,-1.45683485419048,-0.844996213277658,-0.60477119564568,-0.0517937465881769,0.821273104631407,1.50318458611856,0.26033919453097,0.521628843921831,-0.525259906808531,-1.07777898882352,-0.340595705086819,-0.0688224810312909,0.024696280077251,1.76093398424226,0.175448866859079,0.138406693673293,-0.206063763800319,-0.435211393481228,0.529893499013784,0.967447102676697,-2.43106301578848,-0.0424933305199116,-1.99868317055989,-2.47557651859656,-0.965041073936391,-0.435050040400478,-0.507170912986636,-1.37604070291559,0.641342899780833,0.15067215935219,0.647532140383024,-1.21460078326391,1.46931171546606,-1.78034791140857,0.39149639637274,0.953942227218803,1.27980367071278,0.263325295304111,-1.03877191179501,-0.962563912956559,0.895371173457089,0.905913690706474],[0.897436443974543,-0.0057316453273002,1.16704995490867,0.212278744761708,0.0756390693605932,0.754902054697985,0.761441293589772,-1.18106963563568,0.971614093926814,1.07397979515241,0.00228724938649218,-1.6423497643334,0.0137546258808962,0.725670508284857,1.57675197073118,1.35809039914413,-0.325871647748961,-0.386126920694205,0.767225424261643,-2.31780402518183,0.113149344738467,-0.986438140434025,-0.147134539797003,1.28448439148769,-0.383636959394258,2.15496081187861,1.75839656708146,0.139217071571888,0.155734123163411,-1.52370702909948,0.0322021354101081,1.34872155124568,0.204123994741059,-1.02920898455824,-0.323708527533779,2.11055763946078,-1.04633453427139,-0.670347954090598,0.193516149386393,-1.30242300544337,-1.44629050477371,-1.15431476244424,1.41747007359708,0.692915772312528,0.0106629181284879,2.06557038804581,-0.388325672288448,-0.364190341924299,0.843474231317942,0.494908333004578,-1.23392054328972,0.762352302149429,-1.35051852633974,0.442574467579733,-0.350563853641526,0.0563301812172819,-0.400542639380468,-0.26188651639127,-0.0236227575009496,-2.20078819791302,-0.674403063759378,-1.29173058052978,0.475803221826633,0.827335219933468,0.873568832165857,0.394380294782269,1.1661163223133,0.405565839100092,-1.02561675134647,-0.409869481104421,-0.360985839835955,-0.39592990407378,0.802723332603863,-0.0832605860731628,-0.848222451917855,-0.958012748296031,1.21989344429357,0.740110919884035,0.638189390096745,2.23117958248004,0.50257209949884,0.24247427382099,0.435548208459899,-0.0946216796072812,0.215655447098848,-0.812252816451296,-1.48583235406028,0.326842797283394,0.479866307392881,0.327890117053311,-1.08496602629739,-0.395634458955808,1.39030510511665,-1.60729196463132,0.223387338123894,1.88030446287076,-0.490435617408998,-0.652159560023178,-2.14623552357872,-0.272655911965966,-0.119887185805251,0.0851941718072113,-0.888922847791866,-0.619023097099486,-2.27160382388609,-0.332940343231645,0.79076206829564,1.22287449301604,-0.0841670808805901,-0.280100197246338,0.0804089187860943,-1.44163303983857,-0.84415393769145,0.451968281396879,-0.0372871793725579,0.797050686303901,0.0862070947302991,0.453178119204614,2.40891309179436,-0.257394497041661,0.222730991935511,0.921715612021307,-1.02656255728884,0.0601875545811014,-0.0462109548539098,2.00245080416653,0.362631462733139,1.40642070543074,0.948271871742852,-1.36857173331843,0.2752854158759,0.150435637675386,-1.02718912980368,0.336830852502675,-0.283652002367885,1.52605035500304,-0.606880172863337,0.546131743480789,-0.556727694548425,-0.873034309250269,1.31528663880689,1.78446876607986,-0.990279345637079,0.835861148061616,-0.31373673113699,1.35833674077192,-1.01376049716472,-1.45622152169539,-2.21366439681984,1.36809181243605,1.05255494099349,-0.200133208981944,-0.70104503996926,-1.36857000567183,0.906312867609238,0.746119004809783,-0.0637196150205623,2.64077334173349,0.299480043479457,0.633560305392732,0.403494472216696,-0.310813331529148,-0.0645513255233407,-1.38547973157608,-0.321851305976517,1.07512894501701,1.63355763086542,0.545355036535173,0.981040780631543,0.677723496806474,-0.934297704211285,-0.65519212498725,2.00754089661633,-1.64259975229488,-0.655794898201192,-1.67091682158057,0.0176797932616244,0.44383481233516,0.592450475870772,-1.35002942499509,-1.21225597629596,0.720512541975164,-1.10766392471773,0.00980736189577337,-0.0489694612508961,0.181622252484564,-0.726230564979879,-0.221375981974232,-0.100811956251638,-2.10859694691984,-0.996539773565149,-0.562330631506294,-1.71732083633344,1.12132011619452,-0.298901721295449,-1.4224908423887,0.477872217827284,-1.01459978671217,-1.58739635691512,-0.0449164886408111,0.568522654333565,1.11929957116042,0.209794453871568,0.121489437056055,-0.519753896458843,1.11745686001152,0.264161979488282,-0.673089550259352,-1.24832839376181,-1.48219922493513,1.27786969889673,0.184805943535254,1.82598741561409,1.16815051452098,-0.616727301952886,-0.32239486143698,1.04949449244327,0.0334780408105983,-0.414525732628657,2.13691263347972,0.406878782530469,-0.271846445184763,-0.704851385579044,0.978827143582152,-0.455637366397536,-1.0176652384726,-1.00421725114664,-0.370928131497166,-0.324788280107997,0.93105512914557,0.76117737745361,1.23421480096006,0.397599062270376,1.76918641194037,0.282516390482593,-1.47228272096603,-0.574953100513568,0.103337955770318,1.20088432521217,-0.548864120867652,0.22052902232709,0.287313281335362,1.15049740127416,0.182156173509642,-1.46315131146401,-0.181853733817694,0.173904366629213,2.84046719976123,1.69174061952115,1.08623430648738,0.418196859063102,0.0884774075882937,-1.2098641473554,-0.977847393008903,-1.61918982793019,-0.828761148731598,-1.13502040753988,0.166427813458247,0.493653218025048,1.48341814930641,-1.03620322450014,-0.805498033759943,0.910323245770806,-1.82610785899129,0.0220880993480065,0.764153688031386,-1.44136864500233,0.447570869984794,0.837605011504142,-0.336051483064152,-1.55518141860264,-0.103050912783982,0.00805583774887968,-0.315009969754517,-1.6816205578651,1.09628354160663,1.90192638715677,0.105936193407481,-1.99302916302551,-0.817563155881467,-0.275072254574036,0.739786544860087,2.26080486076778,0.611347288300011,0.280472484063264,1.05480267934508,-0.535926011532201,-0.654117101979111,-0.667624284669024,-0.830131605593302,-0.898602600677821,0.210689648626923,-0.171373192038879,-1.04207955113289,-1.39983916108501,-1.18826069198857,0.000232222921511705,-0.0837553075087984,0.194207066116152,0.751021261133676,0.410800406418922,-0.35395232126712,0.137270603685408,0.134278109285233,-0.946741688547105,-0.690202557406259,-0.226586568377443,-2.29517028488412,0.357237436614392,2.09461903572021,0.364700060189519,-0.743418707919708,1.56179611323686,2.03377758758435,1.24601790249401,-1.42099684010005,-0.446849828808209,-1.39457063597958,0.0252938896684254,-0.55554934269844,0.520734692673439,-1.70462826400743,-0.228701670688928,1.46480251942884,0.537775926841104,1.0295966972499,-0.164586151663249,-0.998887304104484,-1.16856228004515,-0.629795896206084,-0.292296677827437,-0.0746619911396131,2.26883470845682,0.74242242267853,0.492647339443029,-1.20592239840541,0.0467880289287076,-0.0271566286083511,-0.588748203825461,-1.63346882700306,0.763887005721979,-0.555871242158368,-0.307960625569718,-0.490324100073411,-2.48889335457985,-1.48525581311021,-1.28318884113513,0.65036329707392,0.725634608206117,-0.253792180788056,-0.36310860915731,-0.862619879818237,0.707101894801545,-1.00074120334536,-0.579715764377919,-0.309506212311182,-0.644623344437089,-0.482263702228782,1.07014316738114,0.415875535083338,0.393924392343508,0.945873255671804,-1.29235625778983,-1.79188244367221,-0.450005328461196,-0.337299118268984,0.471717207876615,-0.0806262024755627,-0.429423941759746,2.11240262152503,1.49314121785408,-0.01193411756403,0.0930683305874326,-2.27287700560835,0.686709980392845,-1.12152093185938,-1.94105211520947,-1.64628336271742,0.546076711873867,-0.241456111843856,-1.98597037485979,0.188370567635865,-0.387172358511681,-0.245821991678137,0.336092719533226,-0.176675606906903,-0.983450525312468,0.0263331730512765,-0.598229408674906,0.633472529020365,1.32232956138309,1.7756865973741,-1.85301418705152,0.137043462497138,-0.436172615418439,-0.211437688064619,0.265874308684863,0.680831378632575,0.401729594685546,-2.06860589546371,-0.643207362901597,0.98668818012645,0.196731504117709,0.218392274406055,0.241547941109399,-0.237220903813005,1.21362638698067,-1.17063477513433,-0.376062441184477,-0.134599736325175,1.05171161458086,-2.0995793784622,0.232927372083883,-1.2724332889769,1.48687028013214,1.80801651037469,0.872258403662032,0.545262217737207,-0.472344008135736,0.420561298756513,1.30481626926351,-1.59346667122712,-0.97755288186259,0.318611288958813,0.592645923682361,0.416768607830698,1.22419296828004,-1.08097025227281,0.485188048725088,0.781323179590227,0.841710738566218,1.29815193694797,-0.108163548132535,-1.21170302906696,1.69439885101936,-0.0358600093018221,-0.362023172095197,1.24736237065221,0.504138792324909,-0.79128588503243,0.512417446509929,-0.155787542962597,-1.63650087407345,1.06257093292241,-0.725677177791731,-0.523727246726668,-0.10248969581508,-0.192014713487261,0.389899759848953,-1.19049745353809,0.0196424720455954,0.910495047769987,-1.11910972225269,1.57764553125074,-0.771095427577848,0.00838612656670609,0.435431877706334,0.376746762627305,-0.53932175153976,-2.31196082396485,0.0306969911548524,-0.459322654358566,-0.297137431857877,1.55145196896409,0.265261830995147,2.14749940393435,0.997135361104189,1.23125365659886,-0.27252274261909,0.376655817229675,1.73272381000372,-0.31204616199578,-0.483466819257807,0.112300557005339,0.286708753650494,2.79949745645614,0.53927001670827,-1.36818720052359,-0.54260221199047,1.7970263557554,-0.0202644530718483,-1.30350953179312,1.1389068361134,-0.18294654005492,-0.829085675346343,0.982685132078709,0.766332254393744,0.377303699161862,0.59548020991685,0.250006820325614,-0.754960937987914,0.542900868153837,-1.33546151278147,0.742359103163122,-0.390564147345302,0.559595372851571,0.772780175880541,1.24547502352755,-1.59710870061485,0.712101072289725,-0.703736449970981,1.29072666398469,0.01157065727224,2.83924327114386,1.36272368001866,-0.11777034816582,-0.0293839593518106,0.142810004567624,1.02198609444276,0.101634126238116,-0.212879401274738,-1.09076361532911,0.252470265027488,0.3529879861464,1.00880150062942,1.78735471688134,-1.10707312357384,0.426404130081166,1.83338346929731,-0.554775001812782,1.07951120228441,-0.296342028774105,2.61121783338585,2.55441174938814,-0.36712812364722,-0.8414027842705,0.95224157346621,-0.435496072285881,0.248708531587267,-0.439274154670325,-0.865887315856099,1.11939329737855,-0.923414980339426,-0.763693267367301,0.536630964170375,0.188892605022192,1.57355688165229,-0.273365195549337,-0.938960210283012,0.100221179173378,0.721069265823844,-0.342435376891683,1.29969000763835,1.38792984240851,0.796259357429942,-0.0900989310358122,1.29625071329059,-0.688046638201807,1.14860020279796,-1.67072810455327,-1.55314269813798,-1.25894562428317,1.38178299108825,-1.34804099406892,-0.979247852705374,0.082307761788022,-1.3303075040078,-0.327002539563423,0.430230250955509,0.491576951569536,0.766348157449589,-0.333406099196733,-1.60204813843949,0.392911623928779,1.71801254420052,1.22347908341733,-0.352315103230174,1.233676463485,-1.50256178896695,2.01214188954756,-1.20115751658163,-1.55926370553416,-0.588363565097332,-0.416313443693249,0.227732900550895,-0.0270828624437531,1.35641718981602,-0.744272338907642,0.643924661114524,-0.899856991151532,0.446811182715155,0.0848028561314026,0.176501740118822,-1.15160014518824,0.108340795695143,0.665621416980531,-1.54243876663038,1.07640323361565,-0.0201286241787861,-0.290913632161658,1.51242647531122,-0.384238130223546,-1.88633210118266,1.83531236700359,1.48142036366979,0.105157864339272,1.2982109402778,0.110438538353686,-1.4845175000109,-0.780295329691304,-0.751223862405832,-0.0234270493949004,0.904972312142645,1.59953932103195,0.235471053064241,0.521125664545693,-0.531527890026482,-1.00631379029685,-0.354196918557472,-0.0447354192769612,-0.0194858343145161,1.75795675834547,0.170322862594262,0.0842906841777622,-0.23695885195517,-0.494004296350582,0.530806509086245,0.914898606151636,-2.42347199790865,-0.0851871420802471,-1.93693714643432,-2.4866807833742,-0.962686096643283,-0.4827745926852,-0.515682081218757,-1.33752222939837,0.647555343076581,0.13863625840267,0.658106091348684,-1.24611580606308,1.34889216905477,-1.75013055203175,0.447338059004789,1.02207539910993,1.30146644408028,0.213313558700574,-1.00399455210883,-0.945411835582842,0.876959574750787,0.978999472420669],[200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>y<\/th>\n      <th>y_pred<\/th>\n      <th>iter<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":25,"columnDefs":[{"className":"dt-right","targets":[1,2,3]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script><img src="rtorch-minimal-book_files/figure-html/dt-250-iters-2.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We see the formation of a line between the values and prediction, which means we are getting closer at finding the right algorithm, in this particular case, weights and bias.</p>
</div>
<div id="iterations-1" class="section level4" number="12.4.4.5">
<h4><span class="header-section-number">12.4.4.5</span> 500 iterations</h4>
<p>Let’s try one more time with 500 iterations:</p>
<div class="sourceCode" id="cb533"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb533-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb533-1" aria-hidden="true" tabindex="-1"></a>df_500 <span class="ot">&lt;-</span> <span class="fu">train</span>(<span class="at">iterations =</span> <span class="dv">500</span>)</span>
<span id="cb533-2"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb533-2" aria-hidden="true" tabindex="-1"></a><span class="fu">datatable</span>(df_500, <span class="at">options =</span> <span class="fu">list</span>(<span class="at">pageLength =</span> <span class="dv">25</span>))</span>
<span id="cb533-3"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb533-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df_500, <span class="fu">aes</span>(<span class="at">x =</span> y_pred, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb533-4"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb533-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>()</span></code></pre></div>
<p><div id="htmlwidget-5fc805ffa30354cf995b" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-5fc805ffa30354cf995b">{"x":{"filter":"none","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153","154","155","156","157","158","159","160","161","162","163","164","165","166","167","168","169","170","171","172","173","174","175","176","177","178","179","180","181","182","183","184","185","186","187","188","189","190","191","192","193","194","195","196","197","198","199","200","201","202","203","204","205","206","207","208","209","210","211","212","213","214","215","216","217","218","219","220","221","222","223","224","225","226","227","228","229","230","231","232","233","234","235","236","237","238","239","240","241","242","243","244","245","246","247","248","249","250","251","252","253","254","255","256","257","258","259","260","261","262","263","264","265","266","267","268","269","270","271","272","273","274","275","276","277","278","279","280","281","282","283","284","285","286","287","288","289","290","291","292","293","294","295","296","297","298","299","300","301","302","303","304","305","306","307","308","309","310","311","312","313","314","315","316","317","318","319","320","321","322","323","324","325","326","327","328","329","330","331","332","333","334","335","336","337","338","339","340","341","342","343","344","345","346","347","348","349","350","351","352","353","354","355","356","357","358","359","360","361","362","363","364","365","366","367","368","369","370","371","372","373","374","375","376","377","378","379","380","381","382","383","384","385","386","387","388","389","390","391","392","393","394","395","396","397","398","399","400","401","402","403","404","405","406","407","408","409","410","411","412","413","414","415","416","417","418","419","420","421","422","423","424","425","426","427","428","429","430","431","432","433","434","435","436","437","438","439","440","441","442","443","444","445","446","447","448","449","450","451","452","453","454","455","456","457","458","459","460","461","462","463","464","465","466","467","468","469","470","471","472","473","474","475","476","477","478","479","480","481","482","483","484","485","486","487","488","489","490","491","492","493","494","495","496","497","498","499","500","501","502","503","504","505","506","507","508","509","510","511","512","513","514","515","516","517","518","519","520","521","522","523","524","525","526","527","528","529","530","531","532","533","534","535","536","537","538","539","540","541","542","543","544","545","546","547","548","549","550","551","552","553","554","555","556","557","558","559","560","561","562","563","564","565","566","567","568","569","570","571","572","573","574","575","576","577","578","579","580","581","582","583","584","585","586","587","588","589","590","591","592","593","594","595","596","597","598","599","600","601","602","603","604","605","606","607","608","609","610","611","612","613","614","615","616","617","618","619","620","621","622","623","624","625","626","627","628","629","630","631","632","633","634","635","636","637","638","639","640"],[0.83994026831249,-0.00261469246496478,1.17331508976272,0.113064550776486,0.0103083101232198,0.741725300575136,0.757130556967624,-1.19798639091765,0.977469466953348,0.958784411472249,0.0208831428895932,-1.64816158891565,-0.0500028243495128,0.733813527145886,1.58085339989783,1.37525327872787,-0.386739371622718,-0.360212335648884,0.757263768677633,-2.33097748052543,-0.280231384151112,-0.958825812111196,-0.0248397757648495,1.0914318283759,-0.458192050288552,1.88579895675686,1.8374918101401,-0.102590503582433,0.158376834397865,-1.69505820821086,0.0630930916536182,1.3418620933296,0.173417858163622,-0.972149178523607,-0.312413823093578,2.16446071258402,-1.08569798526669,-0.570029361135791,0.180937256764405,-1.26229551939497,-1.36950465873129,-1.15493391741737,1.40785825831085,0.781023621423221,0.0780186960888742,2.11273169799433,-0.382805598728771,-0.332249209115966,0.833881240069712,0.59587340242551,-1.10900749622674,0.768900089169295,-1.35001396009045,0.483618658476077,-0.286807646207213,0.141828938278373,-0.374918514638461,-0.231434565088581,-0.00992419102991136,-2.11812756250465,-0.523762933650836,-1.32458158157352,0.431681037219444,0.925057143386304,0.93777525245556,0.546488768273407,1.11596946687424,0.570940155822875,-1.05140854030057,-0.318810634218592,-0.289448897409276,-0.418406412091964,0.781141791150043,-0.114891413907268,-0.845416526254121,-0.91845936893655,1.20789854386747,0.741312355103571,0.634427251618938,2.20585574787314,0.356161490453256,0.267242739519324,0.484831528360182,-0.165738361704601,0.189930678043718,-0.93651989042175,-1.45410391289995,0.233684800337273,0.502157215971716,0.28920149072855,-1.22551276737874,-0.385846452553303,1.42231442741027,-1.7116822994054,0.155053435413657,1.75487195590539,-0.480695737266194,-0.744335584259619,-2.13245053322139,-0.364322343399947,-0.233973774563064,0.111681118770621,-0.88914370304214,-0.722085375742162,-2.32991274785241,-0.479068930295669,0.789931238294979,1.06380136135981,-0.0798134682221333,-0.374168001227348,0.122704935131071,-1.43690716269235,-0.853944502445023,0.456289787735743,-0.0142516296486563,0.816751922214445,0.0849881110641102,0.446428132382165,2.42338976222122,-0.246995607668513,0.276176450241468,0.930116509469416,-1.02544718711831,0.104887979909664,-0.0142328430202254,2.03937753638296,0.373422932290684,1.43684984339667,0.948776626114924,-1.31827952307809,0.292905851185605,0.147308597580878,-1.03373092754513,0.419607277988786,-0.261092633509276,1.6071495149865,-0.647906145700861,0.657156199635267,-0.57098265859795,-0.818600322951857,1.31069089175351,1.81375842080092,-0.943433957369921,0.857269379202826,-0.263283789177259,1.31296346802011,-0.955590491841724,-1.56443546642085,-2.17226562414852,1.45591099037169,1.03814194129823,-0.197772383407226,-0.650297223745001,-1.33005360476313,0.921942504799078,0.752209085763665,-0.0361295533712819,2.63443405115608,0.287654631188146,0.655186314323695,0.400648171873719,-0.307311020383442,-0.0880147158656964,-1.41821667150147,-0.313515957169452,1.02977246630598,1.63557919370632,0.471406593501004,0.996474278664091,0.682501811766753,-0.971734007624617,-0.654540506542262,1.99500285811451,-1.64399263553111,-0.663557865235524,-1.68572596581282,-0.00487618116822535,0.458801236543731,0.5824022309138,-1.36287855122539,-1.27186820648675,0.714902868782501,-1.0165666698022,-0.0235973711331876,-0.0710369673541219,0.142253138818817,-0.675694865931503,-0.256829936236055,-0.101224481444322,-2.15395839517849,-0.964817551233336,-0.58900570028774,-1.77786400328839,1.14147095872602,-0.32772078240833,-1.35670287649227,0.415691882771873,-0.925932387905733,-1.61418322141891,-0.0867796005444939,0.601846649732015,1.12863379512717,0.184978687339329,0.156848912253547,-0.484058595314498,1.12522934475228,0.243459583230952,-0.634978878232694,-1.25090253023824,-1.43563909830578,1.13586385748573,0.215411392403429,1.86239774513426,1.11499996333691,-0.674626192034018,-0.434131329001274,1.03699553226858,-0.0366741372856878,-0.402271366973104,2.10246137335328,0.459018936796291,-0.25503953202401,-0.672610058718924,1.09980827026172,-0.410925835278326,-0.896790320270794,-1.03109344053961,-0.262748015975908,-0.333069074621615,0.977294210489194,0.678715733607743,1.25873526914719,0.42400531072313,1.79340828861104,0.294284737340495,-1.51557096175814,-0.566556739572426,0.0695601296483909,1.20367672097015,-0.549833339055226,0.120064475138402,0.307536795611604,1.21556005041361,0.12602924786391,-1.47214013475447,-0.318687822897892,0.279546357353257,2.70523892501022,1.68587064051207,1.1239941108417,0.424088292374247,0.104431077381569,-1.18021245837732,-0.916706926774142,-1.56221497392067,-0.851852947077733,-1.11863086095433,0.148312190622533,0.502631432428189,1.58774800017961,-1.10166271639973,-0.777449867610656,0.942870568479463,-1.9339574431339,0.00698050744702491,0.673684269774716,-1.39732869525216,0.283501034389694,0.874755970337202,-0.417645129576752,-1.36874292450096,-0.0747812970509281,0.0105724695173432,-0.0456309112554566,-1.47339841259021,1.33700556433092,1.94209421947211,0.284457070328279,-1.98482664413377,-0.50080737939018,-0.274676881900125,0.710938213804541,2.25851536328219,0.618237686663071,0.230927765626917,1.10122470908834,-0.572603422146998,-0.53302471140162,-0.717679874784871,-0.877038791682033,-0.748485888179382,0.184771684002625,-0.22231743531996,-1.00929695552901,-1.37047063999704,-1.07025776684948,-0.0066305826821924,0.00463777722911298,0.200669539977365,0.809383668396162,0.287278665935646,-0.303985246895298,0.139672450093511,0.127722862850529,-0.95565825905712,-0.799909006767028,-0.202940490973971,-2.41551300196281,0.3750531988884,2.0938362702894,0.605430623566889,-0.816263936823711,1.4935718662945,2.15379218446882,1.28063423391778,-1.16859150067533,-0.495931078057793,-1.12572347427127,-0.0289018780637711,-0.491217346285856,0.582465462265955,-1.73320910486715,-0.250616430567373,1.37773646638411,0.505183750595998,1.03329001588251,-0.148129803407604,-1.02024054280369,-1.16747702799092,-0.698262649012394,-0.29088420462642,-0.0873481684419898,2.28367061778488,0.799274120229441,0.496859660507907,-1.16771232655097,0.0560084744790094,0.0697664131694416,-0.63172486335937,-1.61579250963407,0.822541288820032,-0.585096534350738,-0.316981493648855,-0.445729843878749,-2.50838624530302,-1.39899463359675,-1.31102892051884,0.750402144358772,0.703404121659897,-0.25083983221813,-0.25095095379119,-0.872342684631607,0.628192536989848,-0.936217675649455,-0.550371466231215,-0.26970983693769,-0.688604841006888,-0.390605966140334,1.06988684039523,0.498849800595456,0.333419121051892,0.910556713584374,-1.27892253213922,-1.84182185333004,-0.512035156980371,-0.364119099300724,0.459286557995962,-0.0437559190006266,-0.453340737020902,2.02055656551793,1.47116567306761,-0.0115038122635589,0.131722917926454,-2.28429446135232,0.669356739346067,-1.14593193034066,-1.91291968267857,-1.67212741707599,0.54073309384294,-0.256724509418031,-1.89134380794797,0.161915204911539,-0.458809181262073,-0.194078077908355,0.296793193074685,-0.0250807732722297,-1.05794861180844,0.24484382118802,-0.648883034347149,0.618876699768757,1.44663045545316,1.76518871577172,-1.92334252201028,0.202212741132606,-0.342486128124399,-0.119772048017177,0.266562321266478,0.722029306985646,0.403631540478367,-1.9875531289521,-0.781171921052648,0.987488573315664,0.308690446320804,0.106148749253679,0.185085607568157,-0.390160876288962,1.25429513802065,-1.35013386465017,-0.342974766571647,-0.232894537629491,0.980332519980097,-2.11322741735,0.23024995771627,-1.45705269393795,1.38625292848739,1.6457152752694,0.886358263746316,0.296335125360452,-0.442944664529715,0.274627958703436,1.37404495680148,-1.61754075111507,-1.01356309656133,0.34151499813101,0.60561927302074,0.466358958410998,1.18788000370773,-0.998222009747409,0.472411048704897,0.802363134564957,0.778863518671328,1.33312992214265,-0.11483784787192,-1.24014458172761,1.68097845650274,-0.156295579276488,-0.36167688476951,1.12990338986994,0.544741774675208,-0.739519141431302,0.348344512782951,-0.10457968657704,-1.54862790298088,0.991859873430099,-0.712951101315977,-0.688713554688238,-0.019645395217261,-0.4276668324237,0.485554217337533,-1.22769785113341,-0.0839624525226322,0.929646496319533,-1.04399669558851,1.5862211735349,-0.762919277781499,-0.0482214954780706,0.482863741201961,0.272647805689435,-0.529870425645623,-2.32824908375099,0.0361073596021704,-0.451810115323141,-0.282254053923175,1.54911580277808,0.253374436546217,2.1590870111341,1.00793354498743,1.2238963859792,-0.262656713944417,0.371188568837448,1.84975076615656,-0.32057748245843,-0.484757190293574,0.119115296354525,0.311425735299618,2.86340591290978,0.561681683625529,-1.36773128004884,-0.521412125803238,1.8438836271771,0.15291957051073,-1.32938108961612,1.12345625118363,-0.0988379062841592,-0.781398448382246,1.13052436749178,0.767681931298354,0.525356584083132,0.582232899579348,0.333066984200484,-0.806258638926797,0.556552894435339,-1.30665318755998,0.785186326876701,-0.394106214647196,0.550730858543516,0.774230360594257,1.25782444024924,-1.59645579014647,0.726215347670778,-0.63694860577194,1.27493610125473,-0.0352439186929309,2.89168261447142,1.37501515427315,-0.0491456451207457,-0.0546139127595145,0.260510410587632,0.981880273079814,0.14277683851885,-0.254490023068397,-1.08396906594294,0.271948629915079,0.377547175371029,0.981224651122629,1.80754059091588,-1.1073823023655,0.482139414365283,1.8188538137154,-0.58749687491936,1.1093609884837,-0.297608176884006,2.57784620418014,2.52204599693865,-0.384840198277502,-0.855562334499144,0.938512269689899,-0.468196363866475,0.260008292451711,-0.462982716485726,-0.886275855262118,1.12411879590505,-0.909935517062699,-0.751334836930073,0.514366756631972,0.195860128347231,1.55471787782953,-0.253467892483979,-0.944270533093175,0.0655037556538774,0.669046506691992,-0.329793629158233,1.30135750702321,1.40114405750616,0.796269800703752,-0.10853585230126,1.29063556898416,-0.693149828347284,1.15340154198528,-1.67650319533252,-1.49429593747768,-1.273469908539,1.31161289723836,-1.31181443286064,-0.950226359402931,0.144663430311268,-1.37563897316271,-0.229895425421889,0.419692024617746,0.516239563506011,0.740669495728343,-0.361952001881317,-1.60995844987436,0.327757025857471,1.6432950871785,1.20535256388868,-0.353510280844418,1.21473389654699,-1.53240226188522,1.92233286176372,-1.09253954985412,-1.59184155325035,-0.64646780319668,-0.426883748541837,0.209569862271875,0.0415756254512372,1.33238165750511,-0.681027400604152,0.620571329940411,-0.900045782913094,0.309651318876625,0.118175407084332,0.227812270228395,-1.18930438569969,0.0837636991730383,0.588009163095429,-1.5092271780105,0.995898635439402,-0.00639701815793872,-0.3502219106773,1.4288096764155,-0.372525850460849,-1.86657115947184,1.73533502830238,1.4419101995773,-0.0083570684921407,1.3092622868551,0.000392255402139403,-1.45683485419048,-0.844996213277658,-0.60477119564568,-0.0517937465881769,0.821273104631407,1.50318458611856,0.26033919453097,0.521628843921831,-0.525259906808531,-1.07777898882352,-0.340595705086819,-0.0688224810312909,0.024696280077251,1.76093398424226,0.175448866859079,0.138406693673293,-0.206063763800319,-0.435211393481228,0.529893499013784,0.967447102676697,-2.43106301578848,-0.0424933305199116,-1.99868317055989,-2.47557651859656,-0.965041073936391,-0.435050040400478,-0.507170912986636,-1.37604070291559,0.641342899780833,0.15067215935219,0.647532140383024,-1.21460078326391,1.46931171546606,-1.78034791140857,0.39149639637274,0.953942227218803,1.27980367071278,0.263325295304111,-1.03877191179501,-0.962563912956559,0.895371173457089,0.905913690706474],[0.839932083113576,-0.00259619575606763,1.17331875830639,0.113064853997286,0.010318840802271,0.741730549570822,0.757139315220379,-1.19795856420824,0.977472444600644,0.958780004495673,0.0209043778914685,-1.64817402225055,-0.0500123823044297,0.73382640863664,1.58084379965567,1.37529194360188,-0.386758689324995,-0.360229939684905,0.75726596970969,-2.33097394681116,-0.280230457291781,-0.958896373504595,-0.0247948874748447,1.09135671877028,-0.458233065262309,1.8857429384508,1.83748538301562,-0.102699922080712,0.158405783847187,-1.69502511639908,0.0630936664529361,1.34160813963655,0.173548139921436,-0.972376097928472,-0.312463456288052,2.16409325051549,-1.08577887826862,-0.570287896781291,0.180972504748383,-1.26224450313497,-1.3695006244676,-1.15494789238684,1.40786462474266,0.780996277883666,0.0780012128836287,2.11271991948832,-0.382813331920143,-0.332286779965212,0.833875147471968,0.595894760761613,-1.10901558012,0.768897653578126,-1.35000699101197,0.483613872486335,-0.286816095207971,0.141814757333787,-0.374908317643679,-0.231457800767433,-0.00993505878788931,-2.11811637248379,-0.52376047227007,-1.32451166383377,0.431617828221607,0.925129854529669,0.937815581227657,0.546667033934666,1.11596160336241,0.571012747407698,-1.05139158044465,-0.318833937329961,-0.28945013133735,-0.418459527782708,0.781183380744101,-0.114935332836483,-0.845410111277185,-0.918561707122638,1.20789792574914,0.741276696600453,0.634434905822737,2.20587145424306,0.356171415690469,0.267280033337384,0.484786937221845,-0.165691443963711,0.189975025790522,-0.93644099053981,-1.4540959022571,0.233745739025352,0.502176454824269,0.289190010569311,-1.22548216865296,-0.385807730207528,1.4222613004572,-1.71162285591988,0.155044458412923,1.75503816113068,-0.480722488856943,-0.744304527358296,-2.13246177835895,-0.364333842791959,-0.233970596383373,0.111655287689559,-0.889142659818261,-0.722113112123572,-2.32990048286908,-0.479104114510788,0.789919021068439,1.06378348036568,-0.0798101922004653,-0.374171319370942,0.122698258480636,-1.43685071466232,-0.853964975576553,0.45633619933423,-0.0142109637049264,0.816793898996989,0.0850284881035821,0.446528776544619,2.42339999017523,-0.247038844219921,0.276162201768473,0.930090356923581,-1.02540698615012,0.104848658465893,-0.0142222739041787,2.03928705968103,0.373436572710614,1.43683521212489,0.948795154780952,-1.31829417508905,0.292902269705162,0.147265447664038,-1.03369488968688,0.419601802757214,-0.26116236835031,1.60714718709274,-0.647923730709604,0.657055310952598,-0.570982301172999,-0.818555322700522,1.31069933692457,1.81366947360699,-0.943343708079675,0.8571761487393,-0.263359586071382,1.31275783833098,-0.955585470275301,-1.56458519654675,-2.17227423027216,1.45594946795327,1.03812355282491,-0.19777843225781,-0.650260810605541,-1.33010465280895,0.921943613382373,0.752099897059453,-0.0361035298042242,2.63442460113103,0.287654542337084,0.655203442279378,0.400669025844542,-0.307231297205133,-0.0880933318525192,-1.41809584491378,-0.313470258363928,1.02993899352875,1.63557041758005,0.471511110548767,0.996478130401106,0.682477793970818,-0.971708681426492,-0.654502797423659,1.99494025044585,-1.64394404461996,-0.663526676792116,-1.68555668161939,-0.00489117748335132,0.458902497939992,0.582408678848746,-1.36291538884029,-1.27188088771683,0.714905835134309,-1.01655910086827,-0.0236144952677847,-0.0710216477944342,0.142202031986009,-0.675672283372405,-0.256807070402145,-0.101221701228006,-2.15397647564413,-0.96481268748325,-0.589022829042547,-1.77784923392807,1.14145209968849,-0.327729353354482,-1.35675349495689,0.415693384037307,-0.925950582336304,-1.61418987119831,-0.0867621696332334,0.601844691761306,1.12868261468668,0.184951361817435,0.156884087063293,-0.484045795498371,1.1252907386786,0.243476753083458,-0.634941206650473,-1.25091171530619,-1.43564313901342,1.13585625836998,0.215382520892851,1.86241874634666,1.11497613857211,-0.674655227172949,-0.434182962583405,1.03698751936025,-0.0367039940476786,-0.402276938339013,2.10245597019289,0.459003897168083,-0.2550929792321,-0.672555892991385,1.09970638415519,-0.410934296895084,-0.896927707406815,-1.03108492017236,-0.262795193602035,-0.33306357747683,0.977312470976883,0.678685792946961,1.25861755454366,0.424121489924853,1.79324076170642,0.294240135071932,-1.51587856749602,-0.566524647798016,0.0693937683249327,1.20366235962229,-0.549742457547072,0.120060059968974,0.307503184029084,1.21559183626761,0.12599869407511,-1.47214389956581,-0.318762097526372,0.27953523077579,2.70517688596609,1.68585793615529,1.12401712670912,0.424093564788407,0.104591454299667,-1.18033436638111,-0.916557863229089,-1.56216995140384,-0.851593623792894,-1.11860132721208,0.148466612256178,0.502630070090466,1.58769701225849,-1.10168452104251,-0.777557268527187,0.942977637137774,-1.93407049045038,0.00692057792853519,0.673501428641865,-1.39732872099231,0.283362364331555,0.874774846339744,-0.417592865434802,-1.36873507442405,-0.074780638156611,0.0105670939901195,-0.0456454506981905,-1.47339031594443,1.33699234002235,1.94208753069995,0.284473239588992,-1.98482822470314,-0.500809398163269,-0.27465825356606,0.71096830393941,2.25847856696057,0.618289303483039,0.230944591563464,1.10131934305156,-0.572617841726008,-0.532996778265258,-0.71767534303731,-0.877046331784384,-0.748479521260524,0.184867023215922,-0.222414506563467,-1.00917489355176,-1.37048704727301,-1.07000231729456,-0.00663336055391075,0.0046866048007366,0.200654729342558,0.809368627472502,0.287257428583349,-0.303995184881959,0.139702775233948,0.127690399550019,-0.955663527846297,-0.80001338506427,-0.202922402465098,-2.41551072743716,0.375046038453804,2.09383587710965,0.605416442532019,-0.816271658677488,1.4935688463732,2.15381199342431,1.28064156968024,-1.16859831155495,-0.495932350273419,-1.125713409468,-0.028889574269459,-0.491227223719978,0.5824761376403,-1.73316390328493,-0.250684215938911,1.37780680597715,0.505232091444913,1.03344194804974,-0.148151960767324,-1.02016810406335,-1.16748356024622,-0.698289569015988,-0.290896524373538,-0.0873317580265057,2.28364441039843,0.79932631017063,0.49688121972398,-1.16765257942767,0.0559936855538421,0.0698015445148567,-0.631730736962868,-1.61583870723503,0.822540677698252,-0.585089658572006,-0.316985451621209,-0.445728454376598,-2.5083727720648,-1.39899267456753,-1.31102568256435,0.750416140696527,0.703408601835226,-0.250842139801944,-0.250931718917849,-0.872286941566438,0.628123948856593,-0.936167049851316,-0.550332514170009,-0.269581811595385,-0.688621863247659,-0.390548157583217,1.06988544057187,0.498809805707848,0.333414509511711,0.910620921460825,-1.27894154128676,-1.84176881545478,-0.51196951057102,-0.364093769403196,0.459335451829423,-0.0436102154760407,-0.453324557347009,2.02050507263852,1.47112945884243,-0.0115007620531746,0.131772114414162,-2.28432527727926,0.669358831486007,-1.14605297558252,-1.9128814260031,-1.67214035243977,0.5407396224472,-0.256718780231315,-1.89131452765759,0.16194197450802,-0.458867473747144,-0.193982920140097,0.29679452235816,-0.0248717035831861,-1.05800715709835,0.244842455042448,-0.648886524970344,0.618878350387447,1.44658230968475,1.76512383060006,-1.92325119655149,0.202106893005736,-0.342513020362825,-0.120001239541284,0.266597248661844,0.721959385109973,0.40363234560977,-1.98751645656053,-0.781149484824183,0.987511150562542,0.308657764402358,0.106179265354783,0.185103579063681,-0.390073357767878,1.25428576795632,-1.35011219547237,-0.342969454538681,-0.232890312324549,0.980309810906135,-2.11327743840575,0.230304772156053,-1.4571150834923,1.38619773665496,1.64561503913782,0.886359250819167,0.296244989645571,-0.442950260030984,0.274643969107049,1.37404928864477,-1.6175232777018,-1.01358475029733,0.341567502478649,0.60561537986351,0.466413049443511,1.18787818614321,-0.998218282516214,0.472406493704255,0.802360667546199,0.778871469823151,1.33310984714724,-0.114831462914021,-1.24017202598167,1.68100590739283,-0.15633390784057,-0.36167609212776,1.12991666296397,0.544753678603071,-0.739512713901847,0.348336296506843,-0.104647388051992,-1.54853561567333,0.991697173920341,-0.712979705371256,-0.688954007152984,-0.0196266778241499,-0.427762346447595,0.485538124379856,-1.22765648631783,-0.083960850212507,0.929455519211295,-1.04384646411585,1.58598957121596,-0.763090172449313,-0.0484705813412443,0.482795985361656,0.272231777841424,-0.529927460041514,-2.32804557710724,0.0361073682627899,-0.451793590871618,-0.282264326599793,1.54913992182166,0.253377812105607,2.15913968342837,1.00793569459406,1.22392743399602,-0.262649821660099,0.371167166691393,1.84974230384451,-0.320662836222097,-0.484696685120489,0.119058094537442,0.311389394450805,2.86326145950885,0.561660116396575,-1.36784084165547,-0.521413318443614,1.8439062147309,0.152895592171527,-1.32920019262623,1.12332762670839,-0.0986781375952893,-0.781257016533239,1.13076098631002,0.767731998704895,0.525653835668583,0.582270365708368,0.332974322904636,-0.806319643281193,0.556406090906282,-1.30645395347009,0.784941553255604,-0.394255663470198,0.550288661010561,0.774277749074034,1.25766481539557,-1.59643991317239,0.726281103948063,-0.637004356852899,1.27487275247584,-0.0351556271009118,2.89160793585858,1.37494274194516,-0.0493800350744623,-0.0546139706884271,0.26042514391559,0.981866403207689,0.142788179240168,-0.254500581951462,-1.08393829993601,0.271928503984513,0.377553744139634,0.981267252018799,1.80754902543394,-1.10737335433801,0.482225506316817,1.81886240946367,-0.587533117915313,1.10937264859286,-0.297584327128564,2.57780345390015,2.52209598766456,-0.384831612054264,-0.85546706357272,0.938502054611941,-0.468165071625351,0.260001961733677,-0.463015512560948,-0.886240280795104,1.12421016229078,-0.910080616911479,-0.751136496148682,0.514392069233966,0.196135931220853,1.55467825939048,-0.253373978864477,-0.944292798144764,0.0654709941739323,0.669042865874539,-0.329938333369514,1.30144371117829,1.40097331408481,0.796229746080684,-0.108778199182515,1.29064490262109,-0.693269423517586,1.15340784227077,-1.67646410673983,-1.49427692853035,-1.27351657912164,1.31161127502526,-1.31181787527117,-0.95025150038726,0.144660911087438,-1.3756754906712,-0.229942296364522,0.419676338583217,0.516244299604565,0.740700053876854,-0.361858376494823,-1.6100267144655,0.327904602758803,1.64328297971567,1.20564911021629,-0.353527582762891,1.21473695657277,-1.53246532290837,1.92235481174235,-1.09255242419554,-1.59184466957543,-0.646442121336835,-0.426901372997696,0.209573020769598,0.041524952679628,1.33239795898591,-0.681002143601049,0.620588127452734,-0.900056295829631,0.309659580766708,0.118245768378044,0.227779209299565,-1.18925834469809,0.0837886195449669,0.588093918374028,-1.50920005119344,0.995963990416468,-0.00640702565391737,-0.350223166809029,1.42888382018416,-0.372328001827925,-1.86673663632001,1.73555986051223,1.44199292277228,-0.00793908407193417,1.30925868207313,0.000688096345209924,-1.45685228927642,-0.845075936663367,-0.604768685219645,-0.0517572381828703,0.821246209231927,1.50324445088061,0.260368916565616,0.521692863803358,-0.525250951588798,-1.07770081366436,-0.340590578094599,-0.0688542919744606,0.0247178375091149,1.76096935543204,0.175389864857516,0.138477847089169,-0.206026399108051,-0.435064679043954,0.529863878658713,0.967512772881203,-2.43105901782895,-0.0425199340526495,-1.99866967085269,-2.4755115595499,-0.965082220611204,-0.435002851149166,-0.507123617634028,-1.3759728424062,0.641369841232132,0.150751328397364,0.647535891296491,-1.21461161312482,1.46931446400968,-1.78029882732569,0.391454865095158,0.954031408083873,1.27980677379116,0.263434724733184,-1.03876745265014,-0.962508057624333,0.895370500046852,0.905874674107558],[500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>y<\/th>\n      <th>y_pred<\/th>\n      <th>iter<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":25,"columnDefs":[{"className":"dt-right","targets":[1,2,3]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script><img src="rtorch-minimal-book_files/figure-html/dt-500-iters-2.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
<div id="complete-code-for-neural-network-in-rtorch" class="section level2" number="12.5">
<h2><span class="header-section-number">12.5</span> Complete code for neural network in rTorch</h2>
<div class="sourceCode" id="cb534"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb534-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rTorch)</span>
<span id="cb534-2"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb534-3"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tictoc)</span>
<span id="cb534-4"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb534-5"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-5" aria-hidden="true" tabindex="-1"></a><span class="fu">tic</span>()</span>
<span id="cb534-6"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-6" aria-hidden="true" tabindex="-1"></a>device <span class="ot">=</span> torch<span class="sc">$</span><span class="fu">device</span>(<span class="st">&#39;cpu&#39;</span>)</span>
<span id="cb534-7"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-7" aria-hidden="true" tabindex="-1"></a><span class="co"># device = torch.device(&#39;cuda&#39;)  # Uncomment this to run on GPU</span></span>
<span id="cb534-8"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-8" aria-hidden="true" tabindex="-1"></a><span class="fu">invisible</span>(torch<span class="sc">$</span><span class="fu">manual_seed</span>(<span class="dv">0</span>))</span>
<span id="cb534-9"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb534-10"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Properties of tensors and neural network</span></span>
<span id="cb534-11"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-11" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> 64L; D_in <span class="ot">&lt;-</span> 1000L; H <span class="ot">&lt;-</span> 100L; D_out <span class="ot">&lt;-</span> 10L</span>
<span id="cb534-12"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb534-13"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Create random Tensors to hold inputs and outputs</span></span>
<span id="cb534-14"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-14" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">randn</span>(N, D_in, <span class="at">device=</span>device)</span>
<span id="cb534-15"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-15" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">randn</span>(N, D_out, <span class="at">device=</span>device)</span>
<span id="cb534-16"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-16" aria-hidden="true" tabindex="-1"></a><span class="co"># dimensions of both tensors</span></span>
<span id="cb534-17"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb534-18"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-18" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize the weights</span></span>
<span id="cb534-19"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-19" aria-hidden="true" tabindex="-1"></a>w1 <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">randn</span>(D_in, H, <span class="at">device=</span>device)   <span class="co"># layer 1</span></span>
<span id="cb534-20"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-20" aria-hidden="true" tabindex="-1"></a>w2 <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">randn</span>(H, D_out, <span class="at">device=</span>device)  <span class="co"># layer 2</span></span>
<span id="cb534-21"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb534-22"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-22" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="ot">=</span> <span class="fl">1e-6</span></span>
<span id="cb534-23"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-23" aria-hidden="true" tabindex="-1"></a><span class="co"># loop</span></span>
<span id="cb534-24"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">500</span>) {</span>
<span id="cb534-25"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-25" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Forward pass: compute predicted y, y_pred</span></span>
<span id="cb534-26"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-26" aria-hidden="true" tabindex="-1"></a>  h <span class="ot">&lt;-</span> x<span class="sc">$</span><span class="fu">mm</span>(w1)              <span class="co"># matrix multiplication, x*w1</span></span>
<span id="cb534-27"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-27" aria-hidden="true" tabindex="-1"></a>  h_relu <span class="ot">&lt;-</span> h<span class="sc">$</span><span class="fu">clamp</span>(<span class="at">min=</span><span class="dv">0</span>)   <span class="co"># make elements greater than zero</span></span>
<span id="cb534-28"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-28" aria-hidden="true" tabindex="-1"></a>  y_pred <span class="ot">&lt;-</span> h_relu<span class="sc">$</span><span class="fu">mm</span>(w2)    <span class="co"># matrix multiplication, h_relu*w2</span></span>
<span id="cb534-29"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb534-30"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-30" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor</span></span>
<span id="cb534-31"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-31" aria-hidden="true" tabindex="-1"></a>  <span class="co"># of shape (); we can get its value as a Python number with loss.item().</span></span>
<span id="cb534-32"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-32" aria-hidden="true" tabindex="-1"></a>  loss <span class="ot">&lt;-</span> (torch<span class="sc">$</span><span class="fu">sub</span>(y_pred, y))<span class="sc">$</span><span class="fu">pow</span>(<span class="dv">2</span>)<span class="sc">$</span><span class="fu">sum</span>()   <span class="co"># sum((y_pred-y)^2)</span></span>
<span id="cb534-33"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-33" aria-hidden="true" tabindex="-1"></a>  <span class="co"># cat(t, &quot;\t&quot;)</span></span>
<span id="cb534-34"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-34" aria-hidden="true" tabindex="-1"></a>  <span class="co"># cat(loss$item(), &quot;\n&quot;)</span></span>
<span id="cb534-35"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb534-36"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-36" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span>
<span id="cb534-37"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-37" aria-hidden="true" tabindex="-1"></a>  grad_y_pred <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">mul</span>(torch<span class="sc">$</span><span class="fu">scalar_tensor</span>(<span class="fl">2.0</span>), torch<span class="sc">$</span><span class="fu">sub</span>(y_pred, y))</span>
<span id="cb534-38"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-38" aria-hidden="true" tabindex="-1"></a>  grad_w2 <span class="ot">&lt;-</span> h_relu<span class="sc">$</span><span class="fu">t</span>()<span class="sc">$</span><span class="fu">mm</span>(grad_y_pred)        <span class="co"># compute gradient of w2</span></span>
<span id="cb534-39"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-39" aria-hidden="true" tabindex="-1"></a>  grad_h_relu <span class="ot">&lt;-</span> grad_y_pred<span class="sc">$</span><span class="fu">mm</span>(w2<span class="sc">$</span><span class="fu">t</span>())</span>
<span id="cb534-40"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-40" aria-hidden="true" tabindex="-1"></a>  grad_h <span class="ot">&lt;-</span> grad_h_relu<span class="sc">$</span><span class="fu">clone</span>()</span>
<span id="cb534-41"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-41" aria-hidden="true" tabindex="-1"></a>  mask <span class="ot">&lt;-</span> grad_h<span class="sc">$</span><span class="fu">lt</span>(<span class="dv">0</span>)                         <span class="co"># filter values lower than zero </span></span>
<span id="cb534-42"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-42" aria-hidden="true" tabindex="-1"></a>  torch<span class="sc">$</span><span class="fu">masked_select</span>(grad_h, mask)<span class="sc">$</span><span class="fu">fill_</span>(<span class="fl">0.0</span>) <span class="co"># make them equal to zero</span></span>
<span id="cb534-43"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-43" aria-hidden="true" tabindex="-1"></a>  grad_w1 <span class="ot">&lt;-</span> x<span class="sc">$</span><span class="fu">t</span>()<span class="sc">$</span><span class="fu">mm</span>(grad_h)                  <span class="co"># compute gradient of w1</span></span>
<span id="cb534-44"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-44" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb534-45"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-45" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Update weights using gradient descent</span></span>
<span id="cb534-46"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-46" aria-hidden="true" tabindex="-1"></a>  w1 <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">sub</span>(w1, torch<span class="sc">$</span><span class="fu">mul</span>(learning_rate, grad_w1))</span>
<span id="cb534-47"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-47" aria-hidden="true" tabindex="-1"></a>  w2 <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">sub</span>(w2, torch<span class="sc">$</span><span class="fu">mul</span>(learning_rate, grad_w2))</span>
<span id="cb534-48"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-48" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb534-49"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-49" aria-hidden="true" tabindex="-1"></a><span class="co"># y vs predicted y</span></span>
<span id="cb534-50"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-50" aria-hidden="true" tabindex="-1"></a>df<span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">y =</span> y<span class="sc">$</span><span class="fu">flatten</span>()<span class="sc">$</span><span class="fu">numpy</span>(), </span>
<span id="cb534-51"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-51" aria-hidden="true" tabindex="-1"></a>                    <span class="at">y_pred =</span> y_pred<span class="sc">$</span><span class="fu">flatten</span>()<span class="sc">$</span><span class="fu">numpy</span>(), <span class="at">iter =</span> <span class="dv">500</span>)</span>
<span id="cb534-52"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-52" aria-hidden="true" tabindex="-1"></a><span class="fu">datatable</span>(df, <span class="at">options =</span> <span class="fu">list</span>(<span class="at">pageLength =</span> <span class="dv">25</span>))</span>
<span id="cb534-53"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-53" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> y_pred, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb534-54"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-54" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>()</span>
<span id="cb534-55"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb534-56"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb534-56" aria-hidden="true" tabindex="-1"></a><span class="fu">toc</span>()</span></code></pre></div>
<div id="htmlwidget-fff8ed89c1ad8f04e039" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-fff8ed89c1ad8f04e039">{"x":{"filter":"none","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153","154","155","156","157","158","159","160","161","162","163","164","165","166","167","168","169","170","171","172","173","174","175","176","177","178","179","180","181","182","183","184","185","186","187","188","189","190","191","192","193","194","195","196","197","198","199","200","201","202","203","204","205","206","207","208","209","210","211","212","213","214","215","216","217","218","219","220","221","222","223","224","225","226","227","228","229","230","231","232","233","234","235","236","237","238","239","240","241","242","243","244","245","246","247","248","249","250","251","252","253","254","255","256","257","258","259","260","261","262","263","264","265","266","267","268","269","270","271","272","273","274","275","276","277","278","279","280","281","282","283","284","285","286","287","288","289","290","291","292","293","294","295","296","297","298","299","300","301","302","303","304","305","306","307","308","309","310","311","312","313","314","315","316","317","318","319","320","321","322","323","324","325","326","327","328","329","330","331","332","333","334","335","336","337","338","339","340","341","342","343","344","345","346","347","348","349","350","351","352","353","354","355","356","357","358","359","360","361","362","363","364","365","366","367","368","369","370","371","372","373","374","375","376","377","378","379","380","381","382","383","384","385","386","387","388","389","390","391","392","393","394","395","396","397","398","399","400","401","402","403","404","405","406","407","408","409","410","411","412","413","414","415","416","417","418","419","420","421","422","423","424","425","426","427","428","429","430","431","432","433","434","435","436","437","438","439","440","441","442","443","444","445","446","447","448","449","450","451","452","453","454","455","456","457","458","459","460","461","462","463","464","465","466","467","468","469","470","471","472","473","474","475","476","477","478","479","480","481","482","483","484","485","486","487","488","489","490","491","492","493","494","495","496","497","498","499","500","501","502","503","504","505","506","507","508","509","510","511","512","513","514","515","516","517","518","519","520","521","522","523","524","525","526","527","528","529","530","531","532","533","534","535","536","537","538","539","540","541","542","543","544","545","546","547","548","549","550","551","552","553","554","555","556","557","558","559","560","561","562","563","564","565","566","567","568","569","570","571","572","573","574","575","576","577","578","579","580","581","582","583","584","585","586","587","588","589","590","591","592","593","594","595","596","597","598","599","600","601","602","603","604","605","606","607","608","609","610","611","612","613","614","615","616","617","618","619","620","621","622","623","624","625","626","627","628","629","630","631","632","633","634","635","636","637","638","639","640"],[0.83994026831249,-0.00261469246496478,1.17331508976272,0.113064550776486,0.0103083101232198,0.741725300575136,0.757130556967624,-1.19798639091765,0.977469466953348,0.958784411472249,0.0208831428895932,-1.64816158891565,-0.0500028243495128,0.733813527145886,1.58085339989783,1.37525327872787,-0.386739371622718,-0.360212335648884,0.757263768677633,-2.33097748052543,-0.280231384151112,-0.958825812111196,-0.0248397757648495,1.0914318283759,-0.458192050288552,1.88579895675686,1.8374918101401,-0.102590503582433,0.158376834397865,-1.69505820821086,0.0630930916536182,1.3418620933296,0.173417858163622,-0.972149178523607,-0.312413823093578,2.16446071258402,-1.08569798526669,-0.570029361135791,0.180937256764405,-1.26229551939497,-1.36950465873129,-1.15493391741737,1.40785825831085,0.781023621423221,0.0780186960888742,2.11273169799433,-0.382805598728771,-0.332249209115966,0.833881240069712,0.59587340242551,-1.10900749622674,0.768900089169295,-1.35001396009045,0.483618658476077,-0.286807646207213,0.141828938278373,-0.374918514638461,-0.231434565088581,-0.00992419102991136,-2.11812756250465,-0.523762933650836,-1.32458158157352,0.431681037219444,0.925057143386304,0.93777525245556,0.546488768273407,1.11596946687424,0.570940155822875,-1.05140854030057,-0.318810634218592,-0.289448897409276,-0.418406412091964,0.781141791150043,-0.114891413907268,-0.845416526254121,-0.91845936893655,1.20789854386747,0.741312355103571,0.634427251618938,2.20585574787314,0.356161490453256,0.267242739519324,0.484831528360182,-0.165738361704601,0.189930678043718,-0.93651989042175,-1.45410391289995,0.233684800337273,0.502157215971716,0.28920149072855,-1.22551276737874,-0.385846452553303,1.42231442741027,-1.7116822994054,0.155053435413657,1.75487195590539,-0.480695737266194,-0.744335584259619,-2.13245053322139,-0.364322343399947,-0.233973774563064,0.111681118770621,-0.88914370304214,-0.722085375742162,-2.32991274785241,-0.479068930295669,0.789931238294979,1.06380136135981,-0.0798134682221333,-0.374168001227348,0.122704935131071,-1.43690716269235,-0.853944502445023,0.456289787735743,-0.0142516296486563,0.816751922214445,0.0849881110641102,0.446428132382165,2.42338976222122,-0.246995607668513,0.276176450241468,0.930116509469416,-1.02544718711831,0.104887979909664,-0.0142328430202254,2.03937753638296,0.373422932290684,1.43684984339667,0.948776626114924,-1.31827952307809,0.292905851185605,0.147308597580878,-1.03373092754513,0.419607277988786,-0.261092633509276,1.6071495149865,-0.647906145700861,0.657156199635267,-0.57098265859795,-0.818600322951857,1.31069089175351,1.81375842080092,-0.943433957369921,0.857269379202826,-0.263283789177259,1.31296346802011,-0.955590491841724,-1.56443546642085,-2.17226562414852,1.45591099037169,1.03814194129823,-0.197772383407226,-0.650297223745001,-1.33005360476313,0.921942504799078,0.752209085763665,-0.0361295533712819,2.63443405115608,0.287654631188146,0.655186314323695,0.400648171873719,-0.307311020383442,-0.0880147158656964,-1.41821667150147,-0.313515957169452,1.02977246630598,1.63557919370632,0.471406593501004,0.996474278664091,0.682501811766753,-0.971734007624617,-0.654540506542262,1.99500285811451,-1.64399263553111,-0.663557865235524,-1.68572596581282,-0.00487618116822535,0.458801236543731,0.5824022309138,-1.36287855122539,-1.27186820648675,0.714902868782501,-1.0165666698022,-0.0235973711331876,-0.0710369673541219,0.142253138818817,-0.675694865931503,-0.256829936236055,-0.101224481444322,-2.15395839517849,-0.964817551233336,-0.58900570028774,-1.77786400328839,1.14147095872602,-0.32772078240833,-1.35670287649227,0.415691882771873,-0.925932387905733,-1.61418322141891,-0.0867796005444939,0.601846649732015,1.12863379512717,0.184978687339329,0.156848912253547,-0.484058595314498,1.12522934475228,0.243459583230952,-0.634978878232694,-1.25090253023824,-1.43563909830578,1.13586385748573,0.215411392403429,1.86239774513426,1.11499996333691,-0.674626192034018,-0.434131329001274,1.03699553226858,-0.0366741372856878,-0.402271366973104,2.10246137335328,0.459018936796291,-0.25503953202401,-0.672610058718924,1.09980827026172,-0.410925835278326,-0.896790320270794,-1.03109344053961,-0.262748015975908,-0.333069074621615,0.977294210489194,0.678715733607743,1.25873526914719,0.42400531072313,1.79340828861104,0.294284737340495,-1.51557096175814,-0.566556739572426,0.0695601296483909,1.20367672097015,-0.549833339055226,0.120064475138402,0.307536795611604,1.21556005041361,0.12602924786391,-1.47214013475447,-0.318687822897892,0.279546357353257,2.70523892501022,1.68587064051207,1.1239941108417,0.424088292374247,0.104431077381569,-1.18021245837732,-0.916706926774142,-1.56221497392067,-0.851852947077733,-1.11863086095433,0.148312190622533,0.502631432428189,1.58774800017961,-1.10166271639973,-0.777449867610656,0.942870568479463,-1.9339574431339,0.00698050744702491,0.673684269774716,-1.39732869525216,0.283501034389694,0.874755970337202,-0.417645129576752,-1.36874292450096,-0.0747812970509281,0.0105724695173432,-0.0456309112554566,-1.47339841259021,1.33700556433092,1.94209421947211,0.284457070328279,-1.98482664413377,-0.50080737939018,-0.274676881900125,0.710938213804541,2.25851536328219,0.618237686663071,0.230927765626917,1.10122470908834,-0.572603422146998,-0.53302471140162,-0.717679874784871,-0.877038791682033,-0.748485888179382,0.184771684002625,-0.22231743531996,-1.00929695552901,-1.37047063999704,-1.07025776684948,-0.0066305826821924,0.00463777722911298,0.200669539977365,0.809383668396162,0.287278665935646,-0.303985246895298,0.139672450093511,0.127722862850529,-0.95565825905712,-0.799909006767028,-0.202940490973971,-2.41551300196281,0.3750531988884,2.0938362702894,0.605430623566889,-0.816263936823711,1.4935718662945,2.15379218446882,1.28063423391778,-1.16859150067533,-0.495931078057793,-1.12572347427127,-0.0289018780637711,-0.491217346285856,0.582465462265955,-1.73320910486715,-0.250616430567373,1.37773646638411,0.505183750595998,1.03329001588251,-0.148129803407604,-1.02024054280369,-1.16747702799092,-0.698262649012394,-0.29088420462642,-0.0873481684419898,2.28367061778488,0.799274120229441,0.496859660507907,-1.16771232655097,0.0560084744790094,0.0697664131694416,-0.63172486335937,-1.61579250963407,0.822541288820032,-0.585096534350738,-0.316981493648855,-0.445729843878749,-2.50838624530302,-1.39899463359675,-1.31102892051884,0.750402144358772,0.703404121659897,-0.25083983221813,-0.25095095379119,-0.872342684631607,0.628192536989848,-0.936217675649455,-0.550371466231215,-0.26970983693769,-0.688604841006888,-0.390605966140334,1.06988684039523,0.498849800595456,0.333419121051892,0.910556713584374,-1.27892253213922,-1.84182185333004,-0.512035156980371,-0.364119099300724,0.459286557995962,-0.0437559190006266,-0.453340737020902,2.02055656551793,1.47116567306761,-0.0115038122635589,0.131722917926454,-2.28429446135232,0.669356739346067,-1.14593193034066,-1.91291968267857,-1.67212741707599,0.54073309384294,-0.256724509418031,-1.89134380794797,0.161915204911539,-0.458809181262073,-0.194078077908355,0.296793193074685,-0.0250807732722297,-1.05794861180844,0.24484382118802,-0.648883034347149,0.618876699768757,1.44663045545316,1.76518871577172,-1.92334252201028,0.202212741132606,-0.342486128124399,-0.119772048017177,0.266562321266478,0.722029306985646,0.403631540478367,-1.9875531289521,-0.781171921052648,0.987488573315664,0.308690446320804,0.106148749253679,0.185085607568157,-0.390160876288962,1.25429513802065,-1.35013386465017,-0.342974766571647,-0.232894537629491,0.980332519980097,-2.11322741735,0.23024995771627,-1.45705269393795,1.38625292848739,1.6457152752694,0.886358263746316,0.296335125360452,-0.442944664529715,0.274627958703436,1.37404495680148,-1.61754075111507,-1.01356309656133,0.34151499813101,0.60561927302074,0.466358958410998,1.18788000370773,-0.998222009747409,0.472411048704897,0.802363134564957,0.778863518671328,1.33312992214265,-0.11483784787192,-1.24014458172761,1.68097845650274,-0.156295579276488,-0.36167688476951,1.12990338986994,0.544741774675208,-0.739519141431302,0.348344512782951,-0.10457968657704,-1.54862790298088,0.991859873430099,-0.712951101315977,-0.688713554688238,-0.019645395217261,-0.4276668324237,0.485554217337533,-1.22769785113341,-0.0839624525226322,0.929646496319533,-1.04399669558851,1.5862211735349,-0.762919277781499,-0.0482214954780706,0.482863741201961,0.272647805689435,-0.529870425645623,-2.32824908375099,0.0361073596021704,-0.451810115323141,-0.282254053923175,1.54911580277808,0.253374436546217,2.1590870111341,1.00793354498743,1.2238963859792,-0.262656713944417,0.371188568837448,1.84975076615656,-0.32057748245843,-0.484757190293574,0.119115296354525,0.311425735299618,2.86340591290978,0.561681683625529,-1.36773128004884,-0.521412125803238,1.8438836271771,0.15291957051073,-1.32938108961612,1.12345625118363,-0.0988379062841592,-0.781398448382246,1.13052436749178,0.767681931298354,0.525356584083132,0.582232899579348,0.333066984200484,-0.806258638926797,0.556552894435339,-1.30665318755998,0.785186326876701,-0.394106214647196,0.550730858543516,0.774230360594257,1.25782444024924,-1.59645579014647,0.726215347670778,-0.63694860577194,1.27493610125473,-0.0352439186929309,2.89168261447142,1.37501515427315,-0.0491456451207457,-0.0546139127595145,0.260510410587632,0.981880273079814,0.14277683851885,-0.254490023068397,-1.08396906594294,0.271948629915079,0.377547175371029,0.981224651122629,1.80754059091588,-1.1073823023655,0.482139414365283,1.8188538137154,-0.58749687491936,1.1093609884837,-0.297608176884006,2.57784620418014,2.52204599693865,-0.384840198277502,-0.855562334499144,0.938512269689899,-0.468196363866475,0.260008292451711,-0.462982716485726,-0.886275855262118,1.12411879590505,-0.909935517062699,-0.751334836930073,0.514366756631972,0.195860128347231,1.55471787782953,-0.253467892483979,-0.944270533093175,0.0655037556538774,0.669046506691992,-0.329793629158233,1.30135750702321,1.40114405750616,0.796269800703752,-0.10853585230126,1.29063556898416,-0.693149828347284,1.15340154198528,-1.67650319533252,-1.49429593747768,-1.273469908539,1.31161289723836,-1.31181443286064,-0.950226359402931,0.144663430311268,-1.37563897316271,-0.229895425421889,0.419692024617746,0.516239563506011,0.740669495728343,-0.361952001881317,-1.60995844987436,0.327757025857471,1.6432950871785,1.20535256388868,-0.353510280844418,1.21473389654699,-1.53240226188522,1.92233286176372,-1.09253954985412,-1.59184155325035,-0.64646780319668,-0.426883748541837,0.209569862271875,0.0415756254512372,1.33238165750511,-0.681027400604152,0.620571329940411,-0.900045782913094,0.309651318876625,0.118175407084332,0.227812270228395,-1.18930438569969,0.0837636991730383,0.588009163095429,-1.5092271780105,0.995898635439402,-0.00639701815793872,-0.3502219106773,1.4288096764155,-0.372525850460849,-1.86657115947184,1.73533502830238,1.4419101995773,-0.0083570684921407,1.3092622868551,0.000392255402139403,-1.45683485419048,-0.844996213277658,-0.60477119564568,-0.0517937465881769,0.821273104631407,1.50318458611856,0.26033919453097,0.521628843921831,-0.525259906808531,-1.07777898882352,-0.340595705086819,-0.0688224810312909,0.024696280077251,1.76093398424226,0.175448866859079,0.138406693673293,-0.206063763800319,-0.435211393481228,0.529893499013784,0.967447102676697,-2.43106301578848,-0.0424933305199116,-1.99868317055989,-2.47557651859656,-0.965041073936391,-0.435050040400478,-0.507170912986636,-1.37604070291559,0.641342899780833,0.15067215935219,0.647532140383024,-1.21460078326391,1.46931171546606,-1.78034791140857,0.39149639637274,0.953942227218803,1.27980367071278,0.263325295304111,-1.03877191179501,-0.962563912956559,0.895371173457089,0.905913690706474],[0.839913548534372,-0.00258787236806675,1.17326445801642,0.113034397129981,0.0103289388073652,0.741709200645733,0.757132927493812,-1.19798986278408,0.977424502607153,0.958759958483041,0.0208846818970524,-1.64816585771575,-0.0500004390892716,0.733816852493148,1.58085213001182,1.37525645696322,-0.386738162865986,-0.360215028043457,0.757266805188402,-2.33097535807286,-0.280230029886401,-0.958830048080385,-0.0248477772413543,1.0914384415241,-0.458191098753879,1.88580159070826,1.83749124865518,-0.102590480057749,0.15838553980114,-1.6950603734202,0.0630930597664622,1.34186915986901,0.173401451016958,-0.972151247997214,-0.312406983890201,2.16445910566318,-1.08569648917488,-0.570032496333639,0.180933484091584,-1.26230626089635,-1.36951161404504,-1.15492919879104,1.40783665355648,0.7810104680737,0.0780261508667397,2.11272771306656,-0.382805307358927,-0.332246486328796,0.833863022450505,0.595855523401627,-1.10900600352571,0.768901997353853,-1.35001243520008,0.483625347936857,-0.286809284722427,0.141831584335414,-0.374917025431003,-0.231430446863784,-0.0099185367994099,-2.1181242957276,-0.523764095821632,-1.3245780327194,0.431671010964192,0.925051243483283,0.937776607544051,0.546485720184622,1.11596881464033,0.57093798374244,-1.05141552814765,-0.318821841353645,-0.28945625502242,-0.418402639383612,0.781124097674946,-0.114906296442977,-0.845404384974166,-0.91846547110094,1.20790470730197,0.741312997321194,0.63440430668046,2.20584236869693,0.356162081892058,0.267242278476905,0.484838848543199,-0.165738090961005,0.189929774045547,-0.936516525117411,-1.45410274136073,0.233684464458852,0.502156743078766,0.289208616904464,-1.2255081359425,-0.385848379847329,1.42231896323347,-1.71167735796425,0.155048857100102,1.75487385402472,-0.48069689481564,-0.744334523387456,-2.13244313948346,-0.364321781293803,-0.23396733064176,0.111675905130727,-0.8891389568414,-0.722070922038577,-2.32992419947618,-0.479068847619039,0.789926395227164,1.06381036870781,-0.079792902947099,-0.374168305978157,0.122702902704487,-1.43690383100742,-0.853930119491403,0.456289726006739,-0.014252536528041,0.816753534095925,0.0849888556966764,0.446430305360777,2.4233863615748,-0.246984838156123,0.276169461709857,0.93012237255723,-1.02544982457464,0.104880687397645,-0.0142312370976592,2.03937264357414,0.373424382220032,1.43685159890998,0.948761150596412,-1.31828474448681,0.292904130835159,0.147310228756428,-1.03372595847938,0.419603741979271,-0.261093121763321,1.60715082985282,-0.647905446165711,0.65715537742412,-0.570988478404464,-0.818596896313611,1.31069416239349,1.8137587639049,-0.943422330956636,0.857272464644364,-0.26328582577939,1.31296898137972,-0.955589001636968,-1.56443656629831,-2.17226540360992,1.45592051729526,1.03813591475323,-0.19777143839485,-0.650295567070131,-1.33006295483252,0.921944983058324,0.752202862241883,-0.0361287489137461,2.63443160026447,0.287643664899583,0.655181237804631,0.400658255256958,-0.307315592427108,-0.0880020162672765,-1.4182084257932,-0.313520965017903,1.02978138407386,1.63558029915587,0.471404546107081,0.996484129767143,0.682511347452998,-0.971725498883303,-0.654539503518411,1.9950025145245,-1.64398286961826,-0.663561255120923,-1.68572154256486,-0.00487921401549586,0.458799670689864,0.58241494427156,-1.36287408730962,-1.27186878798873,0.714903784373751,-1.01657119771777,-0.0235987531967906,-0.0710377937191493,0.142251252286188,-0.675697628952931,-0.256832712055242,-0.101223100339307,-2.15396211870315,-0.964814528790061,-0.58900330573493,-1.77785861873179,1.14147638281107,-0.327726590960585,-1.35670310282644,0.415689635465171,-0.92593148261845,-1.61417560726588,-0.0867774320988037,0.60184194841017,1.12863900411954,0.184965914039767,0.156829593061561,-0.484045842753597,1.12522180597544,0.243466571444848,-0.634979653702817,-1.25093404891324,-1.43564618623969,1.13585336844654,0.215408789952046,1.86236861457406,1.11499354896313,-0.674613846505975,-0.43413606689344,1.03700516696666,-0.0366774828223079,-0.402283642930047,2.10244761126578,0.459017388966365,-0.255037508112317,-0.672615662270822,1.09980277842076,-0.410916567337035,-0.896789043866022,-1.03108966087791,-0.262753459440137,-0.333082354155729,0.977294566989074,0.67871471120366,1.25873453260806,0.424005522012195,1.79340489830838,0.294283590390496,-1.51557225888824,-0.566557400404045,0.0695595320406091,1.20367387036236,-0.549836130921485,0.120068214856194,0.307532758024057,1.21558171078771,0.126038358749077,-1.47214585638467,-0.31868107188927,0.279543831146715,2.70523537700079,1.68588710316762,1.12400715424783,0.424093764182199,0.104428803649583,-1.18019086680147,-0.916700680556785,-1.5622237070383,-0.851848148224985,-1.11863407938251,0.148310456516182,0.502641050616876,1.58775842779836,-1.10166347373573,-0.777452787893211,0.94287128302882,-1.93395839596369,0.00697944165811194,0.673683215844939,-1.397330083779,0.283500041247497,0.874756033074049,-0.417645089759457,-1.36875084039548,-0.0747718249425306,0.010604925134028,-0.0456341952232458,-1.47340356794426,1.33700892029224,1.94210054046682,0.28445574158121,-1.98483760421236,-0.500795846017144,-0.274682319105469,0.710942799998592,2.25852199311338,0.6182250268147,0.230930399839226,1.10121678493973,-0.572604528413734,-0.533027381020963,-0.7176965209801,-0.877043217358591,-0.748486759792524,0.184772917070714,-0.222319428574694,-1.00929671904305,-1.37046752324561,-1.07025398865643,-0.0066287230519268,0.00463523595082878,0.20066946468697,0.80938780459139,0.28727440717283,-0.303984227284761,0.139665046438343,0.127719663742988,-0.955652234417965,-0.799911666563384,-0.202937397191957,-2.41551123775872,0.375043826421474,2.09383508757916,0.605433040470289,-0.816265554773185,1.4935701721762,2.15379401998194,1.28063402186992,-1.16859068601224,-0.495931213746446,-1.12572393407434,-0.0288990953422831,-0.491218887632818,0.582466898056812,-1.73320797759799,-0.250618799517161,1.37773567984053,0.505184915032841,1.03329019729839,-0.148130245792458,-1.0202394907844,-1.16747825427881,-0.698265878861709,-0.290890251105546,-0.0873443690017022,2.283669647196,0.799259902997867,0.496864026670469,-1.16772217760379,0.0560069889758914,0.0697647068174782,-0.631742146486256,-1.6158020722208,0.822533939087266,-0.585100333152512,-0.316977825658514,-0.44573816650228,-2.50838168081985,-1.39899971856613,-1.31102665972602,0.750397787402512,0.703393597871808,-0.250840219213715,-0.250940581783009,-0.872349651946677,0.628185681743825,-0.93620340324555,-0.550382609936957,-0.269708287909185,-0.688617060298429,-0.390596633689683,1.06991737625437,0.498842879413734,0.333423687411625,0.910547593909951,-1.27890538706665,-1.84181643416829,-0.512044551417815,-0.364112159415469,0.459284544496029,-0.0437528247340715,-0.453330370418473,2.02056940972738,1.4711655329911,-0.0115037727675645,0.131720235939332,-2.28429666643908,0.669357358143575,-1.14593428072262,-1.91292028921362,-1.67212737116874,0.540730537935296,-0.256728217228679,-1.89135458860786,0.161923713751897,-0.45880894451281,-0.194093116240605,0.296799448278433,-0.0250858760457381,-1.05794672496161,0.244840777015996,-0.648903675172624,0.618879032534255,1.44662866767922,1.76518794653031,-1.92336446086539,0.202209903822017,-0.342476330358895,-0.119777298093547,0.266562781211669,0.722029223647274,0.403626352160373,-1.98756254440955,-0.781176360608748,0.987489122011803,0.308687680324707,0.106141910776208,0.185084946493524,-0.390167832722612,1.25429249182984,-1.35013264792223,-0.342979977151236,-0.232904733402817,0.980326705281565,-2.11323168344815,0.230224767304403,-1.45705616963295,1.38624905755782,1.64569692457802,0.886346595917194,0.29633675689802,-0.442929160021382,0.274596538405736,1.37404124355872,-1.61753873051155,-1.01356474565803,0.341505228104669,0.605625240113757,0.466356758396082,1.18788375907875,-0.998226760233022,0.472396053400622,0.802360980598684,0.778870509748369,1.33312667840577,-0.1148293508677,-1.24013481756725,1.68097525412257,-0.15629027746407,-0.361678172554645,1.12990487232368,0.5447550320518,-0.739508991099732,0.348348652377099,-0.104580869913634,-1.54862817948081,0.991866758910092,-0.712953474082082,-0.688709193734229,-0.0196449015764161,-0.427665273063529,0.485561441432709,-1.22769491474408,-0.0839716252515507,0.929638197277388,-1.04402085300715,1.58621339759561,-0.762914884054279,-0.0482290784722356,0.482862627045394,0.272641596149312,-0.529875532795481,-2.32826675978236,0.0360806462082821,-0.451807231002918,-0.282231654136822,1.54909094666731,0.253376396487761,2.15907357038278,1.00792995060678,1.22389204939903,-0.262677168026511,0.371196789466518,1.84975007096263,-0.320579183683584,-0.484759934862902,0.119114428171342,0.311427180205166,2.86340400064886,0.561681519334642,-1.36773194830537,-0.521412008639831,1.8438815057239,0.15293001502939,-1.32938425266363,1.12348191779438,-0.098828182960092,-0.781407776919229,1.1305237765622,0.767679292536553,0.525366170719663,0.582247007045565,0.333078237828316,-0.806253913668936,0.556553286578809,-1.30663823842818,0.785192805832448,-0.394109488179335,0.550736017122259,0.774229123013895,1.25782733087726,-1.59644565487869,0.726224487026773,-0.63694303249434,1.27493628996701,-0.0352351722544312,2.89168569913249,1.37501391394606,-0.0491438610427199,-0.0546140675464213,0.260508947370917,0.98188387764294,0.142782001641591,-0.254494432136471,-1.08397043676242,0.271948096682132,0.377545011206586,0.981224642452517,1.8075397091144,-1.10738323711904,0.482136639244204,1.81885388374107,-0.587497748439159,1.1093648371702,-0.297611822548078,2.57785388240663,2.52205378711077,-0.384846074278908,-0.855558925112427,0.938509156539383,-0.468194320619742,0.26002214490959,-0.462975371184957,-0.886267511491597,1.1241185586142,-0.909922560212657,-0.751326205215179,0.514361263713481,0.195867899225612,1.55471855901027,-0.253468613047718,-0.94426314788694,0.0655127115739869,0.669044246503443,-0.329791729755058,1.30135958344485,1.40114612154444,0.796276147217682,-0.108529644667846,1.29064147481611,-0.693154016271774,1.15339541925385,-1.67649515209392,-1.49428576345607,-1.27346874329394,1.31160765947354,-1.3118047464947,-0.950227745203615,0.144669151196013,-1.37564027921586,-0.229892773641097,0.419701736431681,0.516244042092231,0.740671343410626,-0.361956297908442,-1.60995829124154,0.327759850617908,1.6432946612248,1.20535273136937,-0.353511712988429,1.2147350241449,-1.53239547879113,1.92233372338778,-1.09254143389662,-1.59184176797828,-0.646473071142031,-0.426887134080209,0.209570614772411,0.0415732187865096,1.33238173337405,-0.68102776779717,0.620569757614872,-0.90005113939256,0.30964872769332,0.118178476848962,0.227812209134008,-1.18930550256939,0.083765917186831,0.58801068834544,-1.50922510046914,0.9958983847119,-0.0064022353075127,-0.350218769042406,1.42880693783275,-0.372527085751583,-1.8665933856787,1.73532706858468,1.44191930899082,-0.00835746928668588,1.30926540567801,0.000390243506976562,-1.4568444082666,-0.845006655460633,-0.604761649222912,-0.0517938159759753,0.821263567701469,1.50319776681062,0.26033671123344,0.521630241549456,-0.525261686924827,-1.07777290186381,-0.340576770530692,-0.068827420040524,0.0247002446927788,1.76093343667504,0.175464687010984,0.138406627774082,-0.206069204766683,-0.435208818651666,0.529894842249468,0.967447278373187,-2.43106242232854,-0.0424841372214544,-1.99866320136744,-2.475593352234,-0.965022431354565,-0.435006295304968,-0.507186604629456,-1.37601239819409,0.641349121044549,0.150683139854067,0.647583793686761,-1.21456182324422,1.46932275202185,-1.78034819706805,0.391487351881519,0.953955916805737,1.27979659112715,0.263329463191877,-1.03877570143666,-0.962560646370255,0.895389477399899,0.905910672428035],[500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>y<\/th>\n      <th>y_pred<\/th>\n      <th>iter<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":25,"columnDefs":[{"className":"dt-right","targets":[1,2,3]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<pre><code>#&gt; 25.665 sec elapsed</code></pre>
<p><img src="rtorch-minimal-book_files/figure-html/rotch-complete-2.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="exercise-2" class="section level2" number="12.6">
<h2><span class="header-section-number">12.6</span> Exercise</h2>
<ol style="list-style-type: decimal">
<li><p>Rewrite the code in <code>rTorch</code> but including and plotting the loss at each iteration</p></li>
<li><p>On the neural network written in <code>PyTorch</code>, code, instead of printing a long table, print the table by pages that we could navigate using vertical and horizontal bars. Tip: read the PyThon data structure from R and plot it with <code>ggplot2</code></p></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="rainfall-prediction-with-linear-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="a-step-by-step-neural-network-in-rtorch.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/f0nzie/rtorch-minimal-book/edit/main/0501-neural_networks.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
