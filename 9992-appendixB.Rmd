\cleardoublepage

# Activation Functions {#appendixB}

_Last update: `r system("git log -1 --format=\"%ad (%h)\" -- 9992-appendixB.Rmd", intern = TRUE)`_

```{r}
library(rTorch)
library(ggplot2)
```

## Sigmoid

Using the PyTorch `sigmoid()` function:

```{r sigmoid, fig.asp=1}
x <- torch$range(-5., 5., 0.1)
y <- torch$sigmoid(x)

df <- data.frame(x = x$numpy(), sx = y$numpy())
df

ggplot(df, aes(x = x, y = sx)) + 
    geom_point() +
    ggtitle("Sigmoid")
```


Plot the sigmoid function using an R custom-made function:

```{r, fig.asp=1}
sigmoid = function(x) {
   1 / (1 + exp(-x))
}

x <- seq(-5, 5, 0.01)
plot(x, sigmoid(x), col = 'blue', cex = 0.5, main = "Sigmoid")
```

## ReLU

Using the PyTorch `relu()` function:

```{r relu, fig.asp=1, results='hide'}
x <- torch$range(-5., 5., 0.1)
y <- torch$relu(x)

df <- data.frame(x = x$numpy(), sx = y$numpy())
df

ggplot(df, aes(x = x, y = sx)) + 
    geom_point() +
    ggtitle("ReLU")
```


## tanh

Using the PyTorch `tanh()` function:

```{r tanh, fig.asp=1, results='hide'}
x <- torch$range(-5., 5., 0.1)
y <- torch$tanh(x)

df <- data.frame(x = x$numpy(), sx = y$numpy())
df

ggplot(df, aes(x = x, y = sx)) + 
    geom_point() +
    ggtitle("tanh")
```


## Softmax

Using the PyTorch `softmax()` function:

```{r softmax, fig.asp=1, results='hide'}
x <- torch$range(-5.0, 5.0, 0.1)
y <- torch$softmax(x, dim=0L)

df <- data.frame(x = x$numpy(), sx = y$numpy())

ggplot(df, aes(x = x, y = sx)) + 
    geom_point() +
    ggtitle("Softmax")

```





## Activation functions in Python

```{r}
library(rTorch)
```

```{python}
import numpy as np
import matplotlib.pyplot as plt
np.random.seed(42)
```

### Linear activation {-}

```{python}
def Linear(x, derivative=False):
    """
    Computes the Linear activation function for array x
    inputs:
    x: array
    derivative: if True, return the derivative else the forward pass
    """
    
    if derivative:              # Return derivative of the function at x
        return np.ones_like(x)
    else:                       # Return forward pass of the function at x
        return x
```

### Sigmoid activation {-}

```{python}
def Sigmoid(x, derivative=False):
    """
    Computes the Sigmoid activation function for array x
    inputs:
    x: array 
    derivative: if True, return the derivative else the forward pass
    """
    f = 1/(1+np.exp(-x))
    
    if derivative:              # Return derivative of the function at x
        return f*(1-f)
    else:                       # Return forward pass of the function at x
        return f
```


### Hyperbolic Tangent activation {-}

```{python}
def Tanh(x, derivative=False):
    """
    Computes the Hyperbolic Tangent activation function for array x
    inputs:
    x: array 
    derivative: if True, return the derivative else the forward pass
    """
    f = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))
    
    if derivative:              # Return  derivative of the function at x
        return 1-f**2
    else:                       # Return the forward pass of the function at x
        return f
```


### Rectifier linear unit (ReLU) {-}

```{python}
def ReLU(x, derivative=False):
    """
    Computes the Rectifier Linear Unit activation function for array x
    inputs:
    x: array
    derivative: if True, return the derivative else the forward pass
    """
    
    if derivative:              # Return derivative of the function at x
        return (x>0).astype(int)
    else:                       # Return forward pass of the function at x
        return np.maximum(x, 0)
```


### Visualization with `matplotlib` {-}
Plotting using `matplotlib`:

```{python, results='hide'}
x = np.linspace(-6, 6, 100)
units = {
    "Linear": lambda x: Linear(x),
    "Sigmoid": lambda x: Sigmoid(x),
    "ReLU": lambda x: ReLU(x),
    "tanh": lambda x: Tanh(x)
}

plt.figure(figsize=(5, 5))
[plt.plot(x, unit(x), label=unit_name, lw=2) 
    for unit_name, unit in units.items()]
plt.legend(loc=2, fontsize=16)
plt.title('Activation functions', fontsize=20)
plt.ylim([-2, 5])
plt.xlim([-6, 6])
plt.show()

```


## Softmax code in Python

```{python, results='hide'}
# Source: https://dataaspirant.com/2017/03/07/difference-between-softmax-function-and-sigmoid-function/
import numpy as np
import matplotlib.pyplot as plt
 
 
def softmax(inputs):
    """
    Calculate the softmax for the give inputs (array)
    :param inputs:
    :return:
    """
    return np.exp(inputs) / float(sum(np.exp(inputs)))
 
 
def line_graph(x, y, x_title, y_title):
    """
    Draw line graph with x and y values
    :param x:
    :param y:
    :param x_title:
    :param y_title:
    :return:
    """
    plt.plot(x, y)
    plt.xlabel(x_title)
    plt.ylabel(y_title)
    plt.show()
 
 
graph_x = np.linspace(-6, 6, 100)
graph_y = softmax(graph_x)
 
print("Graph X readings: {}".format(graph_x))
print("Graph Y readings: {}".format(graph_y))
 
line_graph(graph_x, graph_y, "Inputs", "Softmax Scores")
```

