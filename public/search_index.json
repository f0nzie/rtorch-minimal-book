[
["index.html", "A Minimal rTorch Book Prerequisites Installation Python Anaconda", " A Minimal rTorch Book Alfonso R. Reyes 2020-10-19 Prerequisites You need couple of things to get rTorch working: Install Python Anaconda. Preferrably, for 64-bits, and above Python 3.6+. I have successfully tested Anaconda under four different operating systems: Windows (Win10 and Windows Server 2008); macOS (Sierra, Mojave and Catalina); Linux (Debian, Fedora and Ubuntu); and lastly, Solaris 10. ASll these tests are required by CRAN. Install R, Rtools and RStudio. Install the R package reticulate, which is the one that provides the connection between R and Python. Install the stable version rTorch from CRAN, or the latest version under develoipment via GitHub. Note. While it is not mandatory to have a previously created Python environment with Anaconda, where PyTorch and TorchVision have already been installed, it is another option if for some reason reticulate refuses to communicate with the conda environment. Keep in mind that you could also get the rTorch conda environment installed directly from the R console, in very similar fashion as in R-TensorFlow using the function install_pytorch(). Installation The rTorch package can be installed from CRAN or Github. From CRAN: install.packages(&quot;rTorch&quot;) From GitHub, install rTorch with: devtools::install_github(&quot;f0nzie/rTorch&quot;) which will install rTorch from the main or master branch. To install it from the develop branch, you type this: devtools::install_github(&quot;f0nzie/rTorch&quot;, ref=&quot;develop&quot;) or clone with Git with: git clone https://github.com/f0nzie/rTorch.git This will allow you to build rTorch from source. Python Anaconda If your preference is installing an Anaconda environment first, these are the steps: Example Create a conda environment from the terminal with conda create -n r-torch python=3.7 Activate the new environment with conda activate r-torch Install the PyTorch related packages with: conda install python=3.6.6 pytorch torchvision cpuonly matplotlib pandas -c pytorch The last part -c pytorch specifies the stable conda channel to download the PyTorch packages. Your conda installation may not work if you don’t indicate the channel. Now, you can load rTorch in R or RStudio with: library(rTorch) Automatic installation I used the idea for automatic installation in the tensorflow package for R, to create the function rTorch::install_pytorch(). This function will allow you to install a conda environment complete with all PyTorch requirements plus the packages you specify. Example: rTorch:::install_conda(package=&quot;pytorch=1.4&quot;, envname=&quot;r-torch&quot;, conda=&quot;auto&quot;, conda_python_version = &quot;3.6&quot;, pip=FALSE, channel=&quot;pytorch&quot;, extra_packages=c(&quot;torchvision&quot;, &quot;cpuonly&quot;, &quot;matplotlib&quot;, &quot;pandas&quot;)) This is explained in more detailed in the rTorch package manual. Note. matplotlib and pandas are not really necessary for rTorch to work, but I was asked if matplotlib or pandas could work with PyTorch. So, I decided to install them for testing and experimentation. They both work. "],
["intro.html", "Chapter 1 Introduction 1.1 Motivation 1.2 How do we start using rTorch 1.3 What can you do with rTorch 1.4 Getting help", " Chapter 1 Introduction 1.1 Motivation Why do we want a package of something that is already working well, such as PyTorch? There are several reasons, but the main one is to bring another machine learning framework to R. Probably it is just me but I feel PyTorch very comfortable to work with. Feels pretty much like everything else in Python. Very Pythonic. I have tried other frameworks in R. The closest that matches a natural language like PyTorch, is MXnet. Unfortunately, MXnet it is the hardest to install and maintain after updates. Yes. I could have worked directly with PyTorch in a native Python environment, such as Jupyter or PyCharm or vscode notebooks but it very hard to quit RMarkdown once you get used to it. It is the real thing in regards to literate programmingand reproducibility. It does not only contribute to improving the quality of the code but establishes a workflow for a better understanding of a subject by your intended readers (Knuth 1983), in what is been called the literate programming paradigm (Cordes and Brown 1991). This has the additional benefit of giving the ability to write combination of Python and R code together in the same document. There will times when it is better to create a class in Python; and other times where R will be more convenient to handle a data structure. I show some examples using data.frame and data.table along with PyTorch tensors. 1.2 How do we start using rTorch Start using rTorch is very simple. After installing the minimum system requirements -such as conda- you just call it with: library(rTorch) There are several way of testing if rTorch is up and running. Let’s see some of them: 1.2.1 Getting the PyTorch version rTorch::torch_version() #&gt; [1] &quot;1.6&quot; 1.2.2 PyTorch configuration This will show the PyTorch version and the current version of Python installed, as well as the paths to folders where they reside. rTorch::torch_config() #&gt; PyTorch v1.6.0 (~/anaconda3/envs/r-torch/lib/python3.7/site-packages/torch) #&gt; Python v3.7 (~/anaconda3/envs/r-torch/bin/python) #&gt; NumPy v1.19.1) 1.3 What can you do with rTorch Practically, you can do everything you could with PyTorch within the R ecosystem. Additionally to the rTorch module, from where you can extract methods, functions and classes, there are available two more modules: torchvision and np, which is short for numpy. We could use the modules with: rTorch::torchvision rTorch::np rTorch::torch #&gt; Module(torchvision) #&gt; Module(numpy) #&gt; Module(torch) 1.4 Getting help We get a glimpse of the first lines of the help(\"torch\") via a Python markdown chunk: help(&quot;torch&quot;) ... #&gt; NAME #&gt; torch #&gt; #&gt; DESCRIPTION #&gt; The torch package contains data structures for multi-dimensional #&gt; tensors and mathematical operations over these are defined. #&gt; Additionally, it provides many utilities for efficient serializing of #&gt; Tensors and arbitrary types, and other useful utilities. ... help(&quot;torch.tensor&quot;) ... #&gt; Help on built-in function tensor in torch: #&gt; #&gt; torch.tensor = tensor(...) #&gt; tensor(data, dtype=None, device=None, requires_grad=False, pin_memory=False) -&gt; Tensor #&gt; #&gt; Constructs a tensor with :attr:`data`. #&gt; #&gt; .. warning:: #&gt; #&gt; :func:`torch.tensor` always copies :attr:`data`. If you have a Tensor #&gt; ``data`` and want to avoid a copy, use :func:`torch.Tensor.requires_grad_` #&gt; or :func:`torch.Tensor.detach`. #&gt; If you have a NumPy ``ndarray`` and want to avoid a copy, use #&gt; :func:`torch.as_tensor`. #&gt; #&gt; .. warning:: #&gt; #&gt; When data is a tensor `x`, :func:`torch.tensor` reads out &#39;the data&#39; from whatever it is passed, #&gt; and constructs a leaf variable. Therefore ``torch.tensor(x)`` is equivalent to ``x.clone().detach()`` #&gt; and ``torch.tensor(x, requires_grad=True)`` is equivalent to ``x.clone().detach().requires_grad_(True)``. ... help(&quot;torch.cat&quot;) ... #&gt; Help on built-in function cat in torch: #&gt; #&gt; torch.cat = cat(...) #&gt; cat(tensors, dim=0, out=None) -&gt; Tensor #&gt; #&gt; Concatenates the given sequence of :attr:`seq` tensors in the given dimension. #&gt; All tensors must either have the same shape (except in the concatenating #&gt; dimension) or be empty. #&gt; #&gt; :func:`torch.cat` can be seen as an inverse operation for :func:`torch.split` #&gt; and :func:`torch.chunk`. #&gt; #&gt; :func:`torch.cat` can be best understood via examples. #&gt; #&gt; Args: #&gt; tensors (sequence of Tensors): any python sequence of tensors of the same type. #&gt; Non-empty tensors provided must have the same shape, except in the #&gt; cat dimension. #&gt; dim (int, optional): the dimension over which the tensors are concatenated #&gt; out (Tensor, optional): the output tensor. ... help(&quot;numpy.arange&quot;) ... #&gt; Help on built-in function arange in numpy: #&gt; #&gt; numpy.arange = arange(...) #&gt; arange([start,] stop[, step,], dtype=None) #&gt; #&gt; Return evenly spaced values within a given interval. #&gt; #&gt; Values are generated within the half-open interval ``[start, stop)`` #&gt; (in other words, the interval including `start` but excluding `stop`). #&gt; For integer arguments the function is equivalent to the Python built-in #&gt; `range` function, but returns an ndarray rather than a list. #&gt; #&gt; When using a non-integer step, such as 0.1, the results will often not #&gt; be consistent. It is better to use `numpy.linspace` for these cases. #&gt; #&gt; Parameters #&gt; ---------- #&gt; start : number, optional #&gt; Start of interval. The interval includes this value. The default #&gt; start value is 0. #&gt; stop : number #&gt; End of interval. The interval does not include this value, except #&gt; in some cases where `step` is not an integer and floating point #&gt; round-off affects the length of `out`. #&gt; step : number, optional ... Finally, these are the classes for the module torchvision.datasets. We are using Python to list them using the help function. help(&quot;torchvision.datasets&quot;) ... #&gt; Help on package torchvision.datasets in torchvision: #&gt; #&gt; NAME #&gt; torchvision.datasets #&gt; #&gt; PACKAGE CONTENTS #&gt; caltech #&gt; celeba #&gt; cifar #&gt; cityscapes #&gt; coco #&gt; fakedata #&gt; flickr #&gt; folder #&gt; hmdb51 #&gt; imagenet #&gt; kinetics #&gt; lsun #&gt; mnist #&gt; omniglot #&gt; phototour #&gt; samplers (package) #&gt; sbd #&gt; sbu #&gt; semeion #&gt; stl10 #&gt; svhn #&gt; ucf101 #&gt; usps #&gt; utils #&gt; video_utils #&gt; vision #&gt; voc #&gt; #&gt; CLASSES ... In other words, all the functions, modules, classes in PyTorch are available to rTorch. "],
["pytorch-and-numpy.html", "Chapter 2 PyTorch and NumPy 2.1 Callable PyTorch modules from rTorch 2.2 Common array and tensor operations in NumPy and PyTorch 2.3 Python built-in functions", " Chapter 2 PyTorch and NumPy 2.1 Callable PyTorch modules from rTorch 2.1.1 The torchvision module This is an example of using the torchvision module. With torchvision we could download any of the datasets made available by PyTorch. In this example, we will be downloading the training dataset of the MNIST handwritten digits. There are 60,000 images in the training set and 10,000 images in the test set. The images will download on the folder ./datasets. library(rTorch) transforms &lt;- torchvision$transforms # this is the folder where the datasets will be downloaded local_folder &lt;- &#39;./datasets/mnist_digits&#39; train_dataset = torchvision$datasets$MNIST(root = local_folder, train = TRUE, transform = transforms$ToTensor(), download = TRUE) train_dataset #&gt; Dataset MNIST #&gt; Number of datapoints: 60000 #&gt; Root location: ./datasets/mnist_digits #&gt; Split: Train #&gt; StandardTransform #&gt; Transform: ToTensor() You can do similarly for the test dataset if you set the flag train = FALSE. The test dataset has only 10,000 images. test_dataset = torchvision$datasets$MNIST(root = local_folder, train = FALSE, transform = transforms$ToTensor()) test_dataset #&gt; Dataset MNIST #&gt; Number of datapoints: 10000 #&gt; Root location: ./datasets/mnist_digits #&gt; Split: Test #&gt; StandardTransform #&gt; Transform: ToTensor() 2.1.2 The numpy module numpy is automaticaly installed when PyTorch is. There is some interdependence between both. Anytime that we need to do some transformation that is not available in PyTorch, we will use numpy. Just keep in mind that numpy does not have support for GPUs. There are several operations that we could perform with numpy such creating arrays: Create an array Create an array: # do some array manipulations with NumPy a &lt;- np$array(c(1:4)) a #&gt; [1] 1 2 3 4 Create an array of a desired shape: np$reshape(np$arange(0, 9), c(3L, 3L)) #&gt; [,1] [,2] [,3] #&gt; [1,] 0 1 2 #&gt; [2,] 3 4 5 #&gt; [3,] 6 7 8 Create an array by spelling out its components and type: np$array(list( list(73, 67, 43), list(87, 134, 58), list(102, 43, 37), list(73, 67, 43), list(91, 88, 64), list(102, 43, 37), list(69, 96, 70), list(91, 88, 64), list(102, 43, 37), list(69, 96, 70) ), dtype=&#39;float32&#39;) #&gt; [,1] [,2] [,3] #&gt; [1,] 73 67 43 #&gt; [2,] 87 134 58 #&gt; [3,] 102 43 37 #&gt; [4,] 73 67 43 #&gt; [5,] 91 88 64 #&gt; [6,] 102 43 37 #&gt; [7,] 69 96 70 #&gt; [8,] 91 88 64 #&gt; [9,] 102 43 37 #&gt; [10,] 69 96 70 2.2 Common array and tensor operations in NumPy and PyTorch We will use the train and test datasets that we loaded with torchvision. Reshape an array For the same test dataset that we loaded above from MNIST digits, we will show the image of the handwritten digit and its label or class. Before plotting the image, we need to: Extract the image and label from the dataset Convert the tensor to a numpy array Reshape the tensor as a 2D array Plot the digit and its label rotate &lt;- function(x) t(apply(x, 2, rev)) # function to rotate the matrix # label for the image label &lt;- test_dataset[0][[2]] label # convert tensor to numpy array .show_img &lt;- test_dataset[0][[1]]$numpy() dim(.show_img) # reshape 3D array to 2D show_img &lt;- np$reshape(.show_img, c(28L, 28L)) dim(show_img) #&gt; [1] 7 #&gt; [1] 1 28 28 #&gt; [1] 28 28 # show in gray shades and rotate image(rotate(show_img), col = gray.colors(64)) title(label) Generate a random array in NumPy # set the seed np$random$seed(123L) # generate a random array x = np$random$rand(100L) x # calculate the y array y = np$sin(x) * np$power(x, 3L) + 3L * x + np$random$rand(100L) * 0.8 class(x) class(y) #&gt; [1] 0.6965 0.2861 0.2269 0.5513 0.7195 0.4231 0.9808 0.6848 0.4809 0.3921 #&gt; [11] 0.3432 0.7290 0.4386 0.0597 0.3980 0.7380 0.1825 0.1755 0.5316 0.5318 #&gt; [21] 0.6344 0.8494 0.7245 0.6110 0.7224 0.3230 0.3618 0.2283 0.2937 0.6310 #&gt; [31] 0.0921 0.4337 0.4309 0.4937 0.4258 0.3123 0.4264 0.8934 0.9442 0.5018 #&gt; [41] 0.6240 0.1156 0.3173 0.4148 0.8663 0.2505 0.4830 0.9856 0.5195 0.6129 #&gt; [51] 0.1206 0.8263 0.6031 0.5451 0.3428 0.3041 0.4170 0.6813 0.8755 0.5104 #&gt; [61] 0.6693 0.5859 0.6249 0.6747 0.8423 0.0832 0.7637 0.2437 0.1942 0.5725 #&gt; [71] 0.0957 0.8853 0.6272 0.7234 0.0161 0.5944 0.5568 0.1590 0.1531 0.6955 #&gt; [81] 0.3188 0.6920 0.5544 0.3890 0.9251 0.8417 0.3574 0.0436 0.3048 0.3982 #&gt; [91] 0.7050 0.9954 0.3559 0.7625 0.5932 0.6917 0.1511 0.3989 0.2409 0.3435 #&gt; [1] &quot;array&quot; #&gt; [1] &quot;array&quot; From the classes, we can tell that the numpy arrays are automatically converted to R arrays. plot(x, y) Generate a random array in PyTorch The same operation can be performed with pure torch tensors: library(rTorch) invisible(torch$manual_seed(123L)) x &lt;- torch$rand(100L) # use torch$randn(100L): positive and negative numbers y &lt;- torch$sin(x) * torch$pow(x, 3L) + 3L * x + torch$rand(100L) * 0.8 class(x) class(y) #&gt; [1] &quot;torch.Tensor&quot; &quot;torch._C._TensorBase&quot; &quot;python.builtin.object&quot; #&gt; [1] &quot;torch.Tensor&quot; &quot;torch._C._TensorBase&quot; &quot;python.builtin.object&quot; Since the clasess are torch tensors, to plot them in R, they need to be converted to numpy, and then R: plot(x$numpy(), y$numpy()) Convert a numpy array to a PyTorch tensor This is a very common operation that I have seen in examples using PyTorch. Creating first the array in numpy. and then convert it to a torch tensor. # input array x = np$array(rbind( c(0,0,1), c(0,1,1), c(1,0,1), c(1,1,1))) # the numpy array x #&gt; [,1] [,2] [,3] #&gt; [1,] 0 0 1 #&gt; [2,] 0 1 1 #&gt; [3,] 1 0 1 #&gt; [4,] 1 1 1 This is another common operation that will find in the PyTorch tutorials: converting a numpy array from a certain type to a tensor of the same type: # convert the numpy array to a float type Xn &lt;- np$float32(x) # convert the numpy array to a float tensor Xt &lt;- torch$FloatTensor(Xn) Xt #&gt; tensor([[0., 0., 1.], #&gt; [0., 1., 1.], #&gt; [1., 0., 1.], #&gt; [1., 1., 1.]]) 2.3 Python built-in functions To access the Python built-in functions we make use of the package reticulate and the function import_builtins(). Here are part of the built-in functions and operators offered by reticulate: py_bi &lt;- import_builtins() grep(&quot;Error|Warning|Exit&quot;, names(py_bi), value = TRUE, invert = TRUE, perl = TRUE) #&gt; [1] &quot;abs&quot; &quot;all&quot; &quot;any&quot; #&gt; [4] &quot;ascii&quot; &quot;BaseException&quot; &quot;bin&quot; #&gt; [7] &quot;bool&quot; &quot;breakpoint&quot; &quot;bytearray&quot; #&gt; [10] &quot;bytes&quot; &quot;callable&quot; &quot;chr&quot; #&gt; [13] &quot;classmethod&quot; &quot;compile&quot; &quot;complex&quot; #&gt; [16] &quot;copyright&quot; &quot;credits&quot; &quot;delattr&quot; #&gt; [19] &quot;dict&quot; &quot;dir&quot; &quot;divmod&quot; #&gt; [22] &quot;Ellipsis&quot; &quot;enumerate&quot; &quot;eval&quot; #&gt; [25] &quot;Exception&quot; &quot;exec&quot; &quot;exit&quot; #&gt; [28] &quot;False&quot; &quot;filter&quot; &quot;float&quot; #&gt; [31] &quot;format&quot; &quot;frozenset&quot; &quot;getattr&quot; #&gt; [34] &quot;globals&quot; &quot;hasattr&quot; &quot;hash&quot; #&gt; [37] &quot;help&quot; &quot;hex&quot; &quot;id&quot; #&gt; [40] &quot;input&quot; &quot;int&quot; &quot;isinstance&quot; #&gt; [43] &quot;issubclass&quot; &quot;iter&quot; &quot;KeyboardInterrupt&quot; #&gt; [46] &quot;len&quot; &quot;license&quot; &quot;list&quot; #&gt; [49] &quot;locals&quot; &quot;map&quot; &quot;max&quot; #&gt; [52] &quot;memoryview&quot; &quot;min&quot; &quot;next&quot; #&gt; [55] &quot;None&quot; &quot;NotImplemented&quot; &quot;object&quot; #&gt; [58] &quot;oct&quot; &quot;open&quot; &quot;ord&quot; #&gt; [61] &quot;pow&quot; &quot;print&quot; &quot;property&quot; #&gt; [64] &quot;quit&quot; &quot;range&quot; &quot;repr&quot; #&gt; [67] &quot;reversed&quot; &quot;round&quot; &quot;set&quot; #&gt; [70] &quot;setattr&quot; &quot;slice&quot; &quot;sorted&quot; #&gt; [73] &quot;staticmethod&quot; &quot;StopAsyncIteration&quot; &quot;StopIteration&quot; #&gt; [76] &quot;str&quot; &quot;sum&quot; &quot;super&quot; #&gt; [79] &quot;True&quot; &quot;tuple&quot; &quot;type&quot; #&gt; [82] &quot;vars&quot; &quot;zip&quot; Length of a dataset Sometimes, we will need the Python len function to find out the length of an object: py_bi$len(train_dataset) py_bi$len(test_dataset) #&gt; [1] 60000 #&gt; [1] 10000 Iterators Iterators are used a lot in dataset operations when running a neural network. In this example we will iterate through only 100 elements of the 60,000 of the train dataset. The goal is printing the “label” or “class” for the digits we are reading. The digits are not show here; they are stored in tensors. # iterate through training dataset enum_train_dataset &lt;- py_bi$enumerate(train_dataset) cat(sprintf(&quot;%8s %8s \\n&quot;, &quot;index&quot;, &quot;label&quot;)) for (i in 1:py_bi$len(train_dataset)) { obj &lt;- reticulate::iter_next(enum_train_dataset) idx &lt;- obj[[1]] # index number cat(sprintf(&quot;%8d %5d \\n&quot;, idx, obj[[2]][[2]])) if (i &gt;= 100) break # print only 100 labels } #&gt; index label #&gt; 0 5 #&gt; 1 0 #&gt; 2 4 #&gt; 3 1 #&gt; 4 9 #&gt; 5 2 #&gt; 6 1 #&gt; 7 3 #&gt; 8 1 #&gt; 9 4 #&gt; 10 3 #&gt; 11 5 #&gt; 12 3 #&gt; 13 6 #&gt; 14 1 #&gt; 15 7 #&gt; 16 2 #&gt; 17 8 #&gt; 18 6 #&gt; 19 9 #&gt; 20 4 #&gt; 21 0 #&gt; 22 9 #&gt; 23 1 #&gt; 24 1 #&gt; 25 2 #&gt; 26 4 #&gt; 27 3 #&gt; 28 2 #&gt; 29 7 #&gt; 30 3 #&gt; 31 8 #&gt; 32 6 #&gt; 33 9 #&gt; 34 0 #&gt; 35 5 #&gt; 36 6 #&gt; 37 0 #&gt; 38 7 #&gt; 39 6 #&gt; 40 1 #&gt; 41 8 #&gt; 42 7 #&gt; 43 9 #&gt; 44 3 #&gt; 45 9 #&gt; 46 8 #&gt; 47 5 #&gt; 48 9 #&gt; 49 3 #&gt; 50 3 #&gt; 51 0 #&gt; 52 7 #&gt; 53 4 #&gt; 54 9 #&gt; 55 8 #&gt; 56 0 #&gt; 57 9 #&gt; 58 4 #&gt; 59 1 #&gt; 60 4 #&gt; 61 4 #&gt; 62 6 #&gt; 63 0 #&gt; 64 4 #&gt; 65 5 #&gt; 66 6 #&gt; 67 1 #&gt; 68 0 #&gt; 69 0 #&gt; 70 1 #&gt; 71 7 #&gt; 72 1 #&gt; 73 6 #&gt; 74 3 #&gt; 75 0 #&gt; 76 2 #&gt; 77 1 #&gt; 78 1 #&gt; 79 7 #&gt; 80 9 #&gt; 81 0 #&gt; 82 2 #&gt; 83 6 #&gt; 84 7 #&gt; 85 8 #&gt; 86 3 #&gt; 87 9 #&gt; 88 0 #&gt; 89 4 #&gt; 90 6 #&gt; 91 7 #&gt; 92 4 #&gt; 93 6 #&gt; 94 8 #&gt; 95 0 #&gt; 96 7 #&gt; 97 8 #&gt; 98 3 #&gt; 99 1 Types and instances Types, instances and classes are important to take decisions on how we will process data that is being read from the datasets. In this example, we want to know if an object is of certain instance: # get the class of the object py_bi$type(train_dataset) # is train_dataset a torchvision dataset class py_bi$isinstance(train_dataset, torchvision$datasets$mnist$MNIST) #&gt; &lt;class &#39;torchvision.datasets.mnist.MNIST&#39;&gt; #&gt; [1] TRUE "],
["rtorch-vs-pytorch-whats-different.html", "Chapter 3 rTorch vs PyTorch: What’s different 3.1 Calling objects from PyTorch 3.2 Call modules and functions from torch 3.3 Show the attributes (methods) of a class or PyTorch object 3.4 How to iterate through datasets 3.5 Zero gradient 3.6 R generics for PyTorch functions", " Chapter 3 rTorch vs PyTorch: What’s different This chapter will explain the main differences between PyTorch and rTorch. Most of the things work directly in PyTorch but we need to be aware of some minor differences when working with rTorch. Here is a review of existing methods. Let’s start by loading rTorch: library(rTorch) 3.1 Calling objects from PyTorch We use the dollar sign or $ to call a class, function or method from the rTorch modules. In this case, from the torch module: torch$tensor #&gt; &lt;built-in method tensor of type&gt; In Python, what we do is using the dot to separate the sub-members of an object: import torch torch.tensor #&gt; &lt;built-in method tensor of type object at 0x7f9d06507000&gt; 3.2 Call modules and functions from torch # these are the equivalents of the Python import module nn &lt;- torch$nn transforms &lt;- torchvision$transforms dsets &lt;- torchvision$datasets Then we can proceed to extract classes, methods and functions from the nn, transforms, and dsets objects. In this example we use the module torchvision$datasets and the function transforms$ToTensor() local_folder &lt;- &#39;./datasets/mnist_digits&#39; train_dataset = torchvision$datasets$MNIST(root = local_folder, train = TRUE, transform = transforms$ToTensor(), download = TRUE) train_dataset #&gt; Dataset MNIST #&gt; Number of datapoints: 60000 #&gt; Root location: ./datasets/mnist_digits #&gt; Split: Train #&gt; StandardTransform #&gt; Transform: ToTensor() 3.3 Show the attributes (methods) of a class or PyTorch object Sometimes we are interested in knowing the internal components of a class. In that case, we use the reticulate function py_list_attributes(). In this example, we want to show the attributes of train_dataset: reticulate::py_list_attributes(train_dataset) #&gt; [1] &quot;__add__&quot; &quot;__class__&quot; &quot;__delattr__&quot; #&gt; [4] &quot;__dict__&quot; &quot;__dir__&quot; &quot;__doc__&quot; #&gt; [7] &quot;__eq__&quot; &quot;__format__&quot; &quot;__ge__&quot; #&gt; [10] &quot;__getattribute__&quot; &quot;__getitem__&quot; &quot;__gt__&quot; #&gt; [13] &quot;__hash__&quot; &quot;__init__&quot; &quot;__init_subclass__&quot; #&gt; [16] &quot;__le__&quot; &quot;__len__&quot; &quot;__lt__&quot; #&gt; [19] &quot;__module__&quot; &quot;__ne__&quot; &quot;__new__&quot; #&gt; [22] &quot;__reduce__&quot; &quot;__reduce_ex__&quot; &quot;__repr__&quot; #&gt; [25] &quot;__setattr__&quot; &quot;__sizeof__&quot; &quot;__str__&quot; #&gt; [28] &quot;__subclasshook__&quot; &quot;__weakref__&quot; &quot;_check_exists&quot; #&gt; [31] &quot;_format_transform_repr&quot; &quot;_repr_indent&quot; &quot;class_to_idx&quot; #&gt; [34] &quot;classes&quot; &quot;data&quot; &quot;download&quot; #&gt; [37] &quot;extra_repr&quot; &quot;processed_folder&quot; &quot;raw_folder&quot; #&gt; [40] &quot;resources&quot; &quot;root&quot; &quot;target_transform&quot; #&gt; [43] &quot;targets&quot; &quot;test_data&quot; &quot;test_file&quot; #&gt; [46] &quot;test_labels&quot; &quot;train&quot; &quot;train_data&quot; #&gt; [49] &quot;train_labels&quot; &quot;training_file&quot; &quot;transform&quot; #&gt; [52] &quot;transforms&quot; Knowing the internal methods of a class could be useful when we want to refer to a specific property of such class. For example, from the list above, we know that the object train_dataset has an attribute __len__. We can call it like this: train_dataset$`__len__`() #&gt; [1] 60000 3.4 How to iterate through datasets 3.4.1 Enumeration Given the following training dataset x_train, we want to find the number of elements of the tensor. We start by entering a numpy array, which then will convert to a tensor with the PyTorch function from_numpy(): x_train = array(c(3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59, 2.167, 7.042, 10.791, 5.313, 7.997, 3.1), dim = c(15,1)) x_train &lt;- r_to_py(x_train) x_train &lt;- torch$from_numpy(x_train) # convert to tensor x_train &lt;- x_train$type(torch$FloatTensor) # make it a a FloatTensor x_train #&gt; tensor([[ 3.3000], #&gt; [ 4.4000], #&gt; [ 5.5000], #&gt; [ 6.7100], #&gt; [ 6.9300], #&gt; [ 4.1680], #&gt; [ 9.7790], #&gt; [ 6.1820], #&gt; [ 7.5900], #&gt; [ 2.1670], #&gt; [ 7.0420], #&gt; [10.7910], #&gt; [ 5.3130], #&gt; [ 7.9970], #&gt; [ 3.1000]]) with this number of elements: x_train$nelement() # number of elements in the tensor #&gt; [1] 15 3.4.2 Using enumerate and iterate py = import_builtins() enum_x_train = py$enumerate(x_train) enum_x_train py$len(x_train) #&gt; &lt;enumerate&gt; #&gt; [1] 15 If we directly use iterate over the enum_x_train object, we get an R list with the index and the value of the 1D tensor: xit = iterate(enum_x_train, simplify = TRUE) xit #&gt; [[1]] #&gt; [[1]][[1]] #&gt; [1] 0 #&gt; #&gt; [[1]][[2]] #&gt; tensor([3.3000]) #&gt; #&gt; #&gt; [[2]] #&gt; [[2]][[1]] #&gt; [1] 1 #&gt; #&gt; [[2]][[2]] #&gt; tensor([4.4000]) #&gt; #&gt; #&gt; [[3]] #&gt; [[3]][[1]] #&gt; [1] 2 #&gt; #&gt; [[3]][[2]] #&gt; tensor([5.5000]) #&gt; #&gt; #&gt; [[4]] #&gt; [[4]][[1]] #&gt; [1] 3 #&gt; #&gt; [[4]][[2]] #&gt; tensor([6.7100]) #&gt; #&gt; #&gt; [[5]] #&gt; [[5]][[1]] #&gt; [1] 4 #&gt; #&gt; [[5]][[2]] #&gt; tensor([6.9300]) #&gt; #&gt; #&gt; [[6]] #&gt; [[6]][[1]] #&gt; [1] 5 #&gt; #&gt; [[6]][[2]] #&gt; tensor([4.1680]) #&gt; #&gt; #&gt; [[7]] #&gt; [[7]][[1]] #&gt; [1] 6 #&gt; #&gt; [[7]][[2]] #&gt; tensor([9.7790]) #&gt; #&gt; #&gt; [[8]] #&gt; [[8]][[1]] #&gt; [1] 7 #&gt; #&gt; [[8]][[2]] #&gt; tensor([6.1820]) #&gt; #&gt; #&gt; [[9]] #&gt; [[9]][[1]] #&gt; [1] 8 #&gt; #&gt; [[9]][[2]] #&gt; tensor([7.5900]) #&gt; #&gt; #&gt; [[10]] #&gt; [[10]][[1]] #&gt; [1] 9 #&gt; #&gt; [[10]][[2]] #&gt; tensor([2.1670]) #&gt; #&gt; #&gt; [[11]] #&gt; [[11]][[1]] #&gt; [1] 10 #&gt; #&gt; [[11]][[2]] #&gt; tensor([7.0420]) #&gt; #&gt; #&gt; [[12]] #&gt; [[12]][[1]] #&gt; [1] 11 #&gt; #&gt; [[12]][[2]] #&gt; tensor([10.7910]) #&gt; #&gt; #&gt; [[13]] #&gt; [[13]][[1]] #&gt; [1] 12 #&gt; #&gt; [[13]][[2]] #&gt; tensor([5.3130]) #&gt; #&gt; #&gt; [[14]] #&gt; [[14]][[1]] #&gt; [1] 13 #&gt; #&gt; [[14]][[2]] #&gt; tensor([7.9970]) #&gt; #&gt; #&gt; [[15]] #&gt; [[15]][[1]] #&gt; [1] 14 #&gt; #&gt; [[15]][[2]] #&gt; tensor([3.1000]) 3.4.3 Using a for-loop to iterate Another way of iterating through a dataset that you will see a lot in the PyTorch tutorials is a loop through the length of the dataset. In this case, x_train. We are using cat() for the index (an integer), and print() for the tensor, since cat doesn’t know how to deal with tensors: # reset the iterator enum_x_train = py$enumerate(x_train) for (i in 1:py$len(x_train)) { obj &lt;- iter_next(enum_x_train) # next item cat(obj[[1]], &quot;\\t&quot;) # 1st part or index print(obj[[2]]) # 2nd part or tensor } #&gt; 0 tensor([3.3000]) #&gt; 1 tensor([4.4000]) #&gt; 2 tensor([5.5000]) #&gt; 3 tensor([6.7100]) #&gt; 4 tensor([6.9300]) #&gt; 5 tensor([4.1680]) #&gt; 6 tensor([9.7790]) #&gt; 7 tensor([6.1820]) #&gt; 8 tensor([7.5900]) #&gt; 9 tensor([2.1670]) #&gt; 10 tensor([7.0420]) #&gt; 11 tensor([10.7910]) #&gt; 12 tensor([5.3130]) #&gt; 13 tensor([7.9970]) #&gt; 14 tensor([3.1000]) We will find very frequently this kind of iterators when we read a dataset read by torchvision. There are several different ways to iterate through these objects as you will find. 3.5 Zero gradient The zero gradient was one of the most difficult to implement in R if we don’t pay attention to the content of the objects carrying the weights and biases. This happens when the algorithm written in PyTorch is not immediately translatable to rTorch. This can be appreciated in this example. We are using the same seed in the PyTorch and rTorch versions, so, we could compare the results. 3.5.1 Version in Python import numpy as np import torch torch.manual_seed(0) # reproducible # Input (temp, rainfall, humidity) #&gt; &lt;torch._C.Generator object at 0x7f9cfdf843d0&gt; inputs = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], [102, 43, 37], [69, 96, 70]], dtype=&#39;float32&#39;) # Targets (apples, oranges) targets = np.array([[56, 70], [81, 101], [119, 133], [22, 37], [103, 119]], dtype=&#39;float32&#39;) # Convert inputs and targets to tensors inputs = torch.from_numpy(inputs) targets = torch.from_numpy(targets) # random weights and biases w = torch.randn(2, 3, requires_grad=True) b = torch.randn(2, requires_grad=True) # function for the model def model(x): wt = w.t() mm = x @ w.t() return x @ w.t() + b # @ represents matrix multiplication in PyTorch # MSE loss function def mse(t1, t2): diff = t1 - t2 return torch.sum(diff * diff) / diff.numel() # Running all together # Train for 100 epochs for i in range(100): preds = model(inputs) loss = mse(preds, targets) loss.backward() with torch.no_grad(): w -= w.grad * 0.00001 b -= b.grad * 0.00001 w_gz = w.grad.zero_() b_gz = b.grad.zero_() # Calculate loss preds = model(inputs) loss = mse(preds, targets) print(&quot;Loss: &quot;, loss) # predictions #&gt; Loss: tensor(1270.1232, grad_fn=&lt;DivBackward0&gt;) print(&quot;\\nPredictions:&quot;) #&gt; #&gt; Predictions: preds # Targets #&gt; tensor([[ 69.3122, 80.2639], #&gt; [ 73.7528, 97.2381], #&gt; [118.3933, 124.7628], #&gt; [ 89.6111, 93.0286], #&gt; [ 47.3014, 80.6467]], grad_fn=&lt;AddBackward0&gt;) print(&quot;\\nTargets:&quot;) #&gt; #&gt; Targets: targets #&gt; tensor([[ 56., 70.], #&gt; [ 81., 101.], #&gt; [119., 133.], #&gt; [ 22., 37.], #&gt; [103., 119.]]) 3.5.2 Version in R library(rTorch) torch$manual_seed(0) device = torch$device(&#39;cpu&#39;) # Input (temp, rainfall, humidity) inputs = np$array(list(list(73, 67, 43), list(91, 88, 64), list(87, 134, 58), list(102, 43, 37), list(69, 96, 70)), dtype=&#39;float32&#39;) # Targets (apples, oranges) targets = np$array(list(list(56, 70), list(81, 101), list(119, 133), list(22, 37), list(103, 119)), dtype=&#39;float32&#39;) # Convert inputs and targets to tensors inputs = torch$from_numpy(inputs) targets = torch$from_numpy(targets) # random numbers for weights and biases. Then convert to double() torch$set_default_dtype(torch$float64) w = torch$randn(2L, 3L, requires_grad=TRUE) #$double() b = torch$randn(2L, requires_grad=TRUE) #$double() model &lt;- function(x) { wt &lt;- w$t() return(torch$add(torch$mm(x, wt), b)) } # MSE loss mse = function(t1, t2) { diff &lt;- torch$sub(t1, t2) mul &lt;- torch$sum(torch$mul(diff, diff)) return(torch$div(mul, diff$numel())) } # Running all together # Adjust weights and reset gradients for (i in 1:100) { preds = model(inputs) loss = mse(preds, targets) loss$backward() with(torch$no_grad(), { w$data &lt;- torch$sub(w$data, torch$mul(w$grad, torch$scalar_tensor(1e-5))) b$data &lt;- torch$sub(b$data, torch$mul(b$grad, torch$scalar_tensor(1e-5))) w$grad$zero_() b$grad$zero_() }) } # Calculate loss preds = model(inputs) loss = mse(preds, targets) cat(&quot;Loss: &quot;); print(loss) # predictions cat(&quot;\\nPredictions:\\n&quot;) preds # Targets cat(&quot;\\nTargets:\\n&quot;) targets #&gt; &lt;torch._C.Generator&gt; #&gt; Loss: tensor(1270.1237, grad_fn=&lt;DivBackward0&gt;) #&gt; #&gt; Predictions: #&gt; tensor([[ 69.3122, 80.2639], #&gt; [ 73.7528, 97.2381], #&gt; [118.3933, 124.7628], #&gt; [ 89.6111, 93.0286], #&gt; [ 47.3013, 80.6467]], grad_fn=&lt;AddBackward0&gt;) #&gt; #&gt; Targets: #&gt; tensor([[ 56., 70.], #&gt; [ 81., 101.], #&gt; [119., 133.], #&gt; [ 22., 37.], #&gt; [103., 119.]]) Notice that while we are in Python, the tensor operation, gradient (\\(\\nabla\\)) of the weights \\(w\\) times the Learning Rate \\(\\alpha\\), is: \\[w = -w + \\nabla w \\; \\alpha\\] In Python, it is a very straight forwward and clean code: w -= w.grad * 1e-5 In R it shows a little bit more convoluted: w$data &lt;- torch$sub(w$data, torch$mul(w$grad, torch$scalar_tensor(1e-5))) 3.6 R generics for PyTorch functions Which why we simplified these common operations using the R generic function. When we use the generic methods from rTorch the operation looks much neater. w$data &lt;- w$data - w$grad * 1e-5 The following two expressions are equivalent, with the first being the long version natural way of doing it in PyTorch. The second is using the generics in R for subtraction, multiplication and scalar conversion. param$data &lt;- torch$sub(param$data, torch$mul(param$grad$float(), torch$scalar_tensor(learning_rate))) } param$data &lt;- param$data - param$grad * learning_rate "],
["converting-tensors.html", "Chapter 4 Converting tensors 4.1 Transforming a tensor from numpy and viceversa 4.2 Transforming a tensor from PyTorch to R and viceversa", " Chapter 4 Converting tensors library(rTorch) 4.1 Transforming a tensor from numpy and viceversa Explain how transform a tensor back and forth to numpy. Why is this important? In what cases in this necessary? 4.1.1 Convert a tensor to numpy object This is a frequent operation. I have found that this is necessary when: a numpy function is not implemented in PyTorch We need to convert a tensor to R Perform a boolean operation that is not directly available in PyTorch 4.1.2 Convert a numpy object to an R object This is mainly required for these reasons: Create a data structure in R Plot using r-base or ggplot2 Perform an analysis on parts of a tensor Use R statistical functions that are not available in PyTorch 4.2 Transforming a tensor from PyTorch to R and viceversa TODO "],
["tensors.html", "Chapter 5 Tensors 5.1 Tensor data types 5.2 Arithmetic of tensors 5.3 NumPy and PyTorch 5.4 Create tensors 5.5 Tensor resizing 5.6 Concatenate tensors 5.7 Reshape tensors 5.8 Special tensors 5.9 Access to tensor elements 5.10 Other tensor operations 5.11 Logical operations 5.12 Distributions", " Chapter 5 Tensors We describe the most important PyTorch methods in this chapter. library(rTorch) 5.1 Tensor data types # Default data type torch$tensor(list(1.2, 3))$dtype # default for floating point is torch.float32 #&gt; torch.float64 # change default data type to float64 torch$set_default_dtype(torch$float64) torch$tensor(list(1.2, 3))$dtype # a new floating point tensor #&gt; torch.float64 5.1.1 Major tensor types There are five major type of tensors in PyTorch: library(rTorch) byte &lt;- torch$ByteTensor(3L, 3L) float &lt;- torch$FloatTensor(3L, 3L) double &lt;- torch$DoubleTensor(3L, 3L) long &lt;- torch$LongTensor(3L, 3L) boolean &lt;- torch$BoolTensor(5L, 5L) message(&quot;byte tensor&quot;) #&gt; byte tensor byte #&gt; tensor([[ 32, 77, 169], #&gt; [ 29, 157, 127], #&gt; [ 0, 0, 224]], dtype=torch.uint8) message(&quot;float tensor&quot;) #&gt; float tensor float #&gt; tensor([[4.4814e-21, 4.5779e-41, 1.1552e-01], #&gt; [3.0833e-41, 0.0000e+00, 0.0000e+00], #&gt; [0.0000e+00, 0.0000e+00, 0.0000e+00]], dtype=torch.float32) message(&quot;double&quot;) #&gt; double double #&gt; tensor([[0., 0., 0.], #&gt; [0., 0., 0.], #&gt; [0., 0., 0.]]) message(&quot;long&quot;) #&gt; long long #&gt; tensor([[ 0, 0, 0], #&gt; [94503201405744, 0, 94503174161936], #&gt; [ 0, 0, 0]]) message(&quot;boolean&quot;) #&gt; boolean boolean #&gt; tensor([[False, False, False, False, False], #&gt; [False, False, False, False, False], #&gt; [False, False, False, False, False], #&gt; [False, False, False, False, False], #&gt; [False, False, False, False, False]]) 5.1.2 Example: Basic attributes of a 4D tensor A 4D tensor like in MNIST hand-written digits recognition dataset: mnist_4d &lt;- torch$FloatTensor(60000L, 3L, 28L, 28L) message(&quot;size&quot;) #&gt; size mnist_4d$size() #&gt; torch.Size([60000, 3, 28, 28]) message(&quot;length&quot;) #&gt; length length(mnist_4d) #&gt; [1] 141120000 message(&quot;shape, like in numpy&quot;) #&gt; shape, like in numpy mnist_4d$shape #&gt; torch.Size([60000, 3, 28, 28]) message(&quot;number of elements&quot;) #&gt; number of elements mnist_4d$numel() #&gt; [1] 141120000 5.1.3 Example: Attributes of a 3D tensor Given a 3D tensor: ft3d &lt;- torch$FloatTensor(4L, 3L, 2L) ft3d #&gt; tensor([[[5.7453e-44, 0.0000e+00], #&gt; [0.0000e+00, 0.0000e+00], #&gt; [2.8026e-45, 4.2039e-45]], #&gt; #&gt; [[5.6052e-45, 7.0065e-45], #&gt; [8.4078e-45, 9.8091e-45], #&gt; [1.1210e-44, 1.2612e-44]], #&gt; #&gt; [[1.4013e-44, 1.5414e-44], #&gt; [1.6816e-44, 1.8217e-44], #&gt; [1.9618e-44, 2.1019e-44]], #&gt; #&gt; [[2.2421e-44, 2.3822e-44], #&gt; [2.5223e-44, 2.6625e-44], #&gt; [2.8026e-44, 2.9427e-44]]], dtype=torch.float32) ft3d$size() #&gt; torch.Size([4, 3, 2]) length(ft3d) #&gt; [1] 24 ft3d$shape #&gt; torch.Size([4, 3, 2]) ft3d$numel #&gt; &lt;built-in method numel of Tensor&gt; 5.2 Arithmetic of tensors 5.2.1 Add tensors # add a scalar to a tensor # 3x5 matrix uniformly distributed between 0 and 1 mat0 &lt;- torch$FloatTensor(3L, 5L)$uniform_(0L, 1L) mat0 + 0.1 #&gt; tensor([[0.7977, 0.9000, 0.2610, 0.3823, 0.7816], #&gt; [1.0152, 0.4971, 0.9742, 0.5194, 0.6529], #&gt; [1.0527, 0.1362, 0.2852, 0.4734, 0.4051]], dtype=torch.float32) 5.2.2 Add an element of a tensor to another tensor # fill a 3x5 matrix with 0.1 mat1 &lt;- torch$FloatTensor(3L, 5L)$uniform_(0.1, 0.1) print(mat1) #&gt; tensor([[0.1000, 0.1000, 0.1000, 0.1000, 0.1000], #&gt; [0.1000, 0.1000, 0.1000, 0.1000, 0.1000], #&gt; [0.1000, 0.1000, 0.1000, 0.1000, 0.1000]], dtype=torch.float32) # a vector with all ones mat2 &lt;- torch$FloatTensor(5L)$uniform_(1, 1) print(mat2) #&gt; tensor([1., 1., 1., 1., 1.], dtype=torch.float32) # add element (1,1) to another tensor mat1[1, 1] + mat2 #&gt; tensor([1.1000, 1.1000, 1.1000, 1.1000, 1.1000], dtype=torch.float32) Add two tensors using the function add(): # PyTorch add two tensors x = torch$rand(5L, 4L) y = torch$rand(5L, 4L) print(x$add(y)) #&gt; tensor([[0.9994, 1.0360, 1.0500, 1.7360], #&gt; [1.6304, 1.3039, 1.2017, 1.4232], #&gt; [1.8414, 1.4414, 0.9076, 0.9437], #&gt; [1.1563, 0.7022, 1.4463, 0.7936], #&gt; [0.9026, 0.9553, 0.2205, 1.3523]]) Add two tensors using the generic +: print(x + y) #&gt; tensor([[0.9994, 1.0360, 1.0500, 1.7360], #&gt; [1.6304, 1.3039, 1.2017, 1.4232], #&gt; [1.8414, 1.4414, 0.9076, 0.9437], #&gt; [1.1563, 0.7022, 1.4463, 0.7936], #&gt; [0.9026, 0.9553, 0.2205, 1.3523]]) 5.2.3 Multiply a tensor by a scalar # Multiply tensor by scalar tensor = torch$ones(4L, dtype=torch$float64) scalar = np$float64(4.321) print(scalar) print(torch$scalar_tensor(scalar)) #&gt; [1] 4.32 #&gt; tensor(4.3210) Multiply two tensors using the function mul: (prod = torch$mul(tensor, torch$scalar_tensor(scalar))) #&gt; tensor([4.3210, 4.3210, 4.3210, 4.3210]) Short version using generics (prod = tensor * scalar) #&gt; tensor([4.3210, 4.3210, 4.3210, 4.3210]) 5.3 NumPy and PyTorch numpy has been made available as a module in rTorch. We can call functions from numpy refrerring to it as np$_a_function. Examples: # a 2D numpy array syn0 &lt;- np$random$rand(3L, 5L) print(syn0) #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] 0.543 0.0668 0.653 0.996 0.7694 #&gt; [2,] 0.574 0.1026 0.700 0.661 0.0491 #&gt; [3,] 0.792 0.5187 0.426 0.788 0.4116 # numpy arrays of zeros syn1 &lt;- np$zeros(c(5L, 10L)) print(syn1) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] #&gt; [1,] 0 0 0 0 0 0 0 0 0 0 #&gt; [2,] 0 0 0 0 0 0 0 0 0 0 #&gt; [3,] 0 0 0 0 0 0 0 0 0 0 #&gt; [4,] 0 0 0 0 0 0 0 0 0 0 #&gt; [5,] 0 0 0 0 0 0 0 0 0 0 # add a scalar to a numpy array syn1 = syn1 + 0.1 print(syn1) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] #&gt; [1,] 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 #&gt; [2,] 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 #&gt; [3,] 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 #&gt; [4,] 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 #&gt; [5,] 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 And the dot product of both: np$dot(syn0, syn1) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] #&gt; [1,] 0.303 0.303 0.303 0.303 0.303 0.303 0.303 0.303 0.303 0.303 #&gt; [2,] 0.209 0.209 0.209 0.209 0.209 0.209 0.209 0.209 0.209 0.209 #&gt; [3,] 0.294 0.294 0.294 0.294 0.294 0.294 0.294 0.294 0.294 0.294 5.3.1 In Python we use Tuples, in R we use vectors In numpy a multidimensional array needs to be defined with a tuple. in R we do it with a vector. In Python, we use a tuple, (5, 5) to indicate the shape of the array: import numpy as np print(np.ones((5, 5))) #&gt; [[1. 1. 1. 1. 1.] #&gt; [1. 1. 1. 1. 1.] #&gt; [1. 1. 1. 1. 1.] #&gt; [1. 1. 1. 1. 1.] #&gt; [1. 1. 1. 1. 1.]] In R, we use a vector c(5L, 5L). The L indicates an integer. l1 &lt;- np$ones(c(5L, 5L)) print(l1) #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] 1 1 1 1 1 #&gt; [2,] 1 1 1 1 1 #&gt; [3,] 1 1 1 1 1 #&gt; [4,] 1 1 1 1 1 #&gt; [5,] 1 1 1 1 1 5.3.2 Build a numpy array from three R vectors X &lt;- np$array(rbind(c(1,2,3), c(4,5,6), c(7,8,9))) print(X) #&gt; [,1] [,2] [,3] #&gt; [1,] 1 2 3 #&gt; [2,] 4 5 6 #&gt; [3,] 7 8 9 And we could transpose the array using numpy as well: np$transpose(X) #&gt; [,1] [,2] [,3] #&gt; [1,] 1 4 7 #&gt; [2,] 2 5 8 #&gt; [3,] 3 6 9 5.3.3 Convert a numpy array to a tensor with as_tensor() a = np$array(list(1, 2, 3)) # a numpy array t = torch$as_tensor(a) # convert it to tensor print(t) #&gt; tensor([1., 2., 3.]) 5.3.4 Create and fill a tensor We can create the tensor directly from R using tensor(): torch$tensor(list( 1, 2, 3)) # create a tensor t[1L]$fill_(-1) # fill element with -1 print(a) #&gt; tensor([1., 2., 3.]) #&gt; tensor(-1.) #&gt; [1] -1 2 3 5.3.5 Tensor to array, and viceversa This is a very common operation in machine learning: # convert tensor to a numpy array a = torch$rand(5L, 4L) b = a$numpy() print(b) #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] 0.855 0.7947 0.589 0.655 #&gt; [2,] 0.543 0.5402 0.711 0.104 #&gt; [3,] 0.974 0.9348 0.624 0.194 #&gt; [4,] 0.142 0.8700 0.378 0.215 #&gt; [5,] 0.546 0.0517 0.507 0.730 # convert a numpy array to a tensor np_a = np$array(c(c(3, 4), c(3, 6))) t_a = torch$from_numpy(np_a) print(t_a) #&gt; tensor([3., 4., 3., 6.]) 5.4 Create tensors A random 1D tensor: ft1 &lt;- torch$FloatTensor(np$random$rand(5L)) print(ft1) #&gt; tensor([0.4810, 0.1816, 0.3213, 0.8455, 0.1869], dtype=torch.float32) Force a tensor as a float of 64-bits: ft2 &lt;- torch$as_tensor(np$random$rand(5L), dtype= torch$float64) print(ft2) #&gt; tensor([0.4173, 0.9890, 0.2366, 0.9168, 0.9184]) Convert the tensor to a float of 16-bits: ft2_dbl &lt;- torch$as_tensor(ft2, dtype = torch$float16) ft2_dbl #&gt; tensor([0.4172, 0.9893, 0.2366, 0.9170, 0.9185], dtype=torch.float16) Create a tensor of size (5 x 7) with uninitialized memory: a &lt;- torch$FloatTensor(5L, 7L) print(a) #&gt; tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, #&gt; 0.0000e+00], #&gt; [0.0000e+00, 5.8148e-24, 4.5779e-41, 0.0000e+00, 0.0000e+00, 0.0000e+00, #&gt; 0.0000e+00], #&gt; [0.0000e+00, 0.0000e+00, 1.1057e-19, 0.0000e+00, 0.0000e+00, 0.0000e+00, #&gt; 1.6143e-42], #&gt; [9.9632e-43, 7.0299e+00, 3.0833e-41, 7.0299e+00, 3.0833e-41, 1.6143e-42, #&gt; 0.0000e+00], #&gt; [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, #&gt; 0.0000e+00]], dtype=torch.float32) Using arange to create a tensor. arange starts at 0. v = torch$arange(9L) print(v) #&gt; tensor([0, 1, 2, 3, 4, 5, 6, 7, 8]) # reshape (v = v$view(3L, 3L)) #&gt; tensor([[0, 1, 2], #&gt; [3, 4, 5], #&gt; [6, 7, 8]]) 5.4.1 Tensor fill On this tensor: (v = torch$ones(3L, 3L)) #&gt; tensor([[1., 1., 1.], #&gt; [1., 1., 1.], #&gt; [1., 1., 1.]]) Fill row 1 with 2s: invisible(v[1L, ]$fill_(2L)) print(v) #&gt; tensor([[2., 2., 2.], #&gt; [1., 1., 1.], #&gt; [1., 1., 1.]]) Fill row 2 with 3s: invisible(v[2L, ]$fill_(3L)) print(v) #&gt; tensor([[2., 2., 2.], #&gt; [3., 3., 3.], #&gt; [1., 1., 1.]]) Fill column 3 with fours (4): invisible(v[, 3]$fill_(4L)) print(v) #&gt; tensor([[2., 2., 4.], #&gt; [3., 3., 4.], #&gt; [1., 1., 4.]]) 5.4.2 Initialize Tensor with a range of values # Initialize Tensor with a range of value v = torch$arange(10L) # similar to range(5) but creating a Tensor (v = torch$arange(0L, 10L, step = 1L)) # Size 5. Similar to range(0, 5, 1) #&gt; tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 5.4.3 Initialize a linear or log scale Tensor Create a tensor with 10 linear points for (1, 10) inclusive: (v = torch$linspace(1L, 10L, steps = 10L)) #&gt; tensor([ 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.]) Create a tensor with 10 logarithmic points for (1, 10) inclusive: (v = torch$logspace(start=-10L, end = 10L, steps = 5L)) #&gt; tensor([1.0000e-10, 1.0000e-05, 1.0000e+00, 1.0000e+05, 1.0000e+10]) 5.4.4 Fill a tensor In-place / Out-of-place On this uninitialized tensor: (a &lt;- torch$FloatTensor(5L, 7L)) #&gt; tensor([[4.4815e-21, 4.5779e-41, 2.1619e-02, 3.0833e-41, 1.1369e-08, 3.0833e-41, #&gt; 2.2025e-03], #&gt; [3.0833e-41, 4.3614e-07, 3.0833e-41, 2.2025e-03, 3.0833e-41, 4.3613e-07, #&gt; 3.0833e-41], #&gt; [4.3613e-07, 3.0833e-41, 4.3613e-07, 3.0833e-41, 4.3613e-07, 3.0833e-41, #&gt; 2.2025e-03], #&gt; [3.0833e-41, 4.6573e-07, 3.0833e-41, 1.1369e-08, 3.0833e-41, 4.3612e-07, #&gt; 3.0833e-41], #&gt; [1.1369e-08, 3.0833e-41, 4.3612e-07, 3.0833e-41, 4.3612e-07, 3.0833e-41, #&gt; 4.3611e-07]], dtype=torch.float32) Fill the tensor with the value 3.5: a$fill_(3.5) #&gt; tensor([[3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], #&gt; [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], #&gt; [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], #&gt; [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], #&gt; [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000]], #&gt; dtype=torch.float32) Add a scalar to the tensor: b &lt;- a$add(4.0) The tensor a is still filled with 3.5. A new tensor b is returned with values 3.5 + 4.0 = 7.5 print(a) print(b) #&gt; tensor([[3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], #&gt; [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], #&gt; [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], #&gt; [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], #&gt; [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000]], #&gt; dtype=torch.float32) #&gt; tensor([[7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000], #&gt; [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000], #&gt; [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000], #&gt; [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000], #&gt; [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000]], #&gt; dtype=torch.float32) 5.5 Tensor resizing x = torch$randn(2L, 3L) # Size 2x3 print(x) #&gt; tensor([[-0.7391, 0.8027, -0.6817], #&gt; [-0.1335, 0.0658, -0.5919]]) y = x$view(6L) # Resize x to size 6 print(y) #&gt; tensor([-0.7391, 0.8027, -0.6817, -0.1335, 0.0658, -0.5919]) z = x$view(-1L, 2L) # Size 3x2 print(z) #&gt; tensor([[-0.7391, 0.8027], #&gt; [-0.6817, -0.1335], #&gt; [ 0.0658, -0.5919]]) print(z$shape) #&gt; torch.Size([3, 2]) 5.5.1 Exercise Reproduce this tensor: 0 1 2 3 4 5 6 7 8 # create a vector with the number of elements v = torch$arange(9L) # resize to a 3x3 tensor (v = v$view(3L, 3L)) #&gt; tensor([[0, 1, 2], #&gt; [3, 4, 5], #&gt; [6, 7, 8]]) 5.6 Concatenate tensors x = torch$randn(2L, 3L) print(x) print(x$shape) #&gt; tensor([[ 0.7670, 0.6899, 0.3282], #&gt; [ 0.5085, -0.0515, -0.6248]]) #&gt; torch.Size([2, 3]) 5.6.1 Concatenate tensors by dim=0 (rows) (x0 &lt;- torch$cat(list(x, x, x), 0L)) print(x0$shape) #&gt; tensor([[ 0.7670, 0.6899, 0.3282], #&gt; [ 0.5085, -0.0515, -0.6248], #&gt; [ 0.7670, 0.6899, 0.3282], #&gt; [ 0.5085, -0.0515, -0.6248], #&gt; [ 0.7670, 0.6899, 0.3282], #&gt; [ 0.5085, -0.0515, -0.6248]]) #&gt; torch.Size([6, 3]) 5.6.2 Concatenate tensors by dim=1 (columns) (x1 &lt;- torch$cat(list(x, x, x), 1L)) print(x1$shape) #&gt; tensor([[ 0.7670, 0.6899, 0.3282, 0.7670, 0.6899, 0.3282, 0.7670, 0.6899, #&gt; 0.3282], #&gt; [ 0.5085, -0.0515, -0.6248, 0.5085, -0.0515, -0.6248, 0.5085, -0.0515, #&gt; -0.6248]]) #&gt; torch.Size([2, 9]) 5.7 Reshape tensors 5.7.1 With chunk(): Let’s say this is an image tensor with the 3-channels and 28x28 pixels # ----- Reshape tensors ----- img &lt;- torch$ones(3L, 28L, 28L) # Create the tensor of ones print(img$size()) #&gt; torch.Size([3, 28, 28]) On the first dimension dim = 0L, reshape the tensor: img_chunks &lt;- torch$chunk(img, chunks = 3L, dim = 0L) print(length(img_chunks)) print(class(img_chunks)) #&gt; [1] 3 #&gt; [1] &quot;list&quot; img_chunks is a list of three members. The first chunk member: # 1st chunk member img_chunk &lt;- img_chunks[[1]] print(img_chunk$size()) print(img_chunk$sum()) # if the tensor had all ones, what is the sum? #&gt; torch.Size([1, 28, 28]) #&gt; tensor(784.) The second chunk member: # 2nd chunk member img_chunk &lt;- img_chunks[[2]] print(img_chunk$size()) print(img_chunk$sum()) # if the tensor had all ones, what is the sum? #&gt; torch.Size([1, 28, 28]) #&gt; tensor(784.) # 3rd chunk member img_chunk &lt;- img_chunks[[3]] print(img_chunk$size()) print(img_chunk$sum()) # if the tensor had all ones, what is the sum? #&gt; torch.Size([1, 28, 28]) #&gt; tensor(784.) 5.7.1.1 Exercise Create a tensor of shape 3x28x28 filled with values 0.25 on the first channel The second channel with 0.5 The third chanel with 0.75 Find the sum for ecah separate channel Find the sum of all channels 5.7.2 With index_select(): img &lt;- torch$ones(3L, 28L, 28L) # Create the tensor of ones img$size() #&gt; torch.Size([3, 28, 28]) This is the layer 1: # index_select. get layer 1 indices = torch$tensor(c(0L)) img_layer_1 &lt;- torch$index_select(img, dim = 0L, index = indices) The size of the layer: print(img_layer_1$size()) #&gt; torch.Size([1, 28, 28]) The sum of all elements in that layer: print(img_layer_1$sum()) #&gt; tensor(784.) This is the layer 2: # index_select. get layer 2 indices = torch$tensor(c(1L)) img_layer_2 &lt;- torch$index_select(img, dim = 0L, index = indices) print(img_layer_2$size()) print(img_layer_2$sum()) #&gt; torch.Size([1, 28, 28]) #&gt; tensor(784.) This is the layer 3: # index_select. get layer 3 indices = torch$tensor(c(2L)) img_layer_3 &lt;- torch$index_select(img, dim = 0L, index = indices) print(img_layer_3$size()) print(img_layer_3$sum()) #&gt; torch.Size([1, 28, 28]) #&gt; tensor(784.) 5.8 Special tensors 5.8.1 Identity matrix # identity matrix eye = torch$eye(3L) # Create an identity 3x3 tensor print(eye) #&gt; tensor([[1., 0., 0.], #&gt; [0., 1., 0.], #&gt; [0., 0., 1.]]) # a 5x5 identity or unit matrix torch$eye(5L) #&gt; tensor([[1., 0., 0., 0., 0.], #&gt; [0., 1., 0., 0., 0.], #&gt; [0., 0., 1., 0., 0.], #&gt; [0., 0., 0., 1., 0.], #&gt; [0., 0., 0., 0., 1.]]) 5.8.2 Ones (v = torch$ones(10L)) # A tensor of size 10 containing all ones # reshape (v = torch$ones(2L, 1L, 2L, 1L)) # Size 2x1x2x1, a 4D tensor #&gt; tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) #&gt; tensor([[[[1.], #&gt; [1.]]], #&gt; #&gt; #&gt; [[[1.], #&gt; [1.]]]]) The matrix of ones is also called `unitary matrix. This is a 4x4 unitary matrix. torch$ones(c(4L, 4L)) #&gt; tensor([[1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.]]) # eye tensor eye = torch$eye(3L) print(eye) # like eye tensor v = torch$ones_like(eye) # A tensor with same shape as eye. Fill it with 1. v #&gt; tensor([[1., 0., 0.], #&gt; [0., 1., 0.], #&gt; [0., 0., 1.]]) #&gt; tensor([[1., 1., 1.], #&gt; [1., 1., 1.], #&gt; [1., 1., 1.]]) 5.8.3 Zeros (z = torch$zeros(10L)) # A tensor of size 10 containing all zeros #&gt; tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) # matrix of zeros torch$zeros(c(4L, 4L)) #&gt; tensor([[0., 0., 0., 0.], #&gt; [0., 0., 0., 0.], #&gt; [0., 0., 0., 0.], #&gt; [0., 0., 0., 0.]]) # a 3D tensor of zeros torch$zeros(c(3L, 4L, 2L)) #&gt; tensor([[[0., 0.], #&gt; [0., 0.], #&gt; [0., 0.], #&gt; [0., 0.]], #&gt; #&gt; [[0., 0.], #&gt; [0., 0.], #&gt; [0., 0.], #&gt; [0., 0.]], #&gt; #&gt; [[0., 0.], #&gt; [0., 0.], #&gt; [0., 0.], #&gt; [0., 0.]]]) 5.8.4 Diagonal operations Given the 1D tensor a &lt;- torch$tensor(c(1L, 2L, 3L)) a #&gt; tensor([1, 2, 3]) 5.8.4.1 Diagonal matrix We want to fill the main diagonal with the vector: torch$diag(a) #&gt; tensor([[1, 0, 0], #&gt; [0, 2, 0], #&gt; [0, 0, 3]]) What about filling the diagonal above the main: torch$diag(a, 1L) #&gt; tensor([[0, 1, 0, 0], #&gt; [0, 0, 2, 0], #&gt; [0, 0, 0, 3], #&gt; [0, 0, 0, 0]]) Or the diagonal below the main: torch$diag(a, -1L) #&gt; tensor([[0, 0, 0, 0], #&gt; [1, 0, 0, 0], #&gt; [0, 2, 0, 0], #&gt; [0, 0, 3, 0]]) 5.9 Access to tensor elements # replace an element at position 0, 0 (new_tensor = torch$Tensor(list(list(1, 2), list(3, 4)))) #&gt; tensor([[1., 2.], #&gt; [3., 4.]]) Print element at position 1,1: print(new_tensor[1L, 1L]) #&gt; tensor(1.) Fill element at position 1,1 with 5: new_tensor[1L, 1L]$fill_(5) #&gt; tensor(5.) Show the modified tensor: print(new_tensor) # tensor([[ 5., 2.],[ 3., 4.]]) #&gt; tensor([[5., 2.], #&gt; [3., 4.]]) Access an element at position 1, 0: print(new_tensor[2L, 1L]) # tensor([ 3.]) print(new_tensor[2L, 1L]$item()) # 3. #&gt; tensor(3.) #&gt; [1] 3 5.9.1 Using indices to access elements On this tensor: x = torch$randn(3L, 4L) print(x) #&gt; tensor([[ 0.2844, -0.9130, 1.1218, -0.6852], #&gt; [-0.0445, 1.7928, -0.8348, 0.8459], #&gt; [ 0.1077, 0.4118, -2.1578, -0.1408]]) Select indices, dim=0: indices = torch$tensor(list(0L, 2L)) torch$index_select(x, 0L, indices) #&gt; tensor([[ 0.2844, -0.9130, 1.1218, -0.6852], #&gt; [ 0.1077, 0.4118, -2.1578, -0.1408]]) Select indices, dim=1: torch$index_select(x, 1L, indices) #&gt; tensor([[ 0.2844, 1.1218], #&gt; [-0.0445, -0.8348], #&gt; [ 0.1077, -2.1578]]) 5.9.2 Using the take function # Take by indices src = torch$tensor(list(list(4, 3, 5), list(6, 7, 8)) ) print(src) print( torch$take(src, torch$tensor(list(0L, 2L, 5L))) ) #&gt; tensor([[4., 3., 5.], #&gt; [6., 7., 8.]]) #&gt; tensor([4., 5., 8.]) 5.10 Other tensor operations 5.10.1 Cross product m1 = torch$ones(3L, 5L) m2 = torch$ones(3L, 5L) v1 = torch$ones(3L) # Cross product # Size 3x5 (r = torch$cross(m1, m2)) #&gt; tensor([[0., 0., 0., 0., 0.], #&gt; [0., 0., 0., 0., 0.], #&gt; [0., 0., 0., 0., 0.]]) 5.10.2 Dot product # Dot product of 2 tensors # Dot product of 2 tensors p &lt;- torch$Tensor(list(4L, 2L)) q &lt;- torch$Tensor(list(3L, 1L)) (r = torch$dot(p, q)) # 14 #&gt; tensor(14.) (r &lt;- p %.*% q) # 14 #&gt; tensor(14.) 5.11 Logical operations m0 = torch$zeros(3L, 5L) m1 = torch$ones(3L, 5L) m2 = torch$eye(3L, 5L) print(m1 == m0) #&gt; tensor([[False, False, False, False, False], #&gt; [False, False, False, False, False], #&gt; [False, False, False, False, False]]) print(m1 != m1) #&gt; tensor([[False, False, False, False, False], #&gt; [False, False, False, False, False], #&gt; [False, False, False, False, False]]) print(m2 == m2) #&gt; tensor([[True, True, True, True, True], #&gt; [True, True, True, True, True], #&gt; [True, True, True, True, True]]) # AND m1 &amp; m1 #&gt; tensor([[1, 1, 1, 1, 1], #&gt; [1, 1, 1, 1, 1], #&gt; [1, 1, 1, 1, 1]], dtype=torch.uint8) # OR m0 | m2 #&gt; tensor([[1, 0, 0, 0, 0], #&gt; [0, 1, 0, 0, 0], #&gt; [0, 0, 1, 0, 0]], dtype=torch.uint8) # OR m1 | m2 #&gt; tensor([[1, 1, 1, 1, 1], #&gt; [1, 1, 1, 1, 1], #&gt; [1, 1, 1, 1, 1]], dtype=torch.uint8) 5.11.1 Using a function to extract a unique logical result With all: # tensor is less than A &lt;- torch$ones(60000L, 1L, 28L, 28L) C &lt;- A * 0.5 # is C &lt; A all(torch$lt(C, A)) #&gt; tensor(1, dtype=torch.uint8) all(C &lt; A) #&gt; tensor(1, dtype=torch.uint8) # is A &lt; C all(A &lt; C) #&gt; tensor(0, dtype=torch.uint8) With function all_boolean: all_boolean &lt;- function(x) { # convert tensor of 1s and 0s to a unique boolean as.logical(torch$all(x)$numpy()) } # is C &lt; A all_boolean(torch$lt(C, A)) #&gt; [1] TRUE all_boolean(C &lt; A) #&gt; [1] TRUE # is A &lt; C all_boolean(A &lt; C) #&gt; [1] FALSE 5.11.2 Greater than (gt) # tensor is greater than A &lt;- torch$ones(60000L, 1L, 28L, 28L) D &lt;- A * 2.0 all(torch$gt(D, A)) #&gt; tensor(1, dtype=torch.uint8) all(torch$gt(A, D)) #&gt; tensor(0, dtype=torch.uint8) 5.11.3 Less than or equal (le) # tensor is less than or equal A1 &lt;- torch$ones(60000L, 1L, 28L, 28L) all(torch$le(A1, A1)) #&gt; tensor(1, dtype=torch.uint8) all(A1 &lt;= A1) #&gt; tensor(1, dtype=torch.uint8) # tensor is greater than or equal A0 &lt;- torch$zeros(60000L, 1L, 28L, 28L) all(torch$ge(A0, A0)) #&gt; tensor(1, dtype=torch.uint8) all(A0 &gt;= A0) #&gt; tensor(1, dtype=torch.uint8) all(A1 &gt;= A0) #&gt; tensor(1, dtype=torch.uint8) all(A1 &lt;= A0) #&gt; tensor(0, dtype=torch.uint8) 5.11.4 Logical NOT (!`) all_true &lt;- torch$BoolTensor(list(TRUE, TRUE, TRUE, TRUE)) all_true #&gt; tensor([True, True, True, True]) # logical NOT not_all_true &lt;- !all_true not_all_true #&gt; tensor([False, False, False, False]) diag &lt;- torch$eye(5L) diag #&gt; tensor([[1., 0., 0., 0., 0.], #&gt; [0., 1., 0., 0., 0.], #&gt; [0., 0., 1., 0., 0.], #&gt; [0., 0., 0., 1., 0.], #&gt; [0., 0., 0., 0., 1.]]) # logical NOT not_diag &lt;- !diag # convert to integer not_diag$to(dtype=torch$uint8) #&gt; tensor([[0, 1, 1, 1, 1], #&gt; [1, 0, 1, 1, 1], #&gt; [1, 1, 0, 1, 1], #&gt; [1, 1, 1, 0, 1], #&gt; [1, 1, 1, 1, 0]], dtype=torch.uint8) 5.12 Distributions Initialize a tensor randomized with a normal distribution with mean=0, var=1: n &lt;- torch$randn(3500L) n #&gt; tensor([-1.6469, 1.1748, 0.2185, ..., 0.2143, 0.7866, -1.9680]) plot(n$numpy()) hist(n$numpy()) a &lt;- torch$randn(8L, 5L, 6L) # print(a) print(a$size()) #&gt; torch.Size([8, 5, 6]) plot(a$flatten()$numpy()) hist(a$flatten()$numpy()) 5.12.1 Uniform matrix library(rTorch) # 3x5 matrix uniformly distributed between 0 and 1 mat0 &lt;- torch$FloatTensor(13L, 15L)$uniform_(0L, 1L) plot(mat0$flatten()$numpy()) hist(mat0$flatten()$numpy()) # fill a 3x5 matrix with 0.1 mat1 &lt;- torch$FloatTensor(30L, 50L)$uniform_(0.1, 0.2) plot(mat1$flatten()$numpy()) hist(mat1$flatten()$numpy()) # a vector with all ones mat2 &lt;- torch$FloatTensor(500L)$uniform_(1, 2) plot(mat2$flatten()$numpy()) hist(mat2$flatten()$numpy()) 5.12.2 Binomial distribution Binomial &lt;- torch$distributions$binomial$Binomial m = Binomial(100, torch$tensor(list(0 , .2, .8, 1))) (x = m$sample()) #&gt; tensor([ 0., 21., 79., 100.]) m = Binomial(torch$tensor(list(list(5.), list(10.))), torch$tensor(list(0.5, 0.8))) (x = m$sample()) #&gt; tensor([[3., 3.], #&gt; [4., 9.]]) binom &lt;- Binomial(100, torch$FloatTensor(5L, 10L)) print(binom) #&gt; Binomial(total_count: torch.Size([5, 10]), probs: torch.Size([5, 10]), logits: torch.Size([5, 10])) print(binom$sample_n(100L)$shape) #&gt; torch.Size([100, 5, 10]) plot(binom$sample_n(100L)$flatten()$numpy()) hist(binom$sample_n(100L)$flatten()$numpy()) 5.12.3 Exponential distribution Exponential &lt;- torch$distributions$exponential$Exponential m = Exponential(torch$tensor(list(1.0))) m #&gt; Exponential(rate: tensor([1.])) m$sample() # Exponential distributed with rate=1 #&gt; tensor([4.3239]) expo &lt;- Exponential(rate=0.25) expo_sample &lt;- expo$sample_n(250L) # generate 250 samples print(expo_sample$shape) #&gt; torch.Size([250]) plot(expo_sample$flatten()$numpy()) hist(expo_sample$flatten()$numpy()) 5.12.4 Weibull distribution Weibull &lt;- torch$distributions$weibull$Weibull m = Weibull(torch$tensor(list(1.0)), torch$tensor(list(1.0))) m$sample() # sample from a Weibull distribution with scale=1, concentration=1 #&gt; tensor([0.4863]) 5.12.4.1 Weibull at constant scale # constant scale for (k in 1:10) { wei &lt;- Weibull(scale=100, concentration=k) wei_sample &lt;- wei$sample_n(500L) # plot(wei_sample$flatten()$numpy()) hist(main=paste0(&quot;Scale=100; Concentration=&quot;, k), wei_sample$flatten()$numpy()) } 5.12.4.2 Weibull at constant concentration # constant concentration for (s in seq(100, 1000, 100)) { wei &lt;- Weibull(scale=s, concentration=1) wei_sample &lt;- wei$sample_n(500L) # plot(wei_sample$flatten()$numpy()) hist(main=paste0(&quot;Concentration=1; Scale=&quot;, s), wei_sample$flatten()$numpy()) } "],
["linearalgebra.html", "Chapter 6 Linear Algebra with Torch 6.1 Scalars 6.2 Vectors 6.3 Matrices 6.4 3D+ tensors 6.5 Transpose of a matrix 6.6 Vectors, special case of a matrix 6.7 Tensor arithmetic 6.8 Add a scalar to a tensor 6.9 Multiplying tensors 6.10 Dot product", " Chapter 6 Linear Algebra with Torch The following are basic operations of Linear Algebra using PyTorch. library(rTorch) 6.1 Scalars torch$scalar_tensor(2.78654) torch$scalar_tensor(0L) torch$scalar_tensor(1L) torch$scalar_tensor(TRUE) torch$scalar_tensor(FALSE) #&gt; tensor(2.7865) #&gt; tensor(0.) #&gt; tensor(1.) #&gt; tensor(1.) #&gt; tensor(0.) 6.2 Vectors v &lt;- c(0, 1, 2, 3, 4, 5) torch$as_tensor(v) #&gt; tensor([0., 1., 2., 3., 4., 5.]) 6.2.1 Vector to matrix, matrix to tensor # row-vector message(&quot;R matrix&quot;) #&gt; R matrix (mr &lt;- matrix(1:10, nrow=1)) message(&quot;as_tensor&quot;) #&gt; as_tensor torch$as_tensor(mr) message(&quot;shape_of_tensor&quot;) #&gt; shape_of_tensor torch$as_tensor(mr)$shape #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] #&gt; [1,] 1 2 3 4 5 6 7 8 9 10 #&gt; tensor([[ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], dtype=torch.int32) #&gt; torch.Size([1, 10]) # column-vector message(&quot;R matrix, one column&quot;) #&gt; R matrix, one column (mc &lt;- matrix(1:10, ncol=1)) message(&quot;as_tensor&quot;) #&gt; as_tensor torch$as_tensor(mc) message(&quot;size of tensor&quot;) #&gt; size of tensor torch$as_tensor(mc)$shape #&gt; [,1] #&gt; [1,] 1 #&gt; [2,] 2 #&gt; [3,] 3 #&gt; [4,] 4 #&gt; [5,] 5 #&gt; [6,] 6 #&gt; [7,] 7 #&gt; [8,] 8 #&gt; [9,] 9 #&gt; [10,] 10 #&gt; tensor([[ 1], #&gt; [ 2], #&gt; [ 3], #&gt; [ 4], #&gt; [ 5], #&gt; [ 6], #&gt; [ 7], #&gt; [ 8], #&gt; [ 9], #&gt; [10]], dtype=torch.int32) #&gt; torch.Size([10, 1]) 6.3 Matrices message(&quot;R matrix&quot;) #&gt; R matrix (m1 &lt;- matrix(1:24, nrow = 3, byrow = TRUE)) message(&quot;as_tensor&quot;) #&gt; as_tensor (t1 &lt;- torch$as_tensor(m1)) message(&quot;shape&quot;) #&gt; shape torch$as_tensor(m1)$shape message(&quot;size&quot;) #&gt; size torch$as_tensor(m1)$size() message(&quot;dim&quot;) #&gt; dim dim(torch$as_tensor(m1)) message(&quot;length&quot;) #&gt; length length(torch$as_tensor(m1)) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] #&gt; [1,] 1 2 3 4 5 6 7 8 #&gt; [2,] 9 10 11 12 13 14 15 16 #&gt; [3,] 17 18 19 20 21 22 23 24 #&gt; tensor([[ 1, 2, 3, 4, 5, 6, 7, 8], #&gt; [ 9, 10, 11, 12, 13, 14, 15, 16], #&gt; [17, 18, 19, 20, 21, 22, 23, 24]], dtype=torch.int32) #&gt; torch.Size([3, 8]) #&gt; torch.Size([3, 8]) #&gt; [1] 3 8 #&gt; [1] 24 message(&quot;R matrix&quot;) #&gt; R matrix (m2 &lt;- matrix(0:99, ncol = 10)) message(&quot;as_tensor&quot;) #&gt; as_tensor (t2 &lt;- torch$as_tensor(m2)) message(&quot;shape&quot;) #&gt; shape t2$shape message(&quot;dim&quot;) #&gt; dim dim(torch$as_tensor(m2)) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] #&gt; [1,] 0 10 20 30 40 50 60 70 80 90 #&gt; [2,] 1 11 21 31 41 51 61 71 81 91 #&gt; [3,] 2 12 22 32 42 52 62 72 82 92 #&gt; [4,] 3 13 23 33 43 53 63 73 83 93 #&gt; [5,] 4 14 24 34 44 54 64 74 84 94 #&gt; [6,] 5 15 25 35 45 55 65 75 85 95 #&gt; [7,] 6 16 26 36 46 56 66 76 86 96 #&gt; [8,] 7 17 27 37 47 57 67 77 87 97 #&gt; [9,] 8 18 28 38 48 58 68 78 88 98 #&gt; [10,] 9 19 29 39 49 59 69 79 89 99 #&gt; tensor([[ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90], #&gt; [ 1, 11, 21, 31, 41, 51, 61, 71, 81, 91], #&gt; [ 2, 12, 22, 32, 42, 52, 62, 72, 82, 92], #&gt; [ 3, 13, 23, 33, 43, 53, 63, 73, 83, 93], #&gt; [ 4, 14, 24, 34, 44, 54, 64, 74, 84, 94], #&gt; [ 5, 15, 25, 35, 45, 55, 65, 75, 85, 95], #&gt; [ 6, 16, 26, 36, 46, 56, 66, 76, 86, 96], #&gt; [ 7, 17, 27, 37, 47, 57, 67, 77, 87, 97], #&gt; [ 8, 18, 28, 38, 48, 58, 68, 78, 88, 98], #&gt; [ 9, 19, 29, 39, 49, 59, 69, 79, 89, 99]], dtype=torch.int32) #&gt; torch.Size([10, 10]) #&gt; [1] 10 10 m1[1, 1] m2[1, 1] #&gt; [1] 1 #&gt; [1] 0 t1[1, 1] t2[1, 1] #&gt; tensor(1, dtype=torch.int32) #&gt; tensor(0, dtype=torch.int32) 6.4 3D+ tensors # RGB color image has three axes (img &lt;- torch$rand(3L, 28L, 28L)) img$shape #&gt; tensor([[[0.9305, 0.0977, 0.1086, ..., 0.9410, 0.4287, 0.8470], #&gt; [0.3552, 0.9911, 0.9336, ..., 0.4130, 0.6486, 0.3798], #&gt; [0.0309, 0.8260, 0.4665, ..., 0.7730, 0.7296, 0.8287], #&gt; ..., #&gt; [0.6221, 0.4723, 0.6094, ..., 0.6861, 0.6573, 0.8461], #&gt; [0.6244, 0.2570, 0.2129, ..., 0.0746, 0.8410, 0.1278], #&gt; [0.9194, 0.7521, 0.7040, ..., 0.5164, 0.8780, 0.7326]], #&gt; #&gt; [[0.7370, 0.6116, 0.4463, ..., 0.7635, 0.9839, 0.8554], #&gt; [0.5405, 0.7145, 0.5654, ..., 0.5249, 0.5295, 0.4627], #&gt; [0.2392, 0.7286, 0.0214, ..., 0.0293, 0.6653, 0.8782], #&gt; ..., #&gt; [0.6585, 0.2730, 0.8035, ..., 0.8771, 0.7390, 0.6112], #&gt; [0.9894, 0.7420, 0.9999, ..., 0.1553, 0.3077, 0.2575], #&gt; [0.7271, 0.0381, 0.6234, ..., 0.5454, 0.5445, 0.7738]], #&gt; #&gt; [[0.6961, 0.8182, 0.7035, ..., 0.4069, 0.2596, 0.5659], #&gt; [0.5953, 0.8211, 0.1043, ..., 0.3698, 0.9349, 0.4274], #&gt; [0.9318, 0.5161, 0.8102, ..., 0.6375, 0.4332, 0.9190], #&gt; ..., #&gt; [0.1481, 0.3094, 0.6684, ..., 0.6963, 0.1656, 0.8843], #&gt; [0.2439, 0.4446, 0.4962, ..., 0.2318, 0.3293, 0.1729], #&gt; [0.9333, 0.5044, 0.1210, ..., 0.1671, 0.3386, 0.0282]]]) #&gt; torch.Size([3, 28, 28]) img[1, 1, 1] img[3, 28, 28] #&gt; tensor(0.9305) #&gt; tensor(0.0282) 6.5 Transpose of a matrix (m3 &lt;- matrix(1:25, ncol = 5)) # transpose message(&quot;transpose&quot;) #&gt; transpose tm3 &lt;- t(m3) tm3 #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] 1 6 11 16 21 #&gt; [2,] 2 7 12 17 22 #&gt; [3,] 3 8 13 18 23 #&gt; [4,] 4 9 14 19 24 #&gt; [5,] 5 10 15 20 25 #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] 1 2 3 4 5 #&gt; [2,] 6 7 8 9 10 #&gt; [3,] 11 12 13 14 15 #&gt; [4,] 16 17 18 19 20 #&gt; [5,] 21 22 23 24 25 message(&quot;as_tensor&quot;) #&gt; as_tensor (t3 &lt;- torch$as_tensor(m3)) message(&quot;transpose&quot;) #&gt; transpose tt3 &lt;- t3$transpose(dim0 = 0L, dim1 = 1L) tt3 #&gt; tensor([[ 1, 6, 11, 16, 21], #&gt; [ 2, 7, 12, 17, 22], #&gt; [ 3, 8, 13, 18, 23], #&gt; [ 4, 9, 14, 19, 24], #&gt; [ 5, 10, 15, 20, 25]], dtype=torch.int32) #&gt; tensor([[ 1, 2, 3, 4, 5], #&gt; [ 6, 7, 8, 9, 10], #&gt; [11, 12, 13, 14, 15], #&gt; [16, 17, 18, 19, 20], #&gt; [21, 22, 23, 24, 25]], dtype=torch.int32) tm3 == tt3$numpy() # convert first the tensor to numpy #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] TRUE TRUE TRUE TRUE TRUE #&gt; [2,] TRUE TRUE TRUE TRUE TRUE #&gt; [3,] TRUE TRUE TRUE TRUE TRUE #&gt; [4,] TRUE TRUE TRUE TRUE TRUE #&gt; [5,] TRUE TRUE TRUE TRUE TRUE 6.6 Vectors, special case of a matrix message(&quot;R matrix&quot;) #&gt; R matrix m2 &lt;- matrix(0:99, ncol = 10) message(&quot;as_tensor&quot;) #&gt; as_tensor (t2 &lt;- torch$as_tensor(m2)) # in R message(&quot;select column of matrix&quot;) #&gt; select column of matrix (v1 &lt;- m2[, 1]) message(&quot;select row of matrix&quot;) #&gt; select row of matrix (v2 &lt;- m2[10, ]) #&gt; tensor([[ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90], #&gt; [ 1, 11, 21, 31, 41, 51, 61, 71, 81, 91], #&gt; [ 2, 12, 22, 32, 42, 52, 62, 72, 82, 92], #&gt; [ 3, 13, 23, 33, 43, 53, 63, 73, 83, 93], #&gt; [ 4, 14, 24, 34, 44, 54, 64, 74, 84, 94], #&gt; [ 5, 15, 25, 35, 45, 55, 65, 75, 85, 95], #&gt; [ 6, 16, 26, 36, 46, 56, 66, 76, 86, 96], #&gt; [ 7, 17, 27, 37, 47, 57, 67, 77, 87, 97], #&gt; [ 8, 18, 28, 38, 48, 58, 68, 78, 88, 98], #&gt; [ 9, 19, 29, 39, 49, 59, 69, 79, 89, 99]], dtype=torch.int32) #&gt; [1] 0 1 2 3 4 5 6 7 8 9 #&gt; [1] 9 19 29 39 49 59 69 79 89 99 # PyTorch message() #&gt; t2c &lt;- t2[, 1] t2r &lt;- t2[10, ] t2c t2r #&gt; tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32) #&gt; tensor([ 9, 19, 29, 39, 49, 59, 69, 79, 89, 99], dtype=torch.int32) In vectors, the vector and its transpose are equal. tt2r &lt;- t2r$transpose(dim0 = 0L, dim1 = 0L) tt2r #&gt; tensor([ 9, 19, 29, 39, 49, 59, 69, 79, 89, 99], dtype=torch.int32) # a tensor of booleans. is vector equal to its transposed? t2r == tt2r #&gt; tensor([True, True, True, True, True, True, True, True, True, True]) 6.7 Tensor arithmetic message(&quot;x&quot;) #&gt; x (x = torch$ones(5L, 4L)) message(&quot;y&quot;) #&gt; y (y = torch$ones(5L, 4L)) message(&quot;x+y&quot;) #&gt; x+y x + y #&gt; tensor([[1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.]]) #&gt; tensor([[1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.]]) #&gt; tensor([[2., 2., 2., 2.], #&gt; [2., 2., 2., 2.], #&gt; [2., 2., 2., 2.], #&gt; [2., 2., 2., 2.], #&gt; [2., 2., 2., 2.]]) \\[A + B = B + A\\] x + y == y + x #&gt; tensor([[True, True, True, True], #&gt; [True, True, True, True], #&gt; [True, True, True, True], #&gt; [True, True, True, True], #&gt; [True, True, True, True]]) 6.8 Add a scalar to a tensor s &lt;- 0.5 # scalar x + s #&gt; tensor([[1.5000, 1.5000, 1.5000, 1.5000], #&gt; [1.5000, 1.5000, 1.5000, 1.5000], #&gt; [1.5000, 1.5000, 1.5000, 1.5000], #&gt; [1.5000, 1.5000, 1.5000, 1.5000], #&gt; [1.5000, 1.5000, 1.5000, 1.5000]]) # scalar multiplying two tensors s * (x + y) #&gt; tensor([[1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.]]) 6.9 Multiplying tensors \\[A * B = B * A\\] message(&quot;x&quot;) #&gt; x (x = torch$ones(5L, 4L)) message(&quot;y&quot;) #&gt; y (y = torch$ones(5L, 4L)) message(&quot;2x+4y&quot;) #&gt; 2x+4y (z = 2 * x + 4 * y) #&gt; tensor([[1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.]]) #&gt; tensor([[1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.]]) #&gt; tensor([[6., 6., 6., 6.], #&gt; [6., 6., 6., 6.], #&gt; [6., 6., 6., 6.], #&gt; [6., 6., 6., 6.], #&gt; [6., 6., 6., 6.]]) x * y == y * x #&gt; tensor([[True, True, True, True], #&gt; [True, True, True, True], #&gt; [True, True, True, True], #&gt; [True, True, True, True], #&gt; [True, True, True, True]]) 6.10 Dot product \\[dot(a,b)_{i,j,k,a,b,c} = \\sum_m a_{i,j,k,m}b_{a,b,m,c}\\] torch$dot(torch$tensor(c(2, 3)), torch$tensor(c(2, 1))) #&gt; tensor(7.) 6.10.1 Dot product of 2D array using Python import numpy as np a = np.array([[1, 2], [3, 4]]) b = np.array([[1, 2], [3, 4]]) print(a) #&gt; [[1 2] #&gt; [3 4]] print(b) #&gt; [[1 2] #&gt; [3 4]] np.dot(a, b) #&gt; array([[ 7, 10], #&gt; [15, 22]]) 6.10.2 Dot product of 2D array using R a &lt;- np$array(list(list(1, 2), list(3, 4))) a b &lt;- np$array(list(list(1, 2), list(3, 4))) b np$dot(a, b) #&gt; [,1] [,2] #&gt; [1,] 1 2 #&gt; [2,] 3 4 #&gt; [,1] [,2] #&gt; [1,] 1 2 #&gt; [2,] 3 4 #&gt; [,1] [,2] #&gt; [1,] 7 10 #&gt; [2,] 15 22 torch.dot() treats both \\(a\\) and \\(b\\) as 1D vectors (irrespective of their original shape) and computes their inner product. at &lt;- torch$as_tensor(a) bt &lt;- torch$as_tensor(b) # torch$dot(at, bt) &lt;- RuntimeError: dot: Expected 1-D argument self, but got 2-D # at %.*% bt If we perform the same dot product operation in Python, we get the same error: import torch import numpy as np a = np.array([[1, 2], [3, 4]]) a #&gt; array([[1, 2], #&gt; [3, 4]]) b = np.array([[1, 2], [3, 4]]) b #&gt; array([[1, 2], #&gt; [3, 4]]) np.dot(a, b) #&gt; array([[ 7, 10], #&gt; [15, 22]]) at = torch.as_tensor(a) bt = torch.as_tensor(b) at #&gt; tensor([[1, 2], #&gt; [3, 4]]) bt #&gt; tensor([[1, 2], #&gt; [3, 4]]) torch.dot(at, bt) #&gt; Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: 1D tensors expected, got 2D, 2D tensors at /opt/conda/conda-bld/pytorch_1595629401553/work/aten/src/TH/generic/THTensorEvenMoreMath.cpp:83 #&gt; #&gt; Detailed traceback: #&gt; File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; a &lt;- torch$Tensor(list(list(1, 2), list(3, 4))) b &lt;- torch$Tensor(c(c(1, 2), c(3, 4))) c &lt;- torch$Tensor(list(list(11, 12), list(13, 14))) a b torch$dot(a, b) #&gt; Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: 1D tensors expected, got 2D, 1D tensors at /opt/conda/conda-bld/pytorch_1595629401553/work/aten/src/TH/generic/THTensorEvenMoreMath.cpp:83 # this is another way of performing dot product in PyTorch # a$dot(a) #&gt; tensor([[1., 2.], #&gt; [3., 4.]]) #&gt; tensor([1., 2., 3., 4.]) o1 &lt;- torch$ones(2L, 2L) o2 &lt;- torch$ones(2L, 2L) o1 o2 torch$dot(o1, o2) #&gt; Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: 1D tensors expected, got 2D, 2D tensors at /opt/conda/conda-bld/pytorch_1595629401553/work/aten/src/TH/generic/THTensorEvenMoreMath.cpp:83 o1$dot(o2) #&gt; Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: 1D tensors expected, got 2D, 2D tensors at /opt/conda/conda-bld/pytorch_1595629401553/work/aten/src/TH/generic/THTensorEvenMoreMath.cpp:83 #&gt; tensor([[1., 1.], #&gt; [1., 1.]]) #&gt; tensor([[1., 1.], #&gt; [1., 1.]]) # 1D tensors work fine r = torch$dot(torch$Tensor(list(4L, 2L, 4L)), torch$Tensor(list(3L, 4L, 1L))) r #&gt; tensor(24.) 6.10.3 Dot product with mm and matmul functions So, if we cannor perform 2D tensor operations with the dot product, how do we manage then? ## mm and matmul seem to address the dot product we are looking for in tensors a = torch$randn(2L, 3L) b = torch$randn(3L, 4L) a$mm(b) a$matmul(b) #&gt; tensor([[ 0.8691, -1.5437, 0.9368, 0.1141], #&gt; [-0.9413, 2.7064, 0.9507, 0.5996]]) #&gt; tensor([[ 0.8691, -1.5437, 0.9368, 0.1141], #&gt; [-0.9413, 2.7064, 0.9507, 0.5996]]) Here is a good explanation: https://stackoverflow.com/a/44525687/5270873 Let’s now prove the associative property of tensors: \\[(A B)^T = B^T A^T\\] abt &lt;- torch$mm(a, b)$transpose(dim0=0L, dim1=1L) abt #&gt; tensor([[ 0.8691, -0.9413], #&gt; [-1.5437, 2.7064], #&gt; [ 0.9368, 0.9507], #&gt; [ 0.1141, 0.5996]]) at &lt;- a$transpose(dim0=0L, dim1=1L) bt &lt;- b$transpose(dim0=0L, dim1=1L) btat &lt;- torch$matmul(bt, at) btat #&gt; tensor([[ 0.8691, -0.9413], #&gt; [-1.5437, 2.7064], #&gt; [ 0.9368, 0.9507], #&gt; [ 0.1141, 0.5996]]) And we could unit test if the results are nearly the same with allclose(): # tolerance torch$allclose(abt, btat, rtol=0.0001) #&gt; [1] TRUE "],
["creating-pytorch-classes.html", "Chapter 7 Creating PyTorch classes 7.1 Build a PyTorch model class", " Chapter 7 Creating PyTorch classes 7.1 Build a PyTorch model class PyTorch classes cannot not directly be instantiated from R. Yet. We need an intermediate step to create a class. For this, we use reticulate functions like py_run_string() that will read the class implementation in Python code, and then assign it to an R object. 7.1.1 Example 1: a neural network with one layer py_run_string(&quot;import torch&quot;) main = py_run_string( &quot; import torch.nn as nn class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.layer = torch.nn.Linear(1, 1) def forward(self, x): x = self.layer(x) return x &quot;) # build a Linear Rgression model net &lt;- main$Net() The R object net now contains all the object in the PyTorch class Net. 7.1.2 Example 2: Logistic Regression main &lt;- py_run_string( &quot; import torch.nn as nn class LogisticRegressionModel(nn.Module): def __init__(self, input_dim, output_dim): super(LogisticRegressionModel, self).__init__() self.linear = nn.Linear(input_dim, output_dim) def forward(self, x): out = self.linear(x) return out &quot;) # build a Logistic Rgression model LogisticRegressionModel &lt;- main$LogisticRegressionModel The R object LogisticRegressionModel now contains all the objects in the PyTorch class LogisticRegressionModel. "],
["example-1-a-classification-problem-with-logistic-regression.html", "Chapter 8 Example 1: A classification problem with Logistic Regression 8.1 Code in Python", " Chapter 8 Example 1: A classification problem with Logistic Regression 8.1 Code in Python I will combine here R and Python code just to show how easy is integrating R and Python. First thing we have to do is loading the package rTorch. We do that in a chunk: library(rTorch) Then, we proceed to copy the standard Python code but in their own Python chunks. This is a very nice example that I found in the web. It explains the classic challenge of classification. When rTorch is loaded, a number of Python libraries are also loaded, which enable us the immediate use of numpy, torch and matplotlib. # Logistic Regression # https://m-alcu.github.io/blog/2018/02/10/logit-pytorch/ import numpy as np import torch import torch.nn.functional as F from torch.autograd import Variable import matplotlib.pyplot as plt The next thing we do is setting a seed to make the example repeatable, in my machine and yours. np.random.seed(2048) Then we generate some random samples. N = 100 D = 2 X = np.random.randn(N, D) * 2 ctr = int(N/2) # center the first N/2 points at (-2,-2) X[:ctr,:] = X[:ctr,:] - 2 * np.ones((ctr, D)) # center the last N/2 points at (2, 2) X[ctr:,:] = X[ctr:,:] + 2 * np.ones((ctr, D)) # labels: first N/2 are 0, last N/2 are 1 # mark the first half with 0 and the sceond half with 1 T = np.array([0] * ctr + [1] * ctr).reshape(100, 1) And plot the original data for reference. # plot the data. color the dots using T plt.scatter(X[:,0], X[:,1], c=T.reshape(N), s=100, alpha=0.5) plt.xlabel(&#39;X(1)&#39;) plt.ylabel(&#39;X(2)&#39;) What follows is the definition of the model using a neural network and train the model. We set up the model: class Model(torch.nn.Module): def __init__(self): super(Model, self).__init__() self.linear = torch.nn.Linear(2, 1) # 2 in and 1 out def forward(self, x): y_pred = torch.sigmoid(self.linear(x)) return y_pred # Our model model = Model() criterion = torch.nn.BCELoss(reduction=&#39;mean&#39;) optimizer = torch.optim.SGD(model.parameters(), lr=0.01) Train the model: x_data = Variable(torch.Tensor(X)) y_data = Variable(torch.Tensor(T)) # Training loop for epoch in range(1000): # Forward pass: Compute predicted y by passing x to the model y_pred = model(x_data) # Compute and print loss loss = criterion(y_pred, y_data) # print(epoch, loss.data[0]) # Zero gradients, perform a backward pass, and update the weights. optimizer.zero_grad() loss.backward() optimizer.step() w = list(model.parameters()) w0 = w[0].data.numpy() w1 = w[1].data.numpy() Finally, we plot the results, by tracing the line that separates two classes, 0 and 1, which are both colored in the plot. print(&quot;Final gradient descend:&quot;, w) # plot the data and separating line #&gt; Final gradient descend: [Parameter containing: #&gt; tensor([[1.0849, 1.1095]], requires_grad=True), Parameter containing: #&gt; tensor([0.2742], requires_grad=True)] plt.scatter(X[:,0], X[:,1], c=T.reshape(N), s=100, alpha=0.5) x_axis = np.linspace(-6, 6, 100) y_axis = -(w1[0] + x_axis * w0[0][0]) / w0[0][1] line_up, = plt.plot(x_axis, y_axis,&#39;r--&#39;, label=&#39;gradient descent&#39;) plt.legend(handles=[line_up]) plt.xlabel(&#39;X(1)&#39;) plt.ylabel(&#39;X(2)&#39;) plt.show() "],
["mnistdigits.html", "Chapter 9 Example 2: MNIST handwritten digits 9.1 Code in R 9.2 Code in Python", " Chapter 9 Example 2: MNIST handwritten digits 9.1 Code in R Source: https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/logistic_regression/main.py library(rTorch) nn &lt;- torch$nn transforms &lt;- torchvision$transforms torch$set_default_dtype(torch$float) 9.1.1 Hyperparameters # Hyper-parameters input_size &lt;- 784L num_classes &lt;- 10L num_epochs &lt;- 5L batch_size &lt;- 100L learning_rate &lt;- 0.001 9.1.2 Read datasets # MNIST dataset (images and labels) # IDX format local_folder &lt;- &#39;./datasets/raw_data&#39; train_dataset = torchvision$datasets$MNIST(root=local_folder, train=TRUE, transform=transforms$ToTensor(), download=TRUE) test_dataset = torchvision$datasets$MNIST(root=local_folder, train=FALSE, transform=transforms$ToTensor()) # Data loader (input pipeline). Make the datasets iteratble train_loader = torch$utils$data$DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=TRUE) test_loader = torch$utils$data$DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=FALSE) class(train_loader) length(train_loader) #&gt; [1] &quot;torch.utils.data.dataloader.DataLoader&quot; #&gt; [2] &quot;python.builtin.object&quot; #&gt; [1] 2 9.1.3 Define the model # Logistic regression model model = nn$Linear(input_size, num_classes) # Loss and optimizer # nn.CrossEntropyLoss() computes softmax internally criterion = nn$CrossEntropyLoss() optimizer = torch$optim$SGD(model$parameters(), lr=learning_rate) print(model) #&gt; Linear(in_features=784, out_features=10, bias=True) 9.1.4 Training # Train the model iter_train_loader &lt;- iterate(train_loader) total_step &lt;-length(iter_train_loader) for (epoch in 1:num_epochs) { i &lt;- 0 for (obj in iter_train_loader) { images &lt;- obj[[1]] # tensor torch.Size([64, 3, 28, 28]) labels &lt;- obj[[2]] # tensor torch.Size([64]), labels from 0 to 9 # cat(i, &quot;\\t&quot;); print(images$shape) # Reshape images to (batch_size, input_size) images &lt;- images$reshape(-1L, 28L*28L) # images &lt;- torch$as_tensor(images$reshape(-1L, 28L*28L), dtype=torch$double) # Forward pass outputs &lt;- model(images) loss &lt;- criterion(outputs, labels) # Backward and optimize optimizer$zero_grad() loss$backward() optimizer$step() if ((i+1) %% 100 == 0) { cat(sprintf(&#39;Epoch [%d/%d], Step [%d/%d], Loss: %f \\n&#39;, epoch+1, num_epochs, i+1, total_step, loss$item())) } i &lt;- i + 1 } } #&gt; Epoch [2/5], Step [100/600], Loss: 2.232754 #&gt; Epoch [2/5], Step [200/600], Loss: 2.134371 #&gt; Epoch [2/5], Step [300/600], Loss: 2.023998 #&gt; Epoch [2/5], Step [400/600], Loss: 1.952160 #&gt; Epoch [2/5], Step [500/600], Loss: 1.889332 #&gt; Epoch [2/5], Step [600/600], Loss: 1.796754 #&gt; Epoch [3/5], Step [100/600], Loss: 1.759579 #&gt; Epoch [3/5], Step [200/600], Loss: 1.688235 #&gt; Epoch [3/5], Step [300/600], Loss: 1.602870 #&gt; Epoch [3/5], Step [400/600], Loss: 1.579923 #&gt; Epoch [3/5], Step [500/600], Loss: 1.541716 #&gt; Epoch [3/5], Step [600/600], Loss: 1.466372 #&gt; Epoch [4/5], Step [100/600], Loss: 1.452733 #&gt; Epoch [4/5], Step [200/600], Loss: 1.398537 #&gt; Epoch [4/5], Step [300/600], Loss: 1.326588 #&gt; Epoch [4/5], Step [400/600], Loss: 1.341380 #&gt; Epoch [4/5], Step [500/600], Loss: 1.317852 #&gt; Epoch [4/5], Step [600/600], Loss: 1.251857 #&gt; Epoch [5/5], Step [100/600], Loss: 1.250609 #&gt; Epoch [5/5], Step [200/600], Loss: 1.204018 #&gt; Epoch [5/5], Step [300/600], Loss: 1.142893 #&gt; Epoch [5/5], Step [400/600], Loss: 1.182365 #&gt; Epoch [5/5], Step [500/600], Loss: 1.168364 #&gt; Epoch [5/5], Step [600/600], Loss: 1.104990 #&gt; Epoch [6/5], Step [100/600], Loss: 1.112000 #&gt; Epoch [6/5], Step [200/600], Loss: 1.067025 #&gt; Epoch [6/5], Step [300/600], Loss: 1.014896 #&gt; Epoch [6/5], Step [400/600], Loss: 1.070996 #&gt; Epoch [6/5], Step [500/600], Loss: 1.063139 #&gt; Epoch [6/5], Step [600/600], Loss: 0.999358 9.1.5 Prediction # Adjust weights and reset gradients iter_test_loader &lt;- iterate(test_loader) with(torch$no_grad(), { correct &lt;- 0 total &lt;- 0 for (obj in iter_test_loader) { images &lt;- obj[[1]] # tensor torch.Size([64, 3, 28, 28]) labels &lt;- obj[[2]] # tensor torch.Size([64]), labels from 0 to 9 images = images$reshape(-1L, 28L*28L) # images &lt;- torch$as_tensor(images$reshape(-1L, 28L*28L), dtype=torch$double) outputs = model(images) .predicted = torch$max(outputs$data, 1L) predicted &lt;- .predicted[1L] total = total + labels$size(0L) correct = correct + sum((predicted$numpy() == labels$numpy())) } cat(sprintf(&#39;Accuracy of the model on the 10000 test images: %f %%&#39;, (100 * correct / total))) }) #&gt; Accuracy of the model on the 10000 test images: 83.040000 % 9.1.6 Save the model # Save the model checkpoint torch$save(model$state_dict(), &#39;model.ckpt&#39;) 9.2 Code in Python import torch import torchvision import torch.nn as nn import torchvision.transforms as transforms # Hyper-parameters input_size = 784 num_classes = 10 num_epochs = 5 batch_size = 100 learning_rate = 0.001 # MNIST dataset (images and labels) # IDX format local_folder = &#39;./datasets/raw_data&#39; train_dataset = torchvision.datasets.MNIST(root=local_folder, train=True, transform=transforms.ToTensor(), download=True) test_dataset = torchvision.datasets.MNIST(root=local_folder, train=False, transform=transforms.ToTensor()) # Data loader (input pipeline). Make the datasets iteratble train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True) test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) "],
["linear-regression.html", "Chapter 10 Linear Regression 10.1 Introduction 10.2 Generate the dataset 10.3 Convert arrays to tensors 10.4 Converting from numpy to tensor 10.5 Creating the network model 10.6 Optimizer and Loss 10.7 Training 10.8 Results", " Chapter 10 Linear Regression 10.1 Introduction Source: https://www.guru99.com/pytorch-tutorial.html library(rTorch) nn &lt;- torch$nn Variable &lt;- torch$autograd$Variable invisible(torch$manual_seed(123)) 10.2 Generate the dataset Before you start the training process, you need to know our data. You make a random function to test our model. \\(Y = x3 sin(x)+ 3x+0.8 rand(100)\\) np$random$seed(123L) x = np$random$rand(100L) y = np$sin(x) * np$power(x, 3L) + 3L * x + np$random$rand(100L) * 0.8 plot(x, y) 10.3 Convert arrays to tensors Before you start the training process, you need to convert the numpy array to Variables that supported by Torch and autograd. 10.4 Converting from numpy to tensor Notice that before converting to a Torch tensor, we need first to convert the R numeric vector to a numpy array: # convert numpy array to tensor in shape of input size x &lt;- r_to_py(x) y &lt;- r_to_py(y) x = torch$from_numpy(x$reshape(-1L, 1L))$float() y = torch$from_numpy(y$reshape(-1L, 1L))$float() print(x, y) #&gt; tensor([[0.6965], #&gt; [0.2861], #&gt; [0.2269], #&gt; [0.5513], #&gt; [0.7195], #&gt; [0.4231], #&gt; [0.9808], #&gt; [0.6848], #&gt; [0.4809], #&gt; [0.3921], #&gt; [0.3432], #&gt; [0.7290], #&gt; [0.4386], #&gt; [0.0597], #&gt; [0.3980], #&gt; [0.7380], #&gt; [0.1825], #&gt; [0.1755], #&gt; [0.5316], #&gt; [0.5318], #&gt; [0.6344], #&gt; [0.8494], #&gt; [0.7245], #&gt; [0.6110], #&gt; [0.7224], #&gt; [0.3230], #&gt; [0.3618], #&gt; [0.2283], #&gt; [0.2937], #&gt; [0.6310], #&gt; [0.0921], #&gt; [0.4337], #&gt; [0.4309], #&gt; [0.4937], #&gt; [0.4258], #&gt; [0.3123], #&gt; [0.4264], #&gt; [0.8934], #&gt; [0.9442], #&gt; [0.5018], #&gt; [0.6240], #&gt; [0.1156], #&gt; [0.3173], #&gt; [0.4148], #&gt; [0.8663], #&gt; [0.2505], #&gt; [0.4830], #&gt; [0.9856], #&gt; [0.5195], #&gt; [0.6129], #&gt; [0.1206], #&gt; [0.8263], #&gt; [0.6031], #&gt; [0.5451], #&gt; [0.3428], #&gt; [0.3041], #&gt; [0.4170], #&gt; [0.6813], #&gt; [0.8755], #&gt; [0.5104], #&gt; [0.6693], #&gt; [0.5859], #&gt; [0.6249], #&gt; [0.6747], #&gt; [0.8423], #&gt; [0.0832], #&gt; [0.7637], #&gt; [0.2437], #&gt; [0.1942], #&gt; [0.5725], #&gt; [0.0957], #&gt; [0.8853], #&gt; [0.6272], #&gt; [0.7234], #&gt; [0.0161], #&gt; [0.5944], #&gt; [0.5568], #&gt; [0.1590], #&gt; [0.1531], #&gt; [0.6955], #&gt; [0.3188], #&gt; [0.6920], #&gt; [0.5544], #&gt; [0.3890], #&gt; [0.9251], #&gt; [0.8417], #&gt; [0.3574], #&gt; [0.0436], #&gt; [0.3048], #&gt; [0.3982], #&gt; [0.7050], #&gt; [0.9954], #&gt; [0.3559], #&gt; [0.7625], #&gt; [0.5932], #&gt; [0.6917], #&gt; [0.1511], #&gt; [0.3989], #&gt; [0.2409], #&gt; [0.3435]]) 10.5 Creating the network model Our network model is a simple Linear layer with an input and an output shape of one. And the network output should be like this Net( (hidden): Linear(in_features=1, out_features=1, bias=True) ) py_run_string(&quot;import torch&quot;) main = py_run_string( &quot; import torch.nn as nn class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.layer = torch.nn.Linear(1, 1) def forward(self, x): x = self.layer(x) return x &quot;) # build a Linear Rgression model net &lt;- main$Net() print(net) #&gt; Net( #&gt; (layer): Linear(in_features=1, out_features=1, bias=True) #&gt; ) 10.6 Optimizer and Loss Next, you should define the Optimizer and the Loss Function for our training process. # Define Optimizer and Loss Function optimizer &lt;- torch$optim$SGD(net$parameters(), lr=0.2) loss_func &lt;- torch$nn$MSELoss() print(optimizer) print(loss_func) #&gt; SGD ( #&gt; Parameter Group 0 #&gt; dampening: 0 #&gt; lr: 0.2 #&gt; momentum: 0 #&gt; nesterov: False #&gt; weight_decay: 0 #&gt; ) #&gt; MSELoss() 10.7 Training Now let’s start our training process. With an epoch of 250, you will iterate our data to find the best value for our hyperparameters. # x = x$type(torch$float) # make it a a FloatTensor # y = y$type(torch$float) # x &lt;- torch$as_tensor(x, dtype = torch$float) # y &lt;- torch$as_tensor(y, dtype = torch$float) inputs = Variable(x) outputs = Variable(y) # base plot plot(x$data$numpy(), y$data$numpy(), col = &quot;blue&quot;) for (i in 1:250) { prediction = net(inputs) loss = loss_func(prediction, outputs) optimizer$zero_grad() loss$backward() optimizer$step() if (i &gt; 1) break if (i %% 10 == 0) { # plot and show learning process # points(x$data$numpy(), y$data$numpy()) points(x$data$numpy(), prediction$data$numpy(), col=&quot;red&quot;) # cat(i, loss$data$numpy(), &quot;\\n&quot;) } } 10.8 Results As you can see, you successfully performed regression with a neural network. Actually, on every iteration, the red line in the plot will update and change its position to fit the data. But in this picture, you only show you the final result. "],
["rainfall-prediction-with-linear-regression.html", "Chapter 11 Rainfall prediction with Linear Regression 11.1 Training data 11.2 Convert arrays to tensors 11.3 Build the model 11.4 Generate predictions 11.5 Loss Function 11.6 Step by step process 11.7 All together: train for multiple epochs", " Chapter 11 Rainfall prediction with Linear Regression library(rTorch) Select the device: CPU or GPU invisible(torch$manual_seed(0)) device = torch$device(&#39;cpu&#39;) 11.1 Training data The training data can be represented using 2 matrices (inputs and targets), each with one row per observation, and one column per variable. # Input (temp, rainfall, humidity) inputs = np$array(list(list(73, 67, 43), list(91, 88, 64), list(87, 134, 58), list(102, 43, 37), list(69, 96, 70)), dtype=&#39;float32&#39;) # Targets (apples, oranges) targets = np$array(list(list(56, 70), list(81, 101), list(119, 133), list(22, 37), list(103, 119)), dtype=&#39;float32&#39;) 11.2 Convert arrays to tensors Before we build a model, we need to convert inputs and targets to PyTorch tensors. # Convert inputs and targets to tensors inputs = torch$from_numpy(inputs) targets = torch$from_numpy(targets) print(inputs) print(targets) #&gt; tensor([[ 73., 67., 43.], #&gt; [ 91., 88., 64.], #&gt; [ 87., 134., 58.], #&gt; [102., 43., 37.], #&gt; [ 69., 96., 70.]], dtype=torch.float64) #&gt; tensor([[ 56., 70.], #&gt; [ 81., 101.], #&gt; [119., 133.], #&gt; [ 22., 37.], #&gt; [103., 119.]], dtype=torch.float64) The weights and biases can also be represented as matrices, initialized with random values. The first row of \\(w\\) and the first element of \\(b\\) are used to predict the first target variable, i.e. yield for apples, and, similarly, the second for oranges. # random numbers for weights and biases. Then convert to double() torch$set_default_dtype(torch$double) w = torch$randn(2L, 3L, requires_grad=TRUE) #$double() b = torch$randn(2L, requires_grad=TRUE) #$double() print(w) print(b) #&gt; tensor([[ 1.5410, -0.2934, -2.1788], #&gt; [ 0.5684, -1.0845, -1.3986]], requires_grad=True) #&gt; tensor([0.4033, 0.8380], requires_grad=True) 11.3 Build the model The model is simply a function that performs a matrix multiplication of the input \\(x\\) and the weights \\(w\\) (transposed), and adds the bias \\(b\\) (replicated for each observation). model &lt;- function(x) { wt &lt;- w$t() return(torch$add(torch$mm(x, wt), b)) } 11.4 Generate predictions The matrix obtained by passing the input data to the model is a set of predictions for the target variables. # Generate predictions preds = model(inputs) print(preds) #&gt; tensor([[ -0.4516, -90.4691], #&gt; [ -24.6303, -132.3828], #&gt; [ -31.2192, -176.1530], #&gt; [ 64.3523, -39.5645], #&gt; [ -73.9524, -161.9560]], grad_fn=&lt;AddBackward0&gt;) # Compare with targets print(targets) #&gt; tensor([[ 56., 70.], #&gt; [ 81., 101.], #&gt; [119., 133.], #&gt; [ 22., 37.], #&gt; [103., 119.]]) Because we’ve started with random weights and biases, the model does not a very good job of predicting the target variables. 11.5 Loss Function We can compare the predictions with the actual targets, using the following method: Calculate the difference between the two matrices (preds and targets). Square all elements of the difference matrix to remove negative values. Calculate the average of the elements in the resulting matrix. The result is a single number, known as the mean squared error (MSE). # MSE loss mse = function(t1, t2) { diff &lt;- torch$sub(t1, t2) mul &lt;- torch$sum(torch$mul(diff, diff)) return(torch$div(mul, diff$numel())) } print(mse) #&gt; function(t1, t2) { #&gt; diff &lt;- torch$sub(t1, t2) #&gt; mul &lt;- torch$sum(torch$mul(diff, diff)) #&gt; return(torch$div(mul, diff$numel())) #&gt; } 11.6 Step by step process 11.6.1 Compute the losses # Compute loss loss = mse(preds, targets) print(loss) # 46194 # 33060.8070 #&gt; tensor(33060.8053, grad_fn=&lt;DivBackward0&gt;) The resulting number is called the loss, because it indicates how bad the model is at predicting the target variables. Lower the loss, better the model. 11.6.2 Compute Gradients With PyTorch, we can automatically compute the gradient or derivative of the loss w.r.t. to the weights and biases, because they have requires_grad set to True. # Compute gradients loss$backward() The gradients are stored in the .grad property of the respective tensors. # Gradients for weights print(w) print(w$grad) #&gt; tensor([[ 1.5410, -0.2934, -2.1788], #&gt; [ 0.5684, -1.0845, -1.3986]], requires_grad=True) #&gt; tensor([[ -6938.4351, -9674.6757, -5744.0206], #&gt; [-17408.7861, -20595.9333, -12453.4702]]) # Gradients for bias print(b) print(b$grad) #&gt; tensor([0.4033, 0.8380], requires_grad=True) #&gt; tensor([ -89.3802, -212.1051]) A key insight from calculus is that the gradient indicates the rate of change of the loss, or the slope of the loss function w.r.t. the weights and biases. If a gradient element is positive: increasing the element’s value slightly will increase the loss. decreasing the element’s value slightly will decrease the loss. If a gradient element is negative, increasing the element’s value slightly will decrease the loss. decreasing the element’s value slightly will increase the loss. The increase or decrease is proportional to the value of the gradient. 11.6.3 Reset the gradients Finally, we’ll reset the gradients to zero before moving forward, because PyTorch accumulates gradients. # Reset the gradients w$grad$zero_() b$grad$zero_() print(w$grad) print(b$grad) #&gt; tensor([[0., 0., 0.], #&gt; [0., 0., 0.]]) #&gt; tensor([0., 0.]) #&gt; tensor([[0., 0., 0.], #&gt; [0., 0., 0.]]) #&gt; tensor([0., 0.]) 11.6.3.1 Adjust weights and biases using gradient descent We’ll reduce the loss and improve our model using the gradient descent algorithm, which has the following steps: Generate predictions Calculate the loss Compute gradients w.r.t the weights and biases Adjust the weights by subtracting a small quantity proportional to the gradient Reset the gradients to zero # Generate predictions preds = model(inputs) print(preds) #&gt; tensor([[ -0.4516, -90.4691], #&gt; [ -24.6303, -132.3828], #&gt; [ -31.2192, -176.1530], #&gt; [ 64.3523, -39.5645], #&gt; [ -73.9524, -161.9560]], grad_fn=&lt;AddBackward0&gt;) # Calculate the loss loss = mse(preds, targets) print(loss) #&gt; tensor(33060.8053, grad_fn=&lt;DivBackward0&gt;) # Compute gradients loss$backward() print(w$grad) print(b$grad) #&gt; tensor([[ -6938.4351, -9674.6757, -5744.0206], #&gt; [-17408.7861, -20595.9333, -12453.4702]]) #&gt; tensor([ -89.3802, -212.1051]) # Adjust weights and reset gradients with(torch$no_grad(), { print(w); print(b) # requires_grad attribute remains w$data &lt;- torch$sub(w$data, torch$mul(w$grad$data, torch$scalar_tensor(1e-5))) b$data &lt;- torch$sub(b$data, torch$mul(b$grad$data, torch$scalar_tensor(1e-5))) print(w$grad$data$zero_()) print(b$grad$data$zero_()) }) print(w) print(b) #&gt; tensor([[ 1.5410, -0.2934, -2.1788], #&gt; [ 0.5684, -1.0845, -1.3986]], requires_grad=True) #&gt; tensor([0.4033, 0.8380], requires_grad=True) #&gt; tensor([[0., 0., 0.], #&gt; [0., 0., 0.]]) #&gt; tensor([0., 0.]) #&gt; tensor([[ 1.6104, -0.1967, -2.1213], #&gt; [ 0.7425, -0.8786, -1.2741]], requires_grad=True) #&gt; tensor([0.4042, 0.8401], requires_grad=True) With the new weights and biases, the model should have a lower loss. # Calculate loss preds = model(inputs) loss = mse(preds, targets) print(loss) #&gt; tensor(23432.4894, grad_fn=&lt;DivBackward0&gt;) 11.7 All together: train for multiple epochs To reduce the loss further, we repeat the process of adjusting the weights and biases using the gradients multiple times. Each iteration is called an epoch. # Running all together # Adjust weights and reset gradients num_epochs &lt;- 100 for (i in 1:num_epochs) { preds = model(inputs) loss = mse(preds, targets) loss$backward() with(torch$no_grad(), { w$data &lt;- torch$sub(w$data, torch$mul(w$grad, torch$scalar_tensor(1e-5))) b$data &lt;- torch$sub(b$data, torch$mul(b$grad, torch$scalar_tensor(1e-5))) w$grad$zero_() b$grad$zero_() }) } # Calculate loss preds = model(inputs) loss = mse(preds, targets) print(loss) # predictions preds # Targets targets #&gt; tensor(1258.0216, grad_fn=&lt;DivBackward0&gt;) #&gt; tensor([[ 69.2462, 80.2082], #&gt; [ 73.7183, 97.2052], #&gt; [118.5780, 124.9272], #&gt; [ 89.2282, 92.7052], #&gt; [ 47.4648, 80.7782]], grad_fn=&lt;AddBackward0&gt;) #&gt; tensor([[ 56., 70.], #&gt; [ 81., 101.], #&gt; [119., 133.], #&gt; [ 22., 37.], #&gt; [103., 119.]]) "],
["neural-networks-using-numpy-r-base-rtorch-and-pytorch.html", "Chapter 12 Neural Networks using NumPy, r-base, rTorch and PyTorch 12.1 A neural network with numpy 12.2 A neural network with r-base 12.3 The neural network written in PyTorch 12.4 A neural network written in rTorch 12.5 Exercise", " Chapter 12 Neural Networks using NumPy, r-base, rTorch and PyTorch We will compare three neural networks: a neural network written in numpy a neural network written in r-base a neural network written in PyTorch a neural network written in rTorch 12.1 A neural network with numpy We start the neural network by simply using numpy: # A simple neural network using NumPy # Code in file tensor/two_layer_net_numpy.py import time import numpy as np tic = time.process_time() np.random.seed(123) # set a seed for reproducibility # N is batch size; D_in is input dimension; # H is hidden dimension; D_out is output dimension. N, D_in, H, D_out = 64, 1000, 100, 10 # Create random input and output data x = np.random.randn(N, D_in) y = np.random.randn(N, D_out) # print(x.shape) # print(y.shape) w1 = np.random.randn(D_in, H) w2 = np.random.randn(H, D_out) # print(w1.shape) # print(w2.shape) learning_rate = 1e-6 for t in range(500): # Forward pass: compute predicted y h = x.dot(w1) # print(t, h.max()) h_relu = np.maximum(h, 0) y_pred = h_relu.dot(w2) # Compute and print loss sq = np.square(y_pred - y) loss = sq.sum() print(t, loss) # Backprop to compute gradients of w1 and w2 with respect to loss grad_y_pred = 2.0 * (y_pred - y) grad_w2 = h_relu.T.dot(grad_y_pred) grad_h_relu = grad_y_pred.dot(w2.T) grad_h = grad_h_relu.copy() grad_h[h &lt; 0] = 0 grad_w1 = x.T.dot(grad_h) # Update weights w1 -= learning_rate * grad_w1 w2 -= learning_rate * grad_w2 # processing time #&gt; 0 28624200.80093851 #&gt; 1 24402861.381040633 #&gt; 2 23157437.291475512 #&gt; 3 21617191.633971732 #&gt; 4 18598190.361558586 #&gt; 5 14198211.41969284 #&gt; 6 9786244.45261814 #&gt; 7 6233451.217340665 #&gt; 8 3862647.2678296 #&gt; 9 2412366.6327648377 #&gt; 10 1569915.4392193719 #&gt; 11 1078501.338148753 #&gt; 12 785163.9233288623 #&gt; 13 601495.2825043732 #&gt; 14 479906.0403613453 #&gt; 15 394555.1933174624 #&gt; 16 331438.6987273828 #&gt; 17 282679.6687873872 #&gt; 18 243807.84432087577 #&gt; 19 211970.1811070823 #&gt; 20 185451.68615142742 #&gt; 21 163078.20881862933 #&gt; 22 144011.80160918707 #&gt; 23 127662.96132466738 #&gt; 24 113546.29175681758 #&gt; 25 101291.55288493488 #&gt; 26 90623.2083365488 #&gt; 27 81307.3259069289 #&gt; 28 73135.24710426925 #&gt; 29 65937.50294095621 #&gt; 30 59570.264253680405 #&gt; 31 53923.82804264214 #&gt; 32 48909.69273028217 #&gt; 33 44438.89933807683 #&gt; 34 40445.340315697315 #&gt; 35 36873.300419894105 #&gt; 36 33664.990437423796 #&gt; 37 30781.198962949482 #&gt; 38 28184.24227268409 #&gt; 39 25843.997931081976 #&gt; 40 23727.282448406404 #&gt; 41 21810.062067327617 #&gt; 42 20071.326437572203 #&gt; 43 18492.63752543332 #&gt; 44 17056.72779714253 #&gt; 45 15749.299484025263 #&gt; 46 14557.324481207244 #&gt; 47 13468.469764337975 #&gt; 48 12473.575866914001 #&gt; 49 11562.485809665823 #&gt; 50 10727.865926563425 #&gt; 51 9962.4113728161 #&gt; 52 9259.619803682264 #&gt; 53 8613.269071227103 #&gt; 54 8018.523834750735 #&gt; 55 7471.080819104454 #&gt; 56 6966.0065184565265 #&gt; 57 6499.966854225839 #&gt; 58 6069.576425345407 #&gt; 59 5671.28212284083 #&gt; 60 5302.64498008629 #&gt; 61 4961.339043761746 #&gt; 62 4645.025414234535 #&gt; 63 4351.473575805076 #&gt; 64 4079.2165446062886 #&gt; 65 3826.148082088759 #&gt; 66 3590.887308956795 #&gt; 67 3372.0103280622634 #&gt; 68 3168.1734086507527 #&gt; 69 2978.3621000816793 #&gt; 70 2801.3026490979832 #&gt; 71 2636.037950790896 #&gt; 72 2481.7354010452723 #&gt; 73 2337.6093944873373 #&gt; 74 2202.825042568387 #&gt; 75 2076.8872560589457 #&gt; 76 1958.9976460120315 #&gt; 77 1848.5060338548442 #&gt; 78 1744.999338082472 #&gt; 79 1647.9807349258722 #&gt; 80 1556.9947585282348 #&gt; 81 1471.7081797400365 #&gt; 82 1391.6136870762475 #&gt; 83 1316.3329239757234 #&gt; 84 1245.5902641069808 #&gt; 85 1179.0691783286152 #&gt; 86 1116.509520952858 #&gt; 87 1057.6662051951444 #&gt; 88 1002.251968682358 #&gt; 89 950.0167505993277 #&gt; 90 900.7916929993592 #&gt; 91 854.3816389576887 #&gt; 92 810.6277767708937 #&gt; 93 769.359204134852 #&gt; 94 730.3836012940129 #&gt; 95 693.5644048073445 #&gt; 96 658.7807027999572 #&gt; 97 625.9238747325836 #&gt; 98 594.8758111694992 #&gt; 99 565.4973547949343 #&gt; 100 537.7012178149585 #&gt; 101 511.3901106843997 #&gt; 102 486.4837276215498 #&gt; 103 462.9074695545851 #&gt; 104 440.57876228874807 #&gt; 105 419.41212313923995 #&gt; 106 399.34612374956964 #&gt; 107 380.32217772728114 #&gt; 108 362.2821345456041 #&gt; 109 345.1804975712006 #&gt; 110 328.9402861597722 #&gt; 111 313.51912062711443 #&gt; 112 298.87547706727497 #&gt; 113 284.96926791620365 #&gt; 114 271.76429845268774 #&gt; 115 259.2246266311524 #&gt; 116 247.30122156532178 #&gt; 117 235.96203976771284 #&gt; 118 225.178741845231 #&gt; 119 214.92539698060864 #&gt; 120 205.1691616882672 #&gt; 121 195.8892001432394 #&gt; 122 187.052215013267 #&gt; 123 178.64288738757915 #&gt; 124 170.6347989732484 #&gt; 125 163.00806018890742 #&gt; 126 155.74401913460838 #&gt; 127 148.8335289811081 #&gt; 128 142.2496666996883 #&gt; 129 135.9750912283429 #&gt; 130 129.9898261242878 #&gt; 131 124.2841886577775 #&gt; 132 118.84482149781245 #&gt; 133 113.65645952102648 #&gt; 134 108.70543970080944 #&gt; 135 103.98144604071936 #&gt; 136 99.47512083366092 #&gt; 137 95.17318303450631 #&gt; 138 91.06775169947753 #&gt; 139 87.1495259294625 #&gt; 140 83.4075554849762 #&gt; 141 79.83335532838096 #&gt; 142 76.41993249926696 #&gt; 143 73.15953167860118 #&gt; 144 70.04535899921332 #&gt; 145 67.07000377138765 #&gt; 146 64.22536514818634 #&gt; 147 61.50715956099494 #&gt; 148 58.90970110703893 #&gt; 149 56.42818157299062 #&gt; 150 54.0534563439752 #&gt; 151 51.784098992504774 #&gt; 152 49.61304222205953 #&gt; 153 47.53708868183348 #&gt; 154 45.550739513747104 #&gt; 155 43.651385230775844 #&gt; 156 41.833382882033526 #&gt; 157 40.09449255768935 #&gt; 158 38.430465576899365 #&gt; 159 36.83773398481139 #&gt; 160 35.313368600585356 #&gt; 161 33.854369284339015 #&gt; 162 32.45799709272589 #&gt; 163 31.120973836570375 #&gt; 164 29.841057186482367 #&gt; 165 28.615366313659155 #&gt; 166 27.441646501923387 #&gt; 167 26.317677128114568 #&gt; 168 25.24106573435136 #&gt; 169 24.21056866875246 #&gt; 170 23.223366825889713 #&gt; 171 22.276914475964166 #&gt; 172 21.37056177702837 #&gt; 173 20.502013041054887 #&gt; 174 19.669605151003026 #&gt; 175 18.872156637146674 #&gt; 176 18.107932697664374 #&gt; 177 17.37534709306374 #&gt; 178 16.673297052412618 #&gt; 179 16.000313127916463 #&gt; 180 15.355056259808652 #&gt; 181 14.736642044314724 #&gt; 182 14.143657665389998 #&gt; 183 13.575482981168946 #&gt; 184 13.030557920726439 #&gt; 185 12.507813624902791 #&gt; 186 12.006508479643612 #&gt; 187 11.525873890625103 #&gt; 188 11.064924569594794 #&gt; 189 10.62284512860161 #&gt; 190 10.19922427874796 #&gt; 191 9.79248532294338 #&gt; 192 9.402215377695029 #&gt; 193 9.027996925838622 #&gt; 194 8.668895520242348 #&gt; 195 8.324385761674124 #&gt; 196 7.993908670660823 #&gt; 197 7.676665609325071 #&gt; 198 7.372299100127828 #&gt; 199 7.080233920966754 #&gt; 200 6.799940598001485 #&gt; 201 6.530984430179325 #&gt; 202 6.27288786879498 #&gt; 203 6.025197539285538 #&gt; 204 5.787473375781904 #&gt; 205 5.559253501791473 #&gt; 206 5.340172472449478 #&gt; 207 5.129896948041127 #&gt; 208 4.928007606816015 #&gt; 209 4.734225282678773 #&gt; 210 4.548186858907007 #&gt; 211 4.369651328446803 #&gt; 212 4.1982364576470985 #&gt; 213 4.03356501113859 #&gt; 214 3.8754625080282983 #&gt; 215 3.723691411552273 #&gt; 216 3.5779627242856717 #&gt; 217 3.4379821914237114 #&gt; 218 3.3035655875402 #&gt; 219 3.174454405800358 #&gt; 220 3.050474307039242 #&gt; 221 2.9313837093171027 #&gt; 222 2.8170418304756346 #&gt; 223 2.7072412196041817 #&gt; 224 2.601727700086947 #&gt; 225 2.500404091219077 #&gt; 226 2.403078781570885 #&gt; 227 2.3095944818352514 #&gt; 228 2.219794799730771 #&gt; 229 2.1335266786377156 #&gt; 230 2.050676042360689 #&gt; 231 1.9710453639291998 #&gt; 232 1.894559024311108 #&gt; 233 1.8211210547719694 #&gt; 234 1.7505340383434302 #&gt; 235 1.6826932948726598 #&gt; 236 1.6175070289512319 #&gt; 237 1.5549072300348497 #&gt; 238 1.4947316986694295 #&gt; 239 1.436912502601004 #&gt; 240 1.3813729879464858 #&gt; 241 1.32798542050412 #&gt; 242 1.2766884038686654 #&gt; 243 1.2273848146334736 #&gt; 244 1.1800217450319552 #&gt; 245 1.1344919105886015 #&gt; 246 1.0907369940976221 #&gt; 247 1.048682623569126 #&gt; 248 1.008265620640105 #&gt; 249 0.9694282665758402 #&gt; 250 0.9320976601575863 #&gt; 251 0.8962339607475193 #&gt; 252 0.861753386590558 #&gt; 253 0.8286151485835402 #&gt; 254 0.7967578289855202 #&gt; 255 0.7661404678427008 #&gt; 256 0.7367202044069405 #&gt; 257 0.7084227136677593 #&gt; 258 0.6812311487720661 #&gt; 259 0.6550822696784752 #&gt; 260 0.6299469090212146 #&gt; 261 0.6057869953554655 #&gt; 262 0.5825650778278454 #&gt; 263 0.560238214093596 #&gt; 264 0.5387735503108713 #&gt; 265 0.5181403816558078 #&gt; 266 0.49830590931288704 #&gt; 267 0.4792293730809791 #&gt; 268 0.46088901492658985 #&gt; 269 0.44325464817124605 #&gt; 270 0.4263040840612994 #&gt; 271 0.41000543380652404 #&gt; 272 0.3943367329584973 #&gt; 273 0.37927114581518934 #&gt; 274 0.36478176529467893 #&gt; 275 0.3508504444514621 #&gt; 276 0.3374578361160551 #&gt; 277 0.32457682402471333 #&gt; 278 0.31219123729926995 #&gt; 279 0.3002965861471759 #&gt; 280 0.28884848624103193 #&gt; 281 0.27783526470540487 #&gt; 282 0.2672448769700397 #&gt; 283 0.257061810692966 #&gt; 284 0.2472693951467931 #&gt; 285 0.23785306876444096 #&gt; 286 0.2287964823126848 #&gt; 287 0.2200890964310116 #&gt; 288 0.2117131852610878 #&gt; 289 0.2036578219832887 #&gt; 290 0.1959113399380713 #&gt; 291 0.18846041746510409 #&gt; 292 0.1812947700716973 #&gt; 293 0.17440531516160357 #&gt; 294 0.1677799812083831 #&gt; 295 0.16140610523823434 #&gt; 296 0.15527565017156653 #&gt; 297 0.14937904644540106 #&gt; 298 0.143707930394666 #&gt; 299 0.13825290527817413 #&gt; 300 0.13300640130437155 #&gt; 301 0.12796012311329652 #&gt; 302 0.12310750541654694 #&gt; 303 0.11844182274750006 #&gt; 304 0.11395158652039863 #&gt; 305 0.10963187686683722 #&gt; 306 0.10547640155931667 #&gt; 307 0.10148022089421871 #&gt; 308 0.09763637993288563 #&gt; 309 0.09393976586808896 #&gt; 310 0.09038186218006347 #&gt; 311 0.08696004033324446 #&gt; 312 0.08366808215680896 #&gt; 313 0.08050159133383622 #&gt; 314 0.07745565072654795 #&gt; 315 0.07452541616804072 #&gt; 316 0.07170677388799906 #&gt; 317 0.0689949238891718 #&gt; 318 0.06638632065316974 #&gt; 319 0.06387707772652872 #&gt; 320 0.06146291085118909 #&gt; 321 0.05914022943971027 #&gt; 322 0.05690662209830684 #&gt; 323 0.05475707395743522 #&gt; 324 0.052689449069882516 #&gt; 325 0.0506998454506518 #&gt; 326 0.04878568859795683 #&gt; 327 0.04694479519756138 #&gt; 328 0.04517396661890381 #&gt; 329 0.04346938274989057 #&gt; 330 0.04182932192093554 #&gt; 331 0.04025154186790747 #&gt; 332 0.038733588417535325 #&gt; 333 0.03727299017397479 #&gt; 334 0.03586799441056368 #&gt; 335 0.03451589218269463 #&gt; 336 0.03321501089196713 #&gt; 337 0.03196371785315193 #&gt; 338 0.030759357425255512 #&gt; 339 0.029600888472506824 #&gt; 340 0.028485919148247208 #&gt; 341 0.027413172250666702 #&gt; 342 0.026380963791971412 #&gt; 343 0.02538782827695241 #&gt; 344 0.024432256369794788 #&gt; 345 0.023512794719542034 #&gt; 346 0.022628153928014032 #&gt; 347 0.02177684408441299 #&gt; 348 0.02095765200802166 #&gt; 349 0.020169474661596593 #&gt; 350 0.01941096289570436 #&gt; 351 0.018681045066723634 #&gt; 352 0.017978879513485046 #&gt; 353 0.017303468563149377 #&gt; 354 0.016653437842283743 #&gt; 355 0.01602766278434931 #&gt; 356 0.015425464893053308 #&gt; 357 0.014845946789042726 #&gt; 358 0.01428824985022298 #&gt; 359 0.013751635754278416 #&gt; 360 0.013235286650491312 #&gt; 361 0.012738339025971655 #&gt; 362 0.0122601869182692 #&gt; 363 0.011799970856207088 #&gt; 364 0.011357085981177944 #&gt; 365 0.010930950268791513 #&gt; 366 0.010520842685042622 #&gt; 367 0.01012614583012008 #&gt; 368 0.009746393154866176 #&gt; 369 0.009380889339541617 #&gt; 370 0.009029161386732537 #&gt; 371 0.00869059833701307 #&gt; 372 0.008364772077009364 #&gt; 373 0.008051209390688818 #&gt; 374 0.007749432506980628 #&gt; 375 0.007459023266141868 #&gt; 376 0.007179590434342686 #&gt; 377 0.006910623445872005 #&gt; 378 0.0066517499415919105 #&gt; 379 0.006402648026668485 #&gt; 380 0.006162978285335637 #&gt; 381 0.005932194796397628 #&gt; 382 0.0057100850523133465 #&gt; 383 0.005496310244889655 #&gt; 384 0.005290628924128056 #&gt; 385 0.00509262416882405 #&gt; 386 0.004902076613035799 #&gt; 387 0.004718638851161897 #&gt; 388 0.004542078962055229 #&gt; 389 0.004372164586674141 #&gt; 390 0.0042086186268486744 #&gt; 391 0.004051226677918435 #&gt; 392 0.003899737449492815 #&gt; 393 0.003753918301517942 #&gt; 394 0.0036135618379381004 #&gt; 395 0.003478478691764194 #&gt; 396 0.0033484625756204194 #&gt; 397 0.0032233273622630336 #&gt; 398 0.003102863549092089 #&gt; 399 0.002986912218199622 #&gt; 400 0.0028753481463720115 #&gt; 401 0.0027679524720228606 #&gt; 402 0.0026645903413001857 #&gt; 403 0.002565067280110832 #&gt; 404 0.002469270189885967 #&gt; 405 0.002377067171876515 #&gt; 406 0.0022883091777458524 #&gt; 407 0.002202926988989529 #&gt; 408 0.002120737936892398 #&gt; 409 0.0020415781423083032 #&gt; 410 0.0019653808381894892 #&gt; 411 0.0018920388674690015 #&gt; 412 0.0018214489876562734 #&gt; 413 0.0017534990549491567 #&gt; 414 0.0016880979054327092 #&gt; 415 0.0016251364192797018 #&gt; 416 0.0015645343026935593 #&gt; 417 0.0015062064772054069 #&gt; 418 0.0014500530088167006 #&gt; 419 0.001395986809735361 #&gt; 420 0.0013439464213948061 #&gt; 421 0.0012938496041315591 #&gt; 422 0.0012456223977539646 #&gt; 423 0.0011992050880636534 #&gt; 424 0.0011545283489988243 #&gt; 425 0.00111150758568053 #&gt; 426 0.0010701006705381246 #&gt; 427 0.0010302364937631399 #&gt; 428 0.0009918591300854913 #&gt; 429 0.000954924393229692 #&gt; 430 0.0009193639132851613 #&gt; 431 0.000885130846793718 #&gt; 432 0.0008521777959402742 #&gt; 433 0.0008204570911806855 #&gt; 434 0.0007899223397663538 #&gt; 435 0.0007605278374248271 #&gt; 436 0.0007322343466923758 #&gt; 437 0.0007049830913988117 #&gt; 438 0.0006787512341427114 #&gt; 439 0.0006535021203749922 #&gt; 440 0.0006291921955325687 #&gt; 441 0.0006057856348304279 #&gt; 442 0.0005832525024862874 #&gt; 443 0.0005615598539463627 #&gt; 444 0.0005406761235212546 #&gt; 445 0.0005205750249308841 #&gt; 446 0.0005012184845930873 #&gt; 447 0.00048258480282314676 #&gt; 448 0.00046464475752567284 #&gt; 449 0.00044737394619139524 #&gt; 450 0.00043075137592208436 #&gt; 451 0.00041474810355465676 #&gt; 452 0.0003993358048136131 #&gt; 453 0.0003844970781217258 #&gt; 454 0.00037021092506796444 #&gt; 455 0.000356459486191319 #&gt; 456 0.00034322132236759437 #&gt; 457 0.0003304723731924654 #&gt; 458 0.00031819830164123673 #&gt; 459 0.0003063812179908789 #&gt; 460 0.00029500453534664166 #&gt; 461 0.00028405331305463857 #&gt; 462 0.00027350873727132553 #&gt; 463 0.00026335657398047466 #&gt; 464 0.0002535812583676579 #&gt; 465 0.00024416913722500146 #&gt; 466 0.0002351142689434037 #&gt; 467 0.0002263919313751717 #&gt; 468 0.0002179925767375779 #&gt; 469 0.0002099042754011281 #&gt; 470 0.00020211745069089414 #&gt; 471 0.0001946205404420093 #&gt; 472 0.00018740325426796242 #&gt; 473 0.0001804525224963492 #&gt; 474 0.00017375996054049412 #&gt; 475 0.00016731630060453707 #&gt; 476 0.00016111227107184903 #&gt; 477 0.00015513993832780905 #&gt; 478 0.00014938925941842613 #&gt; 479 0.00014385207870840213 #&gt; 480 0.00013852014130335906 #&gt; 481 0.00013338601187803598 #&gt; 482 0.00012844279329313077 #&gt; 483 0.0001236841045649647 #&gt; 484 0.00011910150087052596 #&gt; 485 0.00011468967274778241 #&gt; 486 0.0001104405800218931 #&gt; 487 0.00010634983744801036 #&gt; 488 0.00010241132940383641 #&gt; 489 9.861901302497896e-05 #&gt; 490 9.496682985463865e-05 #&gt; 491 9.144989846228939e-05 #&gt; 492 8.806354488360765e-05 #&gt; 493 8.480312707843102e-05 #&gt; 494 8.166404591760756e-05 #&gt; 495 7.864135637129015e-05 #&gt; 496 7.573027442833174e-05 #&gt; 497 7.29278760308855e-05 #&gt; 498 7.023030228292749e-05 #&gt; 499 6.763183953436163e-05 toc = time.process_time() print(toc - tic, &quot;seconds&quot;) #&gt; 4.439214920000005 seconds 12.2 A neural network with r-base It is the same algorithm above in numpy but written in R base. library(tictoc) tic() set.seed(123) N &lt;- 64; D_in &lt;- 1000; H &lt;- 100; D_out &lt;- 10; # Create random input and output data x &lt;- array(rnorm(N * D_in), dim = c(N, D_in)) y &lt;- array(rnorm(N * D_out), dim = c(N, D_out)) # Randomly initialize weights w1 &lt;- array(rnorm(D_in * H), dim = c(D_in, H)) w2 &lt;- array(rnorm(H * D_out), dim = c(H, D_out)) learning_rate &lt;- 1e-6 for (t in seq(1, 500)) { # Forward pass: compute predicted y h = x %*% w1 h_relu = pmax(h, 0) y_pred = h_relu %*% w2 # Compute and print loss sq &lt;- (y_pred - y)^2 loss = sum(sq) cat(t, loss, &quot;\\n&quot;) # Backprop to compute gradients of w1 and w2 with respect to loss grad_y_pred = 2.0 * (y_pred - y) grad_w2 = t(h_relu) %*% grad_y_pred grad_h_relu = grad_y_pred %*% t(w2) # grad_h &lt;- sapply(grad_h_relu, function(i) i, simplify = FALSE ) # grad_h = grad_h_relu.copy() grad_h &lt;- rlang::duplicate(grad_h_relu) grad_h[h &lt; 0] &lt;- 0 grad_w1 = t(x) %*% grad_h # Update weights w1 = w1 - learning_rate * grad_w1 w2 = w2 - learning_rate * grad_w2 } toc() #&gt; 1 2.8e+07 #&gt; 2 25505803 #&gt; 3 29441299 #&gt; 4 35797650 #&gt; 5 39517126 #&gt; 6 34884942 #&gt; 7 23333535 #&gt; 8 11927525 #&gt; 9 5352787 #&gt; 10 2496984 #&gt; 11 1379780 #&gt; 12 918213 #&gt; 13 695760 #&gt; 14 564974 #&gt; 15 474479 #&gt; 16 405370 #&gt; 17 349747 #&gt; 18 303724 #&gt; 19 265075 #&gt; 20 232325 #&gt; 21 204394 #&gt; 22 180414 #&gt; 23 159752 #&gt; 24 141895 #&gt; 25 126374 #&gt; 26 112820 #&gt; 27 100959 #&gt; 28 90536 #&gt; 29 81352 #&gt; 30 73244 #&gt; 31 66058 #&gt; 32 59675 #&gt; 33 53993 #&gt; 34 48921 #&gt; 35 44388 #&gt; 36 40328 #&gt; 37 36687 #&gt; 38 33414 #&gt; 39 30469 #&gt; 40 27816 #&gt; 41 25419 #&gt; 42 23251 #&gt; 43 21288 #&gt; 44 19508 #&gt; 45 17893 #&gt; 46 16426 #&gt; 47 15092 #&gt; 48 13877 #&gt; 49 12769 #&gt; 50 11758 #&gt; 51 10835 #&gt; 52 9991 #&gt; 53 9218 #&gt; 54 8510 #&gt; 55 7862 #&gt; 56 7267 #&gt; 57 6719 #&gt; 58 6217 #&gt; 59 5754 #&gt; 60 5329 #&gt; 61 4938 #&gt; 62 4577 #&gt; 63 4245 #&gt; 64 3938 #&gt; 65 3655 #&gt; 66 3394 #&gt; 67 3153 #&gt; 68 2930 #&gt; 69 2724 #&gt; 70 2533 #&gt; 71 2357 #&gt; 72 2193 #&gt; 73 2042 #&gt; 74 1902 #&gt; 75 1772 #&gt; 76 1651 #&gt; 77 1539 #&gt; 78 1435 #&gt; 79 1338 #&gt; 80 1249 #&gt; 81 1165 #&gt; 82 1088 #&gt; 83 1016 #&gt; 84 949 #&gt; 85 886 #&gt; 86 828 #&gt; 87 774 #&gt; 88 724 #&gt; 89 677 #&gt; 90 633 #&gt; 91 592 #&gt; 92 554 #&gt; 93 519 #&gt; 94 486 #&gt; 95 455 #&gt; 96 426 #&gt; 97 399 #&gt; 98 374 #&gt; 99 350 #&gt; 100 328 #&gt; 101 308 #&gt; 102 289 #&gt; 103 271 #&gt; 104 254 #&gt; 105 238 #&gt; 106 224 #&gt; 107 210 #&gt; 108 197 #&gt; 109 185 #&gt; 110 174 #&gt; 111 163 #&gt; 112 153 #&gt; 113 144 #&gt; 114 135 #&gt; 115 127 #&gt; 116 119 #&gt; 117 112 #&gt; 118 106 #&gt; 119 99.2 #&gt; 120 93.3 #&gt; 121 87.8 #&gt; 122 82.6 #&gt; 123 77.7 #&gt; 124 73.1 #&gt; 125 68.8 #&gt; 126 64.7 #&gt; 127 60.9 #&gt; 128 57.4 #&gt; 129 54 #&gt; 130 50.9 #&gt; 131 47.9 #&gt; 132 45.1 #&gt; 133 42.5 #&gt; 134 40.1 #&gt; 135 37.8 #&gt; 136 35.6 #&gt; 137 33.5 #&gt; 138 31.6 #&gt; 139 29.8 #&gt; 140 28.1 #&gt; 141 26.5 #&gt; 142 25 #&gt; 143 23.6 #&gt; 144 22.2 #&gt; 145 21 #&gt; 146 19.8 #&gt; 147 18.7 #&gt; 148 17.6 #&gt; 149 16.6 #&gt; 150 15.7 #&gt; 151 14.8 #&gt; 152 14 #&gt; 153 13.2 #&gt; 154 12.5 #&gt; 155 11.8 #&gt; 156 11.1 #&gt; 157 10.5 #&gt; 158 9.94 #&gt; 159 9.39 #&gt; 160 8.87 #&gt; 161 8.38 #&gt; 162 7.92 #&gt; 163 7.49 #&gt; 164 7.08 #&gt; 165 6.69 #&gt; 166 6.32 #&gt; 167 5.98 #&gt; 168 5.65 #&gt; 169 5.35 #&gt; 170 5.06 #&gt; 171 4.78 #&gt; 172 4.52 #&gt; 173 4.28 #&gt; 174 4.05 #&gt; 175 3.83 #&gt; 176 3.62 #&gt; 177 3.43 #&gt; 178 3.25 #&gt; 179 3.07 #&gt; 180 2.91 #&gt; 181 2.75 #&gt; 182 2.6 #&gt; 183 2.47 #&gt; 184 2.33 #&gt; 185 2.21 #&gt; 186 2.09 #&gt; 187 1.98 #&gt; 188 1.88 #&gt; 189 1.78 #&gt; 190 1.68 #&gt; 191 1.6 #&gt; 192 1.51 #&gt; 193 1.43 #&gt; 194 1.36 #&gt; 195 1.29 #&gt; 196 1.22 #&gt; 197 1.15 #&gt; 198 1.09 #&gt; 199 1.04 #&gt; 200 0.983 #&gt; 201 0.932 #&gt; 202 0.883 #&gt; 203 0.837 #&gt; 204 0.794 #&gt; 205 0.753 #&gt; 206 0.714 #&gt; 207 0.677 #&gt; 208 0.642 #&gt; 209 0.609 #&gt; 210 0.577 #&gt; 211 0.548 #&gt; 212 0.519 #&gt; 213 0.493 #&gt; 214 0.467 #&gt; 215 0.443 #&gt; 216 0.421 #&gt; 217 0.399 #&gt; 218 0.379 #&gt; 219 0.359 #&gt; 220 0.341 #&gt; 221 0.324 #&gt; 222 0.307 #&gt; 223 0.292 #&gt; 224 0.277 #&gt; 225 0.263 #&gt; 226 0.249 #&gt; 227 0.237 #&gt; 228 0.225 #&gt; 229 0.213 #&gt; 230 0.203 #&gt; 231 0.192 #&gt; 232 0.183 #&gt; 233 0.173 #&gt; 234 0.165 #&gt; 235 0.156 #&gt; 236 0.149 #&gt; 237 0.141 #&gt; 238 0.134 #&gt; 239 0.127 #&gt; 240 0.121 #&gt; 241 0.115 #&gt; 242 0.109 #&gt; 243 0.104 #&gt; 244 0.0985 #&gt; 245 0.0936 #&gt; 246 0.0889 #&gt; 247 0.0845 #&gt; 248 0.0803 #&gt; 249 0.0763 #&gt; 250 0.0725 #&gt; 251 0.0689 #&gt; 252 0.0655 #&gt; 253 0.0623 #&gt; 254 0.0592 #&gt; 255 0.0563 #&gt; 256 0.0535 #&gt; 257 0.0508 #&gt; 258 0.0483 #&gt; 259 0.0459 #&gt; 260 0.0437 #&gt; 261 0.0415 #&gt; 262 0.0395 #&gt; 263 0.0375 #&gt; 264 0.0357 #&gt; 265 0.0339 #&gt; 266 0.0323 #&gt; 267 0.0307 #&gt; 268 0.0292 #&gt; 269 0.0278 #&gt; 270 0.0264 #&gt; 271 0.0251 #&gt; 272 0.0239 #&gt; 273 0.0227 #&gt; 274 0.0216 #&gt; 275 0.0206 #&gt; 276 0.0196 #&gt; 277 0.0186 #&gt; 278 0.0177 #&gt; 279 0.0168 #&gt; 280 0.016 #&gt; 281 0.0152 #&gt; 282 0.0145 #&gt; 283 0.0138 #&gt; 284 0.0131 #&gt; 285 0.0125 #&gt; 286 0.0119 #&gt; 287 0.0113 #&gt; 288 0.0108 #&gt; 289 0.0102 #&gt; 290 0.00975 #&gt; 291 0.00927 #&gt; 292 0.00883 #&gt; 293 0.0084 #&gt; 294 0.008 #&gt; 295 0.00761 #&gt; 296 0.00724 #&gt; 297 0.0069 #&gt; 298 0.00656 #&gt; 299 0.00625 #&gt; 300 0.00595 #&gt; 301 0.00566 #&gt; 302 0.00539 #&gt; 303 0.00513 #&gt; 304 0.00489 #&gt; 305 0.00465 #&gt; 306 0.00443 #&gt; 307 0.00422 #&gt; 308 0.00401 #&gt; 309 0.00382 #&gt; 310 0.00364 #&gt; 311 0.00347 #&gt; 312 0.0033 #&gt; 313 0.00314 #&gt; 314 0.00299 #&gt; 315 0.00285 #&gt; 316 0.00271 #&gt; 317 0.00259 #&gt; 318 0.00246 #&gt; 319 0.00234 #&gt; 320 0.00223 #&gt; 321 0.00213 #&gt; 322 0.00203 #&gt; 323 0.00193 #&gt; 324 0.00184 #&gt; 325 0.00175 #&gt; 326 0.00167 #&gt; 327 0.00159 #&gt; 328 0.00151 #&gt; 329 0.00144 #&gt; 330 0.00137 #&gt; 331 0.00131 #&gt; 332 0.00125 #&gt; 333 0.00119 #&gt; 334 0.00113 #&gt; 335 0.00108 #&gt; 336 0.00103 #&gt; 337 0.000979 #&gt; 338 0.000932 #&gt; 339 0.000888 #&gt; 340 0.000846 #&gt; 341 0.000807 #&gt; 342 0.000768 #&gt; 343 0.000732 #&gt; 344 0.000698 #&gt; 345 0.000665 #&gt; 346 0.000634 #&gt; 347 0.000604 #&gt; 348 0.000575 #&gt; 349 0.000548 #&gt; 350 0.000523 #&gt; 351 0.000498 #&gt; 352 0.000475 #&gt; 353 0.000452 #&gt; 354 0.000431 #&gt; 355 0.000411 #&gt; 356 0.000392 #&gt; 357 0.000373 #&gt; 358 0.000356 #&gt; 359 0.000339 #&gt; 360 0.000323 #&gt; 361 0.000308 #&gt; 362 0.000294 #&gt; 363 0.00028 #&gt; 364 0.000267 #&gt; 365 0.000254 #&gt; 366 0.000243 #&gt; 367 0.000231 #&gt; 368 0.00022 #&gt; 369 0.00021 #&gt; 370 2e-04 #&gt; 371 0.000191 #&gt; 372 0.000182 #&gt; 373 0.000174 #&gt; 374 0.000165 #&gt; 375 0.000158 #&gt; 376 0.00015 #&gt; 377 0.000143 #&gt; 378 0.000137 #&gt; 379 0.00013 #&gt; 380 0.000124 #&gt; 381 0.000119 #&gt; 382 0.000113 #&gt; 383 0.000108 #&gt; 384 0.000103 #&gt; 385 9.8e-05 #&gt; 386 9.34e-05 #&gt; 387 8.91e-05 #&gt; 388 8.49e-05 #&gt; 389 8.1e-05 #&gt; 390 7.72e-05 #&gt; 391 7.37e-05 #&gt; 392 7.02e-05 #&gt; 393 6.7e-05 #&gt; 394 6.39e-05 #&gt; 395 6.09e-05 #&gt; 396 5.81e-05 #&gt; 397 5.54e-05 #&gt; 398 5.28e-05 #&gt; 399 5.04e-05 #&gt; 400 4.81e-05 #&gt; 401 4.58e-05 #&gt; 402 4.37e-05 #&gt; 403 4.17e-05 #&gt; 404 3.98e-05 #&gt; 405 3.79e-05 #&gt; 406 3.62e-05 #&gt; 407 3.45e-05 #&gt; 408 3.29e-05 #&gt; 409 3.14e-05 #&gt; 410 2.99e-05 #&gt; 411 2.86e-05 #&gt; 412 2.72e-05 #&gt; 413 2.6e-05 #&gt; 414 2.48e-05 #&gt; 415 2.36e-05 #&gt; 416 2.25e-05 #&gt; 417 2.15e-05 #&gt; 418 2.05e-05 #&gt; 419 1.96e-05 #&gt; 420 1.87e-05 #&gt; 421 1.78e-05 #&gt; 422 1.7e-05 #&gt; 423 1.62e-05 #&gt; 424 1.55e-05 #&gt; 425 1.48e-05 #&gt; 426 1.41e-05 #&gt; 427 1.34e-05 #&gt; 428 1.28e-05 #&gt; 429 1.22e-05 #&gt; 430 1.17e-05 #&gt; 431 1.11e-05 #&gt; 432 1.06e-05 #&gt; 433 1.01e-05 #&gt; 434 9.66e-06 #&gt; 435 9.22e-06 #&gt; 436 8.79e-06 #&gt; 437 8.39e-06 #&gt; 438 8e-06 #&gt; 439 7.64e-06 #&gt; 440 7.29e-06 #&gt; 441 6.95e-06 #&gt; 442 6.63e-06 #&gt; 443 6.33e-06 #&gt; 444 6.04e-06 #&gt; 445 5.76e-06 #&gt; 446 5.5e-06 #&gt; 447 5.25e-06 #&gt; 448 5.01e-06 #&gt; 449 4.78e-06 #&gt; 450 4.56e-06 #&gt; 451 4.35e-06 #&gt; 452 4.15e-06 #&gt; 453 3.96e-06 #&gt; 454 3.78e-06 #&gt; 455 3.61e-06 #&gt; 456 3.44e-06 #&gt; 457 3.28e-06 #&gt; 458 3.13e-06 #&gt; 459 2.99e-06 #&gt; 460 2.85e-06 #&gt; 461 2.72e-06 #&gt; 462 2.6e-06 #&gt; 463 2.48e-06 #&gt; 464 2.37e-06 #&gt; 465 2.26e-06 #&gt; 466 2.15e-06 #&gt; 467 2.06e-06 #&gt; 468 1.96e-06 #&gt; 469 1.87e-06 #&gt; 470 1.79e-06 #&gt; 471 1.71e-06 #&gt; 472 1.63e-06 #&gt; 473 1.55e-06 #&gt; 474 1.48e-06 #&gt; 475 1.42e-06 #&gt; 476 1.35e-06 #&gt; 477 1.29e-06 #&gt; 478 1.23e-06 #&gt; 479 1.17e-06 #&gt; 480 1.12e-06 #&gt; 481 1.07e-06 #&gt; 482 1.02e-06 #&gt; 483 9.74e-07 #&gt; 484 9.3e-07 #&gt; 485 8.88e-07 #&gt; 486 8.47e-07 #&gt; 487 8.09e-07 #&gt; 488 7.72e-07 #&gt; 489 7.37e-07 #&gt; 490 7.03e-07 #&gt; 491 6.71e-07 #&gt; 492 6.41e-07 #&gt; 493 6.12e-07 #&gt; 494 5.84e-07 #&gt; 495 5.57e-07 #&gt; 496 5.32e-07 #&gt; 497 5.08e-07 #&gt; 498 4.85e-07 #&gt; 499 4.63e-07 #&gt; 500 4.42e-07 #&gt; 4.668 sec elapsed 12.3 The neural network written in PyTorch Here is the same example we have used above but written in PyTorch. Notice the following differences with the numpy code: we select the computation device which could be cpu or gpu when building or creating the tensors, we specify which device we want to use the tensors have torch methods and properties. Example: mm(), clamp(), sum(), clone(), and t(), also notice the use some torch functions: device(), randn() reticulate::use_condaenv(&quot;r-torch&quot;) # Code in file tensor/two_layer_net_tensor.py import torch import time ms = torch.manual_seed(0) tic = time.process_time() device = torch.device(&#39;cpu&#39;) # device = torch.device(&#39;cuda&#39;) # Uncomment this to run on GPU # N is batch size; D_in is input dimension; # H is hidden dimension; D_out is output dimension. N, D_in, H, D_out = 64, 1000, 100, 10 # Create random input and output data x = torch.randn(N, D_in, device=device) y = torch.randn(N, D_out, device=device) # Randomly initialize weights w1 = torch.randn(D_in, H, device=device) w2 = torch.randn(H, D_out, device=device) learning_rate = 1e-6 for t in range(500): # Forward pass: compute predicted y h = x.mm(w1) h_relu = h.clamp(min=0) y_pred = h_relu.mm(w2) # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor # of shape (); we can get its value as a Python number with loss.item(). loss = (y_pred - y).pow(2).sum() print(t, loss.item()) # Backprop to compute gradients of w1 and w2 with respect to loss grad_y_pred = 2.0 * (y_pred - y) grad_w2 = h_relu.t().mm(grad_y_pred) grad_h_relu = grad_y_pred.mm(w2.t()) grad_h = grad_h_relu.clone() grad_h[h &lt; 0] = 0 grad_w1 = x.t().mm(grad_h) # Update weights using gradient descent w1 -= learning_rate * grad_w1 w2 -= learning_rate * grad_w2 #&gt; 0 28304951.678202875 #&gt; 1 24927328.6639303 #&gt; 2 28155855.77058386 #&gt; 3 34180357.719430685 #&gt; 4 37707746.887789354 #&gt; 5 33620991.028499454 #&gt; 6 22446553.70772245 #&gt; 7 11618280.121770754 #&gt; 8 5223431.478905056 #&gt; 9 2461507.5224659974 #&gt; 10 1379540.558178643 #&gt; 11 932839.5714197141 #&gt; 12 715554.0194977624 #&gt; 13 585999.3830106166 #&gt; 14 495224.1190599696 #&gt; 15 425139.8269296169 #&gt; 16 368213.26971499936 #&gt; 17 320861.36850887095 #&gt; 18 280855.4664459439 #&gt; 19 246786.8849917458 #&gt; 20 217605.04415180004 #&gt; 21 192469.10582625505 #&gt; 22 170743.82020579863 #&gt; 23 151851.94095434336 #&gt; 24 135381.09682470877 #&gt; 25 120989.87510382346 #&gt; 26 108377.08193979261 #&gt; 27 97286.46466266317 #&gt; 28 87516.74262182202 #&gt; 29 78871.40974253973 #&gt; 30 71194.55619418155 #&gt; 31 64368.69806881405 #&gt; 32 58290.88589673794 #&gt; 33 52862.53039132413 #&gt; 34 48006.48628965374 #&gt; 35 43648.17638706032 #&gt; 36 39733.12183144018 #&gt; 37 36212.83667739256 #&gt; 38 33041.3423372085 #&gt; 39 30180.249173055217 #&gt; 40 27596.031029894235 #&gt; 41 25257.71896952727 #&gt; 42 23140.477484791758 #&gt; 43 21219.215332345837 #&gt; 44 19477.346011970214 #&gt; 45 17898.942492450115 #&gt; 46 16461.186144096366 #&gt; 47 15150.468945112118 #&gt; 48 13954.32764792479 #&gt; 49 12862.049952206942 #&gt; 50 11863.846542866391 #&gt; 51 10950.190923462585 #&gt; 52 10112.94426786356 #&gt; 53 9346.057705583586 #&gt; 54 8643.227456215845 #&gt; 55 7998.013709383486 #&gt; 56 7404.314143395906 #&gt; 57 6858.340342363264 #&gt; 58 6355.823822117182 #&gt; 59 5893.272718725863 #&gt; 60 5467.590824799911 #&gt; 61 5075.314900617862 #&gt; 62 4713.257357968152 #&gt; 63 4378.928521878623 #&gt; 64 4070.0899099030976 #&gt; 65 3784.681538189922 #&gt; 66 3520.732192065661 #&gt; 67 3276.4374575280553 #&gt; 68 3050.1573582667443 #&gt; 69 2840.7435683049735 #&gt; 70 2646.8554702835545 #&gt; 71 2467.1301658974303 #&gt; 72 2300.493116013281 #&gt; 73 2145.7384163982897 #&gt; 74 2002.0645278499069 #&gt; 75 1868.62585232437 #&gt; 76 1744.5305134660546 #&gt; 77 1629.2088434191014 #&gt; 78 1521.9212637440608 #&gt; 79 1422.070202881318 #&gt; 80 1329.1314050876533 #&gt; 81 1242.57843915715 #&gt; 82 1161.9741527800966 #&gt; 83 1086.859274942149 #&gt; 84 1016.8590222123014 #&gt; 85 951.566250754007 #&gt; 86 890.6602544816294 #&gt; 87 833.8443278601629 #&gt; 88 780.8578032641242 #&gt; 89 731.3973374911332 #&gt; 90 685.1947512043134 #&gt; 91 642.044298154148 #&gt; 92 601.7328625418818 #&gt; 93 564.0571957786619 #&gt; 94 528.8261654125919 #&gt; 95 495.9025536938795 #&gt; 96 465.09740183895497 #&gt; 97 436.2913070884209 #&gt; 98 409.31979498319464 #&gt; 99 384.0735885326187 #&gt; 100 360.44638041822657 #&gt; 101 338.3392483462631 #&gt; 102 317.6202138550293 #&gt; 103 298.2113941729458 #&gt; 104 280.034373801912 #&gt; 105 263.0044766552596 #&gt; 106 247.03986887883838 #&gt; 107 232.07821803150009 #&gt; 108 218.048818247877 #&gt; 109 204.90061200354214 #&gt; 110 192.56394398444112 #&gt; 111 180.98923443492257 #&gt; 112 170.13203285049397 #&gt; 113 159.95050694576628 #&gt; 114 150.38972485782446 #&gt; 115 141.41734198411572 #&gt; 116 132.99611497367866 #&gt; 117 125.09156325963338 #&gt; 118 117.66642940606687 #&gt; 119 110.69533918173776 #&gt; 120 104.14845396396463 #&gt; 121 97.99893007959557 #&gt; 122 92.22011750510285 #&gt; 123 86.78965563408303 #&gt; 124 81.68832953028851 #&gt; 125 76.8936948513737 #&gt; 126 72.38701330673442 #&gt; 127 68.15098341186042 #&gt; 128 64.17155344502598 #&gt; 129 60.428038476921586 #&gt; 130 56.908227350924264 #&gt; 131 53.597359299554086 #&gt; 132 50.48291263509982 #&gt; 133 47.55356853186795 #&gt; 134 44.79743700983829 #&gt; 135 42.20495473902311 #&gt; 136 39.76511807450793 #&gt; 137 37.468859040581314 #&gt; 138 35.308453101613594 #&gt; 139 33.27467715145929 #&gt; 140 31.360220767968155 #&gt; 141 29.558058573195954 #&gt; 142 27.861685433714385 #&gt; 143 26.26412411616196 #&gt; 144 24.759623750310396 #&gt; 145 23.343125632673978 #&gt; 146 22.008981468973293 #&gt; 147 20.752466464076754 #&gt; 148 19.568797416671696 #&gt; 149 18.453977382984238 #&gt; 150 17.4033728523491 #&gt; 151 16.413591679613166 #&gt; 152 15.481311105283973 #&gt; 153 14.602503969149222 #&gt; 154 13.774233452363154 #&gt; 155 12.994002407101597 #&gt; 156 12.25856122789346 #&gt; 157 11.56544379208444 #&gt; 158 10.912019785853294 #&gt; 159 10.295986095133893 #&gt; 160 9.715282302954558 #&gt; 161 9.16779373332011 #&gt; 162 8.651771772979203 #&gt; 163 8.16494247053697 #&gt; 164 7.705943656616575 #&gt; 165 7.273075881962532 #&gt; 166 6.864819666904757 #&gt; 167 6.479820163401722 #&gt; 168 6.116708158449551 #&gt; 169 5.774100177816865 #&gt; 170 5.450995917449771 #&gt; 171 5.1461795385705855 #&gt; 172 4.858643025681388 #&gt; 173 4.587382709817182 #&gt; 174 4.3313872873690915 #&gt; 175 4.089901424925121 #&gt; 176 3.862103131723745 #&gt; 177 3.6470527511395345 #&gt; 178 3.444106712594519 #&gt; 179 3.252609836897004 #&gt; 180 3.07181676586878 #&gt; 181 2.901218541067528 #&gt; 182 2.7402092504329087 #&gt; 183 2.588219680628511 #&gt; 184 2.444769343935288 #&gt; 185 2.3093519385929073 #&gt; 186 2.1815040103726817 #&gt; 187 2.0608004075821134 #&gt; 188 1.9468628778712722 #&gt; 189 1.8395089185876619 #&gt; 190 1.7383087355430398 #&gt; 191 1.6427212247232674 #&gt; 192 1.5524241674870793 #&gt; 193 1.467167641479254 #&gt; 194 1.3866326949062429 #&gt; 195 1.3105816336367393 #&gt; 196 1.2387287987377498 #&gt; 197 1.1708405590526374 #&gt; 198 1.1067184254730398 #&gt; 199 1.0461214720168739 #&gt; 200 0.9888821187836597 #&gt; 201 0.9348112964047595 #&gt; 202 0.8837227088870757 #&gt; 203 0.8354674137424224 #&gt; 204 0.7898553794335115 #&gt; 205 0.7467719311431791 #&gt; 206 0.7060574954970198 #&gt; 207 0.6675658111377627 #&gt; 208 0.6311950860163134 #&gt; 209 0.5968226032996442 #&gt; 210 0.5643375968524703 #&gt; 211 0.533635574131998 #&gt; 212 0.5046100881587827 #&gt; 213 0.477185660092641 #&gt; 214 0.4512577894894808 #&gt; 215 0.42675382897272557 #&gt; 216 0.4035914490049111 #&gt; 217 0.3816938006385138 #&gt; 218 0.36099433662345515 #&gt; 219 0.34142901569774986 #&gt; 220 0.3229336644019144 #&gt; 221 0.30544089540020086 #&gt; 222 0.28890489048687534 #&gt; 223 0.2732695786637561 #&gt; 224 0.25848651426787905 #&gt; 225 0.2445098453883533 #&gt; 226 0.2312929773275953 #&gt; 227 0.21879670108271798 #&gt; 228 0.20697931842620834 #&gt; 229 0.195806713788599 #&gt; 230 0.18523834142720977 #&gt; 231 0.17524520436756333 #&gt; 232 0.1657943113708178 #&gt; 233 0.15685667535616374 #&gt; 234 0.14840827333326329 #&gt; 235 0.1404142723758336 #&gt; 236 0.13285399914584078 #&gt; 237 0.12570202269005357 #&gt; 238 0.1189386653565856 #&gt; 239 0.11254012548433291 #&gt; 240 0.1064897371596009 #&gt; 241 0.10076465672097966 #&gt; 242 0.09535057382492915 #&gt; 243 0.09022846810146344 #&gt; 244 0.0853831482366543 #&gt; 245 0.08079988857826755 #&gt; 246 0.07646497849045485 #&gt; 247 0.07236264442671389 #&gt; 248 0.06848434833109891 #&gt; 249 0.06481281290915701 #&gt; 250 0.061339266016385195 #&gt; 251 0.05805307662477996 #&gt; 252 0.0549438470297849 #&gt; 253 0.05200185854871002 #&gt; 254 0.04921917115136563 #&gt; 255 0.046585968334898406 #&gt; 256 0.04409498516073983 #&gt; 257 0.041737229429376375 #&gt; 258 0.039506460963111716 #&gt; 259 0.03739526392873394 #&gt; 260 0.03539767598323861 #&gt; 261 0.033507149403411894 #&gt; 262 0.03171914800559517 #&gt; 263 0.030026565963995385 #&gt; 264 0.028424539700389058 #&gt; 265 0.026908264596902605 #&gt; 266 0.025473451972246715 #&gt; 267 0.024115391447909144 #&gt; 268 0.022829917524529335 #&gt; 269 0.021613534126580944 #&gt; 270 0.020462113245879446 #&gt; 271 0.019372541709334515 #&gt; 272 0.018341010110627966 #&gt; 273 0.017364750002383487 #&gt; 274 0.016440801352077355 #&gt; 275 0.01556635157094091 #&gt; 276 0.014738758943593583 #&gt; 277 0.013955031983018732 #&gt; 278 0.0132131375273473 #&gt; 279 0.012510963462682914 #&gt; 280 0.011846186595046146 #&gt; 281 0.011216909862565686 #&gt; 282 0.010621170683656261 #&gt; 283 0.010057232252595813 #&gt; 284 0.009523402267302118 #&gt; 285 0.00901796344305956 #&gt; 286 0.008539560661121416 #&gt; 287 0.00808664139518218 #&gt; 288 0.007657798604422466 #&gt; 289 0.007251985398671241 #&gt; 290 0.006867552434240349 #&gt; 291 0.006503615024208403 #&gt; 292 0.0061589977274497335 #&gt; 293 0.005832728877824728 #&gt; 294 0.00552383330657353 #&gt; 295 0.005231316887344834 #&gt; 296 0.004954423112991046 #&gt; 297 0.004692217253589943 #&gt; 298 0.004443918325610545 #&gt; 299 0.004208844585195957 #&gt; 300 0.003986240553018342 #&gt; 301 0.003775486718927447 #&gt; 302 0.0035759236905801945 #&gt; 303 0.003386943755826484 #&gt; 304 0.0032079271944684923 #&gt; 305 0.0030384316988295917 #&gt; 306 0.0028779268787492195 #&gt; 307 0.002725934467665546 #&gt; 308 0.002581984589507745 #&gt; 309 0.0024456583936272873 #&gt; 310 0.0023165757211508816 #&gt; 311 0.0021943198339197862 #&gt; 312 0.0020785379932883805 #&gt; 313 0.0019688908467591876 #&gt; 314 0.001865039138166403 #&gt; 315 0.0017667028380198072 #&gt; 316 0.001673582502883412 #&gt; 317 0.001585367679173598 #&gt; 318 0.001501810539526399 #&gt; 319 0.001422673117716745 #&gt; 320 0.0013477143467884333 #&gt; 321 0.0012767229593053158 #&gt; 322 0.0012094776044852774 #&gt; 323 0.0011457905013111825 #&gt; 324 0.0010854694552161603 #&gt; 325 0.0010283296937639874 #&gt; 326 0.0009742096258547116 #&gt; 327 0.000922945854411098 #&gt; 328 0.0008743961838104937 #&gt; 329 0.000828422228956311 #&gt; 330 0.0007848532048364633 #&gt; 331 0.0007435814161682407 #&gt; 332 0.0007044863185208374 #&gt; 333 0.0006674517964915639 #&gt; 334 0.0006323708525695086 #&gt; 335 0.0005991396001984025 #&gt; 336 0.0005676591273796805 #&gt; 337 0.0005378410068842657 #&gt; 338 0.000509593206205131 #&gt; 339 0.00048283262452890964 #&gt; 340 0.0004574801372963568 #&gt; 341 0.00043346425597078986 #&gt; 342 0.0004107167685464537 #&gt; 343 0.00038916698112475027 #&gt; 344 0.00036874509105280215 #&gt; 345 0.0003493980023397482 #&gt; 346 0.00033106827511510654 #&gt; 347 0.00031370294820848413 #&gt; 348 0.00029725119213414107 #&gt; 349 0.00028166684242299266 #&gt; 350 0.0002669014349461206 #&gt; 351 0.0002529104551692411 #&gt; 352 0.0002396549909150608 #&gt; 353 0.00022709499915641242 #&gt; 354 0.0002151964743005534 #&gt; 355 0.0002039240941797398 #&gt; 356 0.00019324565890046152 #&gt; 357 0.0001831235294622124 #&gt; 358 0.00017353441806004343 #&gt; 359 0.0001644493172112498 #&gt; 360 0.00015584082439274125 #&gt; 361 0.00014768295498887574 #&gt; 362 0.00013995332436295415 #&gt; 363 0.00013262955726596432 #&gt; 364 0.00012569091305008096 #&gt; 365 0.00011911540230945581 #&gt; 366 0.00011288450617749258 #&gt; 367 0.00010698023609061709 #&gt; 368 0.00010138656406183746 #&gt; 369 9.608703941253804e-05 #&gt; 370 9.106416188787175e-05 #&gt; 371 8.630363500639326e-05 #&gt; 372 8.179250816284079e-05 #&gt; 373 7.75177155755295e-05 #&gt; 374 7.346701689449564e-05 #&gt; 375 6.962838417570068e-05 #&gt; 376 6.599115070747433e-05 #&gt; 377 6.254418117212432e-05 #&gt; 378 5.927752171205165e-05 #&gt; 379 5.6181730723921865e-05 #&gt; 380 5.3248296817560075e-05 #&gt; 381 5.0468440039186904e-05 #&gt; 382 4.783531311318069e-05 #&gt; 383 4.533824051135289e-05 #&gt; 384 4.2971925902375094e-05 #&gt; 385 4.072931820046649e-05 #&gt; 386 3.860419712035776e-05 #&gt; 387 3.658998409536165e-05 #&gt; 388 3.468116364031449e-05 #&gt; 389 3.287222611892173e-05 #&gt; 390 3.115784475157776e-05 #&gt; 391 2.9533143774768434e-05 #&gt; 392 2.7993239385118317e-05 #&gt; 393 2.6533713457293835e-05 #&gt; 394 2.515050370686264e-05 #&gt; 395 2.384005448398065e-05 #&gt; 396 2.259773576968608e-05 #&gt; 397 2.1420010467094908e-05 #&gt; 398 2.0303827964836834e-05 #&gt; 399 1.9245876019466693e-05 #&gt; 400 1.8243200207082624e-05 #&gt; 401 1.7292859010059725e-05 #&gt; 402 1.6392242334282593e-05 #&gt; 403 1.5538607874558062e-05 #&gt; 404 1.4729443663403283e-05 #&gt; 405 1.396245233615671e-05 #&gt; 406 1.3235478072004572e-05 #&gt; 407 1.2546412926492304e-05 #&gt; 408 1.1893604229102106e-05 #&gt; 409 1.1274605651207216e-05 #&gt; 410 1.0687845492609113e-05 #&gt; 411 1.0131680171617125e-05 #&gt; 412 9.604526090125228e-06 #&gt; 413 9.104866882652663e-06 #&gt; 414 8.631198697407043e-06 #&gt; 415 8.182218044143847e-06 #&gt; 416 7.75664115622133e-06 #&gt; 417 7.353270255677984e-06 #&gt; 418 6.970852594884578e-06 #&gt; 419 6.608387207051329e-06 #&gt; 420 6.26478439097928e-06 #&gt; 421 5.939231288672959e-06 #&gt; 422 5.630510532150695e-06 #&gt; 423 5.337868490845549e-06 #&gt; 424 5.060468769124625e-06 #&gt; 425 4.797479196554172e-06 #&gt; 426 4.548175419581931e-06 #&gt; 427 4.3118425691448e-06 #&gt; 428 4.0878168952303456e-06 #&gt; 429 3.875456427743628e-06 #&gt; 430 3.6741748596127134e-06 #&gt; 431 3.4833269248291047e-06 #&gt; 432 3.302413300508785e-06 #&gt; 433 3.1309131720164374e-06 #&gt; 434 2.9684202192256292e-06 #&gt; 435 2.8143063954514532e-06 #&gt; 436 2.6681942942812437e-06 #&gt; 437 2.529678830678848e-06 #&gt; 438 2.3983594748975304e-06 #&gt; 439 2.2738738446053443e-06 #&gt; 440 2.155852990893322e-06 #&gt; 441 2.0439734146668315e-06 #&gt; 442 1.9379149561299024e-06 #&gt; 443 1.8373813024806672e-06 #&gt; 444 1.7420525193107436e-06 #&gt; 445 1.651681837934868e-06 #&gt; 446 1.5660026986343254e-06 #&gt; 447 1.4848126887738326e-06 #&gt; 448 1.407805243604439e-06 #&gt; 449 1.3347904750171164e-06 #&gt; 450 1.265571380938484e-06 #&gt; 451 1.1999412748704718e-06 #&gt; 452 1.137722153780124e-06 #&gt; 453 1.0787354005070172e-06 #&gt; 454 1.0228096436038175e-06 #&gt; 455 9.697877295536134e-07 #&gt; 456 9.195232359617768e-07 #&gt; 457 8.718667277682245e-07 #&gt; 458 8.266807141735369e-07 #&gt; 459 7.838400421318883e-07 #&gt; 460 7.432422670610245e-07 #&gt; 461 7.047295293241073e-07 #&gt; 462 6.682172780720125e-07 #&gt; 463 6.335979168913036e-07 #&gt; 464 6.007739196907603e-07 #&gt; 465 5.696536281572727e-07 #&gt; 466 5.401483768129584e-07 #&gt; 467 5.121740258956327e-07 #&gt; 468 4.856503635693801e-07 #&gt; 469 4.604994834612916e-07 #&gt; 470 4.3665309177253875e-07 #&gt; 471 4.140445324903026e-07 #&gt; 472 3.92606762471423e-07 #&gt; 473 3.722903378943339e-07 #&gt; 474 3.530199059473835e-07 #&gt; 475 3.347453713024692e-07 #&gt; 476 3.174178528193289e-07 #&gt; 477 3.0098863903561565e-07 #&gt; 478 2.8541218481059226e-07 #&gt; 479 2.7064186642557194e-07 #&gt; 480 2.5663718610336863e-07 #&gt; 481 2.4335703458243967e-07 #&gt; 482 2.3076549087628935e-07 #&gt; 483 2.1882618344371512e-07 #&gt; 484 2.0750489949470836e-07 #&gt; 485 1.967709863796408e-07 #&gt; 486 1.8659788188413924e-07 #&gt; 487 1.7694719372113647e-07 #&gt; 488 1.6779487766772397e-07 #&gt; 489 1.591173463100117e-07 #&gt; 490 1.5088941982411513e-07 #&gt; 491 1.4308626189302695e-07 #&gt; 492 1.3568760330487115e-07 #&gt; 493 1.286714248189776e-07 #&gt; 494 1.2201888487984934e-07 #&gt; 495 1.1571047960904828e-07 #&gt; 496 1.0972832075560864e-07 #&gt; 497 1.0405612689057184e-07 #&gt; 498 9.867753044394926e-08 #&gt; 499 9.357999190790877e-08 toc = time.process_time() print(toc - tic, &quot;seconds&quot;) #&gt; 9.014092823999995 seconds 12.4 A neural network written in rTorch The example shows the long and manual way of calculating the forward and backward passes but using rTorch. The objective is getting familiarized with the rTorch tensor operations. The following example was converted from PyTorch to rTorch to show differences and similarities of both approaches. The original source can be found here: Source. 12.4.1 Load the libraries library(rTorch) library(ggplot2) device = torch$device(&#39;cpu&#39;) # device = torch.device(&#39;cuda&#39;) # Uncomment this to run on GPU invisible(torch$manual_seed(0)) N is batch size; D_in is input dimension; H is hidden dimension; D_out is output dimension. 12.4.2 Dataset We will create a random dataset for a two layer neural network. N &lt;- 64L; D_in &lt;- 1000L; H &lt;- 100L; D_out &lt;- 10L # Create random Tensors to hold inputs and outputs x &lt;- torch$randn(N, D_in, device=device) y &lt;- torch$randn(N, D_out, device=device) # dimensions of both tensors dim(x) dim(y) #&gt; [1] 64 1000 #&gt; [1] 64 10 12.4.3 Initialize the weights # Randomly initialize weights w1 &lt;- torch$randn(D_in, H, device=device) # layer 1 w2 &lt;- torch$randn(H, D_out, device=device) # layer 2 dim(w1) dim(w2) #&gt; [1] 1000 100 #&gt; [1] 100 10 12.4.4 Iterate through the dataset Now, we are going to train our neural network on the training dataset. The equestion is: “how many times do we have to expose the training data to the algorithm?” By looking at the graph of the loss we may get an idea when we should stop. 12.4.4.1 Iterate 50 times Let’s say that for the sake of time we select to run only 50 iterations of the loop doing the training. learning_rate = 1e-6 # loop for (t in 1:50) { # Forward pass: compute predicted y, y_pred h &lt;- x$mm(w1) # matrix multiplication, x*w1 h_relu &lt;- h$clamp(min=0) # make elements greater than zero y_pred &lt;- h_relu$mm(w2) # matrix multiplication, h_relu*w2 # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor # of shape (); we can get its value as a Python number with loss.item(). loss &lt;- (torch$sub(y_pred, y))$pow(2)$sum() # sum((y_pred-y)^2) # cat(t, &quot;\\t&quot;) # cat(loss$item(), &quot;\\n&quot;) # Backprop to compute gradients of w1 and w2 with respect to loss grad_y_pred &lt;- torch$mul(torch$scalar_tensor(2.0), torch$sub(y_pred, y)) grad_w2 &lt;- h_relu$t()$mm(grad_y_pred) # compute gradient of w2 grad_h_relu &lt;- grad_y_pred$mm(w2$t()) grad_h &lt;- grad_h_relu$clone() mask &lt;- grad_h$lt(0) # filter values lower than zero torch$masked_select(grad_h, mask)$fill_(0.0) # make them equal to zero grad_w1 &lt;- x$t()$mm(grad_h) # compute gradient of w1 # Update weights using gradient descent w1 &lt;- torch$sub(w1, torch$mul(learning_rate, grad_w1)) w2 &lt;- torch$sub(w2, torch$mul(learning_rate, grad_w2)) } # y vs predicted y df_50 &lt;- data.frame(y = y$flatten()$numpy(), y_pred = y_pred$flatten()$numpy(), iter = 50) ggplot(df_50, aes(x = y, y = y_pred)) + geom_point() We see a lot of dispersion between the predicted values, \\(y_{pred}\\) and the real values, \\(y\\). We are far from our goal. Let’s take a look at the dataframe: library(&#39;DT&#39;) datatable(df_50, options = list(pageLength = 10)) 12.4.4.2 A function to train the neural network Now, we convert the script above to a function, so we could reuse it several times. We want to study the effect of the iteration on the performance of the algorithm. This time we create a function train to input the number of iterations that we want to run: train &lt;- function(iterations) { # Randomly initialize weights w1 &lt;- torch$randn(D_in, H, device=device) # layer 1 w2 &lt;- torch$randn(H, D_out, device=device) # layer 2 learning_rate = 1e-6 # loop for (t in 1:iterations) { # Forward pass: compute predicted y h &lt;- x$mm(w1) h_relu &lt;- h$clamp(min=0) y_pred &lt;- h_relu$mm(w2) # Compute and print loss; loss is a scalar stored in a PyTorch Tensor # of shape (); we can get its value as a Python number with loss.item(). loss &lt;- (torch$sub(y_pred, y))$pow(2)$sum() # cat(t, &quot;\\t&quot;); cat(loss$item(), &quot;\\n&quot;) # Backprop to compute gradients of w1 and w2 with respect to loss grad_y_pred &lt;- torch$mul(torch$scalar_tensor(2.0), torch$sub(y_pred, y)) grad_w2 &lt;- h_relu$t()$mm(grad_y_pred) grad_h_relu &lt;- grad_y_pred$mm(w2$t()) grad_h &lt;- grad_h_relu$clone() mask &lt;- grad_h$lt(0) torch$masked_select(grad_h, mask)$fill_(0.0) grad_w1 &lt;- x$t()$mm(grad_h) # Update weights using gradient descent w1 &lt;- torch$sub(w1, torch$mul(learning_rate, grad_w1)) w2 &lt;- torch$sub(w2, torch$mul(learning_rate, grad_w2)) } data.frame(y = y$flatten()$numpy(), y_pred = y_pred$flatten()$numpy(), iter = iterations) } 12.4.4.3 Run it at 100 iterations # retrieve the results and store them in a dataframe df_100 &lt;- train(iterations = 100) datatable(df_100, options = list(pageLength = 10)) # plot ggplot(df_100, aes(x = y_pred, y = y)) + geom_point() 12.4.4.4 250 iterations Still there are differences between the value and the prediction. Let’s try with more iterations, like 250: df_250 &lt;- train(iterations = 200) datatable(df_250, options = list(pageLength = 25)) # plot ggplot(df_250, aes(x = y_pred, y = y)) + geom_point() We see the formation of a line between the values and prediction, which means we are getting closer at finding the right algorithm, in this particular case, weights and bias. 12.4.4.5 500 iterations Let’s try one more time with 500 iterations: df_500 &lt;- train(iterations = 500) datatable(df_500, options = list(pageLength = 25)) ggplot(df_500, aes(x = y_pred, y = y)) + geom_point() ### Complete code for neural network in rTorch library(rTorch) library(ggplot2) library(tictoc) tic() device = torch$device(&#39;cpu&#39;) # device = torch.device(&#39;cuda&#39;) # Uncomment this to run on GPU invisible(torch$manual_seed(0)) # Properties of tensors and neural network N &lt;- 64L; D_in &lt;- 1000L; H &lt;- 100L; D_out &lt;- 10L # Create random Tensors to hold inputs and outputs x &lt;- torch$randn(N, D_in, device=device) y &lt;- torch$randn(N, D_out, device=device) # dimensions of both tensors # initialize the weights w1 &lt;- torch$randn(D_in, H, device=device) # layer 1 w2 &lt;- torch$randn(H, D_out, device=device) # layer 2 learning_rate = 1e-6 # loop for (t in 1:500) { # Forward pass: compute predicted y, y_pred h &lt;- x$mm(w1) # matrix multiplication, x*w1 h_relu &lt;- h$clamp(min=0) # make elements greater than zero y_pred &lt;- h_relu$mm(w2) # matrix multiplication, h_relu*w2 # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor # of shape (); we can get its value as a Python number with loss.item(). loss &lt;- (torch$sub(y_pred, y))$pow(2)$sum() # sum((y_pred-y)^2) # cat(t, &quot;\\t&quot;) # cat(loss$item(), &quot;\\n&quot;) # Backprop to compute gradients of w1 and w2 with respect to loss grad_y_pred &lt;- torch$mul(torch$scalar_tensor(2.0), torch$sub(y_pred, y)) grad_w2 &lt;- h_relu$t()$mm(grad_y_pred) # compute gradient of w2 grad_h_relu &lt;- grad_y_pred$mm(w2$t()) grad_h &lt;- grad_h_relu$clone() mask &lt;- grad_h$lt(0) # filter values lower than zero torch$masked_select(grad_h, mask)$fill_(0.0) # make them equal to zero grad_w1 &lt;- x$t()$mm(grad_h) # compute gradient of w1 # Update weights using gradient descent w1 &lt;- torch$sub(w1, torch$mul(learning_rate, grad_w1)) w2 &lt;- torch$sub(w2, torch$mul(learning_rate, grad_w2)) } # y vs predicted y df&lt;- data.frame(y = y$flatten()$numpy(), y_pred = y_pred$flatten()$numpy(), iter = 500) datatable(df, options = list(pageLength = 25)) ggplot(df, aes(x = y_pred, y = y)) + geom_point() toc() #&gt; 11.503 sec elapsed 12.5 Exercise Rewrite the code in rTorch but including and plotting the loss at each iteration On the neural network written in PyTorch, code, instead of printing a long table, print the table by pages that we could navigate using vertical and horizontal bars. Tip: read the PyThon data structure from R and plot it with ggplot2 "],
["a-step-by-step-neural-network-in-rtorch.html", "Chapter 13 A step-by-step neural network in rTorch 13.1 Introduction 13.2 Select device 13.3 Create the dataset 13.4 Define the model 13.5 The Loss function 13.6 Iterate through dataset 13.7 Using R generics to simplify tensor operations 13.8 A more elegant way of writing the neural network", " Chapter 13 A step-by-step neural network in rTorch 13.1 Introduction Source: https://github.com/jcjohnson/pytorch-examples#pytorch-nn In this example we use the torch nn package to implement our two-layer network: 13.2 Select device library(rTorch) device = torch$device(&#39;cpu&#39;) # device = torch.device(&#39;cuda&#39;) # Uncomment this to run on GPU N is batch size; D_in is input dimension; H is hidden dimension; D_out is output dimension. 13.3 Create the dataset invisible(torch$manual_seed(0)) # do not show the generator output N &lt;- 64L; D_in &lt;- 1000L; H &lt;- 100L; D_out &lt;- 10L # Create random Tensors to hold inputs and outputs x = torch$randn(N, D_in, device=device) y = torch$randn(N, D_out, device=device) 13.4 Define the model We use the nn package to define our model as a sequence of layers. nn.Sequential is a Module which contains other modules, and applies them in sequence to produce an output. Each Linear Module computes the output by using a linear function, and holds internal tensors for its weights and biases. After constructing the model we use the .to() method to move it to the desired device, which could be CPU or GPU. Remember that we selected CPU with torch$device('cpu'). model &lt;- torch$nn$Sequential( torch$nn$Linear(D_in, H), # first layer torch$nn$ReLU(), torch$nn$Linear(H, D_out))$to(device) # output layer print(model) #&gt; Sequential( #&gt; (0): Linear(in_features=1000, out_features=100, bias=True) #&gt; (1): ReLU() #&gt; (2): Linear(in_features=100, out_features=10, bias=True) #&gt; ) 13.5 The Loss function The nn package also contains definitions of several loss functions; in this case we will use Mean Squared Error (\\(MSE\\)) as our loss function. Setting reduction='sum' means that we are computing the sum of squared errors rather than the mean; this is for consistency with the examples above where we manually compute the loss, but in practice it is more common to use the mean squared error as a loss by setting reduction='elementwise_mean'. loss_fn = torch$nn$MSELoss(reduction = &#39;sum&#39;) 13.6 Iterate through dataset learning_rate = 1e-4 for (t in 1:500) { # Forward pass: compute predicted y by passing x to the model. Module objects # override the __call__ operator so you can call them like functions. When # doing so you pass a Tensor of input data to the Module and it produces # a Tensor of output data. y_pred = model(x) # Compute and print loss. We pass Tensors containing the predicted and true # values of y, and the loss function returns a Tensor containing the loss. loss = loss_fn(y_pred, y) cat(t, &quot;\\t&quot;) cat(loss$item(), &quot;\\n&quot;) # Zero the gradients before running the backward pass. model$zero_grad() # Backward pass: compute gradient of the loss with respect to all the learnable # parameters of the model. Internally, the parameters of each Module are stored # in Tensors with requires_grad=True, so this call will compute gradients for # all learnable parameters in the model. loss$backward() # Update the weights using gradient descent. Each parameter is a Tensor, so # we can access its data and gradients like we did before. with(torch$no_grad(), { for (param in iterate(model$parameters())) { # in Python this code is much simpler. In R we have to do some conversions # param$data &lt;- torch$sub(param$data, # torch$mul(param$grad$float(), # torch$scalar_tensor(learning_rate))) param$data &lt;- param$data - param$grad * learning_rate } }) } #&gt; 1 708 #&gt; 2 658 #&gt; 3 613 #&gt; 4 575 #&gt; 5 540 #&gt; 6 508 #&gt; 7 479 #&gt; 8 452 #&gt; 9 428 #&gt; 10 404 #&gt; 11 383 #&gt; 12 362 #&gt; 13 343 #&gt; 14 325 #&gt; 15 307 #&gt; 16 291 #&gt; 17 275 #&gt; 18 261 #&gt; 19 247 #&gt; 20 233 #&gt; 21 220 #&gt; 22 208 #&gt; 23 196 #&gt; 24 185 #&gt; 25 174 #&gt; 26 164 #&gt; 27 155 #&gt; 28 146 #&gt; 29 137 #&gt; 30 129 #&gt; 31 121 #&gt; 32 114 #&gt; 33 107 #&gt; 34 100 #&gt; 35 94.1 #&gt; 36 88.4 #&gt; 37 83 #&gt; 38 77.9 #&gt; 39 73.2 #&gt; 40 68.7 #&gt; 41 64.5 #&gt; 42 60.6 #&gt; 43 57 #&gt; 44 53.5 #&gt; 45 50.3 #&gt; 46 47.3 #&gt; 47 44.5 #&gt; 48 41.8 #&gt; 49 39.3 #&gt; 50 37 #&gt; 51 34.8 #&gt; 52 32.8 #&gt; 53 30.8 #&gt; 54 29 #&gt; 55 27.4 #&gt; 56 25.8 #&gt; 57 24.3 #&gt; 58 22.9 #&gt; 59 21.6 #&gt; 60 20.4 #&gt; 61 19.2 #&gt; 62 18.1 #&gt; 63 17.1 #&gt; 64 16.2 #&gt; 65 15.3 #&gt; 66 14.4 #&gt; 67 13.6 #&gt; 68 12.9 #&gt; 69 12.2 #&gt; 70 11.5 #&gt; 71 10.9 #&gt; 72 10.3 #&gt; 73 9.79 #&gt; 74 9.27 #&gt; 75 8.79 #&gt; 76 8.32 #&gt; 77 7.89 #&gt; 78 7.48 #&gt; 79 7.09 #&gt; 80 6.73 #&gt; 81 6.39 #&gt; 82 6.06 #&gt; 83 5.75 #&gt; 84 5.47 #&gt; 85 5.19 #&gt; 86 4.93 #&gt; 87 4.69 #&gt; 88 4.46 #&gt; 89 4.24 #&gt; 90 4.03 #&gt; 91 3.83 #&gt; 92 3.65 #&gt; 93 3.47 #&gt; 94 3.3 #&gt; 95 3.14 #&gt; 96 2.99 #&gt; 97 2.85 #&gt; 98 2.72 #&gt; 99 2.59 #&gt; 100 2.47 #&gt; 101 2.35 #&gt; 102 2.24 #&gt; 103 2.14 #&gt; 104 2.04 #&gt; 105 1.95 #&gt; 106 1.86 #&gt; 107 1.77 #&gt; 108 1.69 #&gt; 109 1.62 #&gt; 110 1.54 #&gt; 111 1.48 #&gt; 112 1.41 #&gt; 113 1.35 #&gt; 114 1.29 #&gt; 115 1.23 #&gt; 116 1.18 #&gt; 117 1.12 #&gt; 118 1.08 #&gt; 119 1.03 #&gt; 120 0.985 #&gt; 121 0.942 #&gt; 122 0.902 #&gt; 123 0.863 #&gt; 124 0.826 #&gt; 125 0.791 #&gt; 126 0.758 #&gt; 127 0.726 #&gt; 128 0.695 #&gt; 129 0.666 #&gt; 130 0.638 #&gt; 131 0.612 #&gt; 132 0.586 #&gt; 133 0.562 #&gt; 134 0.539 #&gt; 135 0.517 #&gt; 136 0.496 #&gt; 137 0.476 #&gt; 138 0.457 #&gt; 139 0.438 #&gt; 140 0.421 #&gt; 141 0.404 #&gt; 142 0.388 #&gt; 143 0.373 #&gt; 144 0.358 #&gt; 145 0.344 #&gt; 146 0.33 #&gt; 147 0.318 #&gt; 148 0.305 #&gt; 149 0.293 #&gt; 150 0.282 #&gt; 151 0.271 #&gt; 152 0.261 #&gt; 153 0.251 #&gt; 154 0.241 #&gt; 155 0.232 #&gt; 156 0.223 #&gt; 157 0.215 #&gt; 158 0.207 #&gt; 159 0.199 #&gt; 160 0.192 #&gt; 161 0.185 #&gt; 162 0.178 #&gt; 163 0.171 #&gt; 164 0.165 #&gt; 165 0.159 #&gt; 166 0.153 #&gt; 167 0.147 #&gt; 168 0.142 #&gt; 169 0.137 #&gt; 170 0.132 #&gt; 171 0.127 #&gt; 172 0.123 #&gt; 173 0.118 #&gt; 174 0.114 #&gt; 175 0.11 #&gt; 176 0.106 #&gt; 177 0.102 #&gt; 178 0.0987 #&gt; 179 0.0952 #&gt; 180 0.0918 #&gt; 181 0.0886 #&gt; 182 0.0855 #&gt; 183 0.0825 #&gt; 184 0.0797 #&gt; 185 0.0769 #&gt; 186 0.0742 #&gt; 187 0.0717 #&gt; 188 0.0692 #&gt; 189 0.0668 #&gt; 190 0.0645 #&gt; 191 0.0623 #&gt; 192 0.0602 #&gt; 193 0.0582 #&gt; 194 0.0562 #&gt; 195 0.0543 #&gt; 196 0.0525 #&gt; 197 0.0507 #&gt; 198 0.049 #&gt; 199 0.0473 #&gt; 200 0.0458 #&gt; 201 0.0442 #&gt; 202 0.0428 #&gt; 203 0.0413 #&gt; 204 0.04 #&gt; 205 0.0386 #&gt; 206 0.0374 #&gt; 207 0.0361 #&gt; 208 0.0349 #&gt; 209 0.0338 #&gt; 210 0.0327 #&gt; 211 0.0316 #&gt; 212 0.0306 #&gt; 213 0.0296 #&gt; 214 0.0286 #&gt; 215 0.0277 #&gt; 216 0.0268 #&gt; 217 0.0259 #&gt; 218 0.0251 #&gt; 219 0.0243 #&gt; 220 0.0235 #&gt; 221 0.0228 #&gt; 222 0.022 #&gt; 223 0.0213 #&gt; 224 0.0206 #&gt; 225 0.02 #&gt; 226 0.0193 #&gt; 227 0.0187 #&gt; 228 0.0181 #&gt; 229 0.0176 #&gt; 230 0.017 #&gt; 231 0.0165 #&gt; 232 0.016 #&gt; 233 0.0155 #&gt; 234 0.015 #&gt; 235 0.0145 #&gt; 236 0.014 #&gt; 237 0.0136 #&gt; 238 0.0132 #&gt; 239 0.0128 #&gt; 240 0.0124 #&gt; 241 0.012 #&gt; 242 0.0116 #&gt; 243 0.0113 #&gt; 244 0.0109 #&gt; 245 0.0106 #&gt; 246 0.0102 #&gt; 247 0.00993 #&gt; 248 0.00963 #&gt; 249 0.00933 #&gt; 250 0.00905 #&gt; 251 0.00877 #&gt; 252 0.0085 #&gt; 253 0.00824 #&gt; 254 0.00799 #&gt; 255 0.00775 #&gt; 256 0.00751 #&gt; 257 0.00728 #&gt; 258 0.00706 #&gt; 259 0.00685 #&gt; 260 0.00664 #&gt; 261 0.00644 #&gt; 262 0.00625 #&gt; 263 0.00606 #&gt; 264 0.00588 #&gt; 265 0.0057 #&gt; 266 0.00553 #&gt; 267 0.00536 #&gt; 268 0.0052 #&gt; 269 0.00505 #&gt; 270 0.00489 #&gt; 271 0.00475 #&gt; 272 0.00461 #&gt; 273 0.00447 #&gt; 274 0.00434 #&gt; 275 0.00421 #&gt; 276 0.00408 #&gt; 277 0.00396 #&gt; 278 0.00384 #&gt; 279 0.00373 #&gt; 280 0.00362 #&gt; 281 0.00351 #&gt; 282 0.00341 #&gt; 283 0.00331 #&gt; 284 0.00321 #&gt; 285 0.00311 #&gt; 286 0.00302 #&gt; 287 0.00293 #&gt; 288 0.00285 #&gt; 289 0.00276 #&gt; 290 0.00268 #&gt; 291 0.0026 #&gt; 292 0.00253 #&gt; 293 0.00245 #&gt; 294 0.00238 #&gt; 295 0.00231 #&gt; 296 0.00224 #&gt; 297 0.00218 #&gt; 298 0.00212 #&gt; 299 0.00205 #&gt; 300 0.00199 #&gt; 301 0.00194 #&gt; 302 0.00188 #&gt; 303 0.00183 #&gt; 304 0.00177 #&gt; 305 0.00172 #&gt; 306 0.00167 #&gt; 307 0.00162 #&gt; 308 0.00158 #&gt; 309 0.00153 #&gt; 310 0.00149 #&gt; 311 0.00145 #&gt; 312 0.0014 #&gt; 313 0.00136 #&gt; 314 0.00132 #&gt; 315 0.00129 #&gt; 316 0.00125 #&gt; 317 0.00121 #&gt; 318 0.00118 #&gt; 319 0.00115 #&gt; 320 0.00111 #&gt; 321 0.00108 #&gt; 322 0.00105 #&gt; 323 0.00102 #&gt; 324 0.000992 #&gt; 325 0.000964 #&gt; 326 0.000936 #&gt; 327 0.00091 #&gt; 328 0.000884 #&gt; 329 0.000859 #&gt; 330 0.000834 #&gt; 331 0.000811 #&gt; 332 0.000788 #&gt; 333 0.000766 #&gt; 334 0.000744 #&gt; 335 0.000723 #&gt; 336 0.000702 #&gt; 337 0.000683 #&gt; 338 0.000663 #&gt; 339 0.000645 #&gt; 340 0.000626 #&gt; 341 0.000609 #&gt; 342 0.000592 #&gt; 343 0.000575 #&gt; 344 0.000559 #&gt; 345 0.000543 #&gt; 346 0.000528 #&gt; 347 0.000513 #&gt; 348 0.000499 #&gt; 349 0.000485 #&gt; 350 0.000471 #&gt; 351 0.000458 #&gt; 352 0.000445 #&gt; 353 0.000433 #&gt; 354 0.000421 #&gt; 355 0.000409 #&gt; 356 0.000397 #&gt; 357 0.000386 #&gt; 358 0.000375 #&gt; 359 0.000365 #&gt; 360 0.000355 #&gt; 361 0.000345 #&gt; 362 0.000335 #&gt; 363 0.000326 #&gt; 364 0.000317 #&gt; 365 0.000308 #&gt; 366 0.000299 #&gt; 367 0.000291 #&gt; 368 0.000283 #&gt; 369 0.000275 #&gt; 370 0.000268 #&gt; 371 0.00026 #&gt; 372 0.000253 #&gt; 373 0.000246 #&gt; 374 0.000239 #&gt; 375 0.000232 #&gt; 376 0.000226 #&gt; 377 0.00022 #&gt; 378 0.000214 #&gt; 379 0.000208 #&gt; 380 0.000202 #&gt; 381 0.000196 #&gt; 382 0.000191 #&gt; 383 0.000186 #&gt; 384 0.000181 #&gt; 385 0.000176 #&gt; 386 0.000171 #&gt; 387 0.000166 #&gt; 388 0.000161 #&gt; 389 0.000157 #&gt; 390 0.000153 #&gt; 391 0.000148 #&gt; 392 0.000144 #&gt; 393 0.00014 #&gt; 394 0.000136 #&gt; 395 0.000133 #&gt; 396 0.000129 #&gt; 397 0.000125 #&gt; 398 0.000122 #&gt; 399 0.000119 #&gt; 400 0.000115 #&gt; 401 0.000112 #&gt; 402 0.000109 #&gt; 403 0.000106 #&gt; 404 0.000103 #&gt; 405 1e-04 #&gt; 406 9.77e-05 #&gt; 407 9.5e-05 #&gt; 408 9.24e-05 #&gt; 409 8.98e-05 #&gt; 410 8.74e-05 #&gt; 411 8.5e-05 #&gt; 412 8.26e-05 #&gt; 413 8.04e-05 #&gt; 414 7.82e-05 #&gt; 415 7.6e-05 #&gt; 416 7.4e-05 #&gt; 417 7.19e-05 #&gt; 418 7e-05 #&gt; 419 6.81e-05 #&gt; 420 6.62e-05 #&gt; 421 6.44e-05 #&gt; 422 6.26e-05 #&gt; 423 6.09e-05 #&gt; 424 5.92e-05 #&gt; 425 5.76e-05 #&gt; 426 5.61e-05 #&gt; 427 5.45e-05 #&gt; 428 5.3e-05 #&gt; 429 5.16e-05 #&gt; 430 5.02e-05 #&gt; 431 4.88e-05 #&gt; 432 4.75e-05 #&gt; 433 4.62e-05 #&gt; 434 4.49e-05 #&gt; 435 4.37e-05 #&gt; 436 4.25e-05 #&gt; 437 4.14e-05 #&gt; 438 4.02e-05 #&gt; 439 3.91e-05 #&gt; 440 3.81e-05 #&gt; 441 3.7e-05 #&gt; 442 3.6e-05 #&gt; 443 3.5e-05 #&gt; 444 3.41e-05 #&gt; 445 3.32e-05 #&gt; 446 3.23e-05 #&gt; 447 3.14e-05 #&gt; 448 3.05e-05 #&gt; 449 2.97e-05 #&gt; 450 2.89e-05 #&gt; 451 2.81e-05 #&gt; 452 2.73e-05 #&gt; 453 2.66e-05 #&gt; 454 2.59e-05 #&gt; 455 2.52e-05 #&gt; 456 2.45e-05 #&gt; 457 2.38e-05 #&gt; 458 2.32e-05 #&gt; 459 2.26e-05 #&gt; 460 2.19e-05 #&gt; 461 2.14e-05 #&gt; 462 2.08e-05 #&gt; 463 2.02e-05 #&gt; 464 1.97e-05 #&gt; 465 1.91e-05 #&gt; 466 1.86e-05 #&gt; 467 1.81e-05 #&gt; 468 1.76e-05 #&gt; 469 1.71e-05 #&gt; 470 1.67e-05 #&gt; 471 1.62e-05 #&gt; 472 1.58e-05 #&gt; 473 1.54e-05 #&gt; 474 1.49e-05 #&gt; 475 1.45e-05 #&gt; 476 1.41e-05 #&gt; 477 1.38e-05 #&gt; 478 1.34e-05 #&gt; 479 1.3e-05 #&gt; 480 1.27e-05 #&gt; 481 1.23e-05 #&gt; 482 1.2e-05 #&gt; 483 1.17e-05 #&gt; 484 1.14e-05 #&gt; 485 1.11e-05 #&gt; 486 1.08e-05 #&gt; 487 1.05e-05 #&gt; 488 1.02e-05 #&gt; 489 9.92e-06 #&gt; 490 9.65e-06 #&gt; 491 9.39e-06 #&gt; 492 9.14e-06 #&gt; 493 8.89e-06 #&gt; 494 8.65e-06 #&gt; 495 8.42e-06 #&gt; 496 8.19e-06 #&gt; 497 7.97e-06 #&gt; 498 7.75e-06 #&gt; 499 7.55e-06 #&gt; 500 7.34e-06 13.7 Using R generics to simplify tensor operations The following two expressions are equivalent, with the first being the long version natural way of doing it in PyTorch. The second is using the generics in R for subtraction, multiplication and scalar conversion. param$data &lt;- torch$sub(param$data, torch$mul(param$grad$float(), torch$scalar_tensor(learning_rate))) param$data &lt;- param$data - param$grad * learning_rate 13.8 A more elegant way of writing the neural network invisible(torch$manual_seed(0)) # do not show the generator output N &lt;- 64L; D_in &lt;- 1000L; H &lt;- 100L; D_out &lt;- 10L # Create random Tensors to hold inputs and outputs x = torch$randn(N, D_in, device=device) y = torch$randn(N, D_out, device=device) model &lt;- torch$nn$Sequential( torch$nn$Linear(D_in, H), # first layer torch$nn$ReLU(), # activation torch$nn$Linear(H, D_out))$to(device) # output layer loss_fn = torch$nn$MSELoss(reduction = &#39;sum&#39;) learning_rate = 1e-4 loss_row &lt;- list(vector()) for (t in 1:500) { # Forward pass: compute predicted y by passing x to the model. Module objects # override the __call__ operator so you can call them like functions. When # doing so you pass a Tensor of input data to the Module and it produces # a Tensor of output data. y_pred = model(x) # Compute and print loss. We pass Tensors containing the predicted and true # values of y, and the loss function returns a Tensor containing the loss. loss = loss_fn(y_pred, y) # (y_pred - y) is a tensor; loss_fn output is a scalar loss_row[[t]] &lt;- c(t, loss$item()) # Zero the gradients before running the backward pass. model$zero_grad() # Backward pass: compute gradient of the loss with respect to all the learnable # parameters of the model. Internally, the parameters of each module are stored # in tensors with `requires_grad=True`, so this call will compute gradients for # all learnable parameters in the model. loss$backward() # Update the weights using gradient descent. Each parameter is a tensor, so # we can access its data and gradients like we did before. with(torch$no_grad(), { for (param in iterate(model$parameters())) { # using R generics param$data &lt;- param$data - param$grad * learning_rate } }) } library(DT) loss_df &lt;- data.frame(Reduce(rbind, loss_row), row.names = NULL) names(loss_df)[1] &lt;- &quot;iter&quot; names(loss_df)[2] &lt;- &quot;loss&quot; DT::datatable(loss_df) library(ggplot2) # plot ggplot(loss_df, aes(x = iter, y = loss)) + geom_point() "],
["working-with-data-frame.html", "Chapter 14 Working with data.frame 14.1 Load PyTorch libraries 14.2 Load dataset 14.3 Summary statistics for tensors", " Chapter 14 Working with data.frame 14.1 Load PyTorch libraries library(rTorch) torch &lt;- import(&quot;torch&quot;) torchvision &lt;- import(&quot;torchvision&quot;) nn &lt;- import(&quot;torch.nn&quot;) transforms &lt;- import(&quot;torchvision.transforms&quot;) dsets &lt;- import(&quot;torchvision.datasets&quot;) builtins &lt;- import_builtins() np &lt;- import(&quot;numpy&quot;) 14.2 Load dataset # folders where the images are located train_data_path = &#39;./mnist_png_full/training/&#39; test_data_path = &#39;./mnist_png_full/testing/&#39; # read the datasets without normalization train_dataset = torchvision$datasets$ImageFolder(root = train_data_path, transform = torchvision$transforms$ToTensor() ) print(train_dataset) #&gt; Dataset ImageFolder #&gt; Number of datapoints: 60000 #&gt; Root location: ./mnist_png_full/training/ #&gt; StandardTransform #&gt; Transform: ToTensor() 14.3 Summary statistics for tensors 14.3.1 using data.frame library(tictoc) tic() fun_list &lt;- list( size = c(&quot;size&quot;), numel = c(&quot;numel&quot;), sum = c(&quot;sum&quot;, &quot;item&quot;), mean = c(&quot;mean&quot;, &quot;item&quot;), std = c(&quot;std&quot;, &quot;item&quot;), med = c(&quot;median&quot;, &quot;item&quot;), max = c(&quot;max&quot;, &quot;item&quot;), min = c(&quot;min&quot;, &quot;item&quot;) ) idx &lt;- seq(0L, 599L) # how many samples fun_get_tensor &lt;- function(x) py_get_item(train_dataset, x)[[0]] stat_fun &lt;- function(x, str_fun) { fun_var &lt;- paste0(&quot;fun_get_tensor(x)&quot;, &quot;$&quot;, str_fun, &quot;()&quot;) sapply(idx, function(x) ifelse(is.numeric(eval(parse(text = fun_var))), # size return chracater eval(parse(text = fun_var)), # all else are numeric as.character(eval(parse(text = fun_var))))) } df &lt;- data.frame(ridx = idx+1, # index number for the sample do.call(data.frame, lapply( sapply(fun_list, function(x) paste(x, collapse = &quot;()$&quot;)), function(y) stat_fun(1, y) ) ) ) Summary statistics: head(df, 20) #&gt; ridx size numel sum mean std med max min #&gt; 1 1 torch.Size([3, 28, 28]) 2352 366 0.156 0.329 0 1.000 0 #&gt; 2 2 torch.Size([3, 28, 28]) 2352 284 0.121 0.297 0 1.000 0 #&gt; 3 3 torch.Size([3, 28, 28]) 2352 645 0.274 0.420 0 1.000 0 #&gt; 4 4 torch.Size([3, 28, 28]) 2352 410 0.174 0.355 0 1.000 0 #&gt; 5 5 torch.Size([3, 28, 28]) 2352 321 0.137 0.312 0 1.000 0 #&gt; 6 6 torch.Size([3, 28, 28]) 2352 654 0.278 0.421 0 1.000 0 #&gt; 7 7 torch.Size([3, 28, 28]) 2352 496 0.211 0.374 0 1.000 0 #&gt; 8 8 torch.Size([3, 28, 28]) 2352 549 0.233 0.399 0 1.000 0 #&gt; 9 9 torch.Size([3, 28, 28]) 2352 449 0.191 0.365 0 1.000 0 #&gt; 10 10 torch.Size([3, 28, 28]) 2352 465 0.198 0.367 0 1.000 0 #&gt; 11 11 torch.Size([3, 28, 28]) 2352 383 0.163 0.338 0 1.000 0 #&gt; 12 12 torch.Size([3, 28, 28]) 2352 499 0.212 0.378 0 1.000 0 #&gt; 13 13 torch.Size([3, 28, 28]) 2352 313 0.133 0.309 0 0.996 0 #&gt; 14 14 torch.Size([3, 28, 28]) 2352 360 0.153 0.325 0 1.000 0 #&gt; 15 15 torch.Size([3, 28, 28]) 2352 435 0.185 0.358 0 0.996 0 #&gt; 16 16 torch.Size([3, 28, 28]) 2352 429 0.182 0.358 0 1.000 0 #&gt; 17 17 torch.Size([3, 28, 28]) 2352 596 0.254 0.408 0 1.000 0 #&gt; 18 18 torch.Size([3, 28, 28]) 2352 527 0.224 0.392 0 1.000 0 #&gt; 19 19 torch.Size([3, 28, 28]) 2352 303 0.129 0.301 0 1.000 0 #&gt; 20 20 torch.Size([3, 28, 28]) 2352 458 0.195 0.364 0 1.000 0 Elapsed time per size of sample: toc() # 60 1.663s # 600 13.5s # 6000 54.321 sec; # 60000 553.489 sec elapsed #&gt; 12.425 sec elapsed "],
["working-with-data-table.html", "Chapter 15 Working with data.table 15.1 Load PyTorch libraries 15.2 Load dataset 15.3 Read the datasets without normalization 15.4 Using data.table", " Chapter 15 Working with data.table 15.1 Load PyTorch libraries library(rTorch) torch &lt;- import(&quot;torch&quot;) torchvision &lt;- import(&quot;torchvision&quot;) nn &lt;- import(&quot;torch.nn&quot;) transforms &lt;- import(&quot;torchvision.transforms&quot;) dsets &lt;- import(&quot;torchvision.datasets&quot;) builtins &lt;- import_builtins() np &lt;- import(&quot;numpy&quot;) 15.2 Load dataset ## Dataset iteration batch settings # folders where the images are located train_data_path = &#39;./mnist_png_full/training/&#39; test_data_path = &#39;./mnist_png_full/testing/&#39; 15.3 Read the datasets without normalization train_dataset = torchvision$datasets$ImageFolder(root = train_data_path, transform = torchvision$transforms$ToTensor() ) print(train_dataset) #&gt; Dataset ImageFolder #&gt; Number of datapoints: 60000 #&gt; Root location: ./mnist_png_full/training/ #&gt; StandardTransform #&gt; Transform: ToTensor() 15.4 Using data.table library(data.table) library(tictoc) tic() fun_list &lt;- list( numel = c(&quot;numel&quot;), sum = c(&quot;sum&quot;, &quot;item&quot;), mean = c(&quot;mean&quot;, &quot;item&quot;), std = c(&quot;std&quot;, &quot;item&quot;), med = c(&quot;median&quot;, &quot;item&quot;), max = c(&quot;max&quot;, &quot;item&quot;), min = c(&quot;min&quot;, &quot;item&quot;) ) idx &lt;- seq(0L, 599L) fun_get_tensor &lt;- function(x) py_get_item(train_dataset, x)[[0]] stat_fun &lt;- function(x, str_fun) { fun_var &lt;- paste0(&quot;fun_get_tensor(x)&quot;, &quot;$&quot;, str_fun, &quot;()&quot;) sapply(idx, function(x) ifelse(is.numeric(eval(parse(text = fun_var))), # size return character eval(parse(text = fun_var)), # all else are numeric as.character(eval(parse(text = fun_var))))) } dt &lt;- data.table(ridx = idx+1, do.call(data.table, lapply( sapply(fun_list, function(x) paste(x, collapse = &quot;()$&quot;)), function(y) stat_fun(1, y) ) ) ) Summary statistics: head(dt) #&gt; ridx numel sum mean std med max min #&gt; 1: 1 2352 366 0.156 0.329 0 1 0 #&gt; 2: 2 2352 284 0.121 0.297 0 1 0 #&gt; 3: 3 2352 645 0.274 0.420 0 1 0 #&gt; 4: 4 2352 410 0.174 0.355 0 1 0 #&gt; 5: 5 2352 321 0.137 0.312 0 1 0 #&gt; 6: 6 2352 654 0.278 0.421 0 1 0 Elapsed time per size of sample: toc() # 60 1.266 sec elapsed # 600 11.798 sec elapsed; # 6000 119.256 sec elapsed; # 60000 1117.619 sec elapsed #&gt; 11.8 sec elapsed "],
["appendixA.html", "A Statistical Background A.1 Basic statistical terms", " A Statistical Background A.1 Basic statistical terms A.1.1 Five-number summary The five-number summary consists of five values: minimum, first quantile, second quantile, third quantile, and maximum. The quantiles are calculated as: first quantile (\\(Q_1\\)): the median of the first half of the sorted data third quantile (\\(Q_3\\)): the median of the second half of the sorted data First quantile: 25th percentile. Second quantile: 50th percentile. Third quantile: 75th percentile. The interquartile range or IQR is defined as \\(Q_3 - Q_1\\) and is a measure of how spread out the middle 50% of values is. "],
["appendixB.html", "B Activation Functions B.1 The Sigmoid function B.2 The ReLU function B.3 The tanh function B.4 The Softmax Activation function B.5 Coding your own activation functions in Python B.6 Softmax in Python", " B Activation Functions library(rTorch) library(ggplot2) B.1 The Sigmoid function Using the PyTorch sigmoid() function: x &lt;- torch$range(-5., 5., 0.1) y &lt;- torch$sigmoid(x) df &lt;- data.frame(x = x$numpy(), sx = y$numpy()) df ggplot(df, aes(x = x, y = sx)) + geom_point() + ggtitle(&quot;Sigmoid&quot;) #&gt; x sx #&gt; 1 -5.0 0.00669 #&gt; 2 -4.9 0.00739 #&gt; 3 -4.8 0.00816 #&gt; 4 -4.7 0.00901 #&gt; 5 -4.6 0.00995 #&gt; 6 -4.5 0.01099 #&gt; 7 -4.4 0.01213 #&gt; 8 -4.3 0.01339 #&gt; 9 -4.2 0.01477 #&gt; 10 -4.1 0.01630 #&gt; 11 -4.0 0.01799 #&gt; 12 -3.9 0.01984 #&gt; 13 -3.8 0.02188 #&gt; 14 -3.7 0.02413 #&gt; 15 -3.6 0.02660 #&gt; 16 -3.5 0.02931 #&gt; 17 -3.4 0.03230 #&gt; 18 -3.3 0.03557 #&gt; 19 -3.2 0.03917 #&gt; 20 -3.1 0.04311 #&gt; 21 -3.0 0.04743 #&gt; 22 -2.9 0.05215 #&gt; 23 -2.8 0.05732 #&gt; 24 -2.7 0.06297 #&gt; 25 -2.6 0.06914 #&gt; 26 -2.5 0.07586 #&gt; 27 -2.4 0.08317 #&gt; 28 -2.3 0.09112 #&gt; 29 -2.2 0.09975 #&gt; 30 -2.1 0.10910 #&gt; 31 -2.0 0.11920 #&gt; 32 -1.9 0.13011 #&gt; 33 -1.8 0.14185 #&gt; 34 -1.7 0.15447 #&gt; 35 -1.6 0.16798 #&gt; 36 -1.5 0.18243 #&gt; 37 -1.4 0.19782 #&gt; 38 -1.3 0.21417 #&gt; 39 -1.2 0.23148 #&gt; 40 -1.1 0.24974 #&gt; 41 -1.0 0.26894 #&gt; 42 -0.9 0.28905 #&gt; 43 -0.8 0.31003 #&gt; 44 -0.7 0.33181 #&gt; 45 -0.6 0.35434 #&gt; 46 -0.5 0.37754 #&gt; 47 -0.4 0.40131 #&gt; 48 -0.3 0.42556 #&gt; 49 -0.2 0.45017 #&gt; 50 -0.1 0.47502 #&gt; 51 0.0 0.50000 #&gt; 52 0.1 0.52498 #&gt; 53 0.2 0.54983 #&gt; 54 0.3 0.57444 #&gt; 55 0.4 0.59869 #&gt; 56 0.5 0.62246 #&gt; 57 0.6 0.64566 #&gt; 58 0.7 0.66819 #&gt; 59 0.8 0.68997 #&gt; 60 0.9 0.71095 #&gt; 61 1.0 0.73106 #&gt; 62 1.1 0.75026 #&gt; 63 1.2 0.76852 #&gt; 64 1.3 0.78583 #&gt; 65 1.4 0.80218 #&gt; 66 1.5 0.81757 #&gt; 67 1.6 0.83202 #&gt; 68 1.7 0.84553 #&gt; 69 1.8 0.85815 #&gt; 70 1.9 0.86989 #&gt; 71 2.0 0.88080 #&gt; 72 2.1 0.89090 #&gt; 73 2.2 0.90025 #&gt; 74 2.3 0.90888 #&gt; 75 2.4 0.91683 #&gt; 76 2.5 0.92414 #&gt; 77 2.6 0.93086 #&gt; 78 2.7 0.93703 #&gt; 79 2.8 0.94268 #&gt; 80 2.9 0.94785 #&gt; 81 3.0 0.95257 #&gt; 82 3.1 0.95689 #&gt; 83 3.2 0.96083 #&gt; 84 3.3 0.96443 #&gt; 85 3.4 0.96770 #&gt; 86 3.5 0.97069 #&gt; 87 3.6 0.97340 #&gt; 88 3.7 0.97587 #&gt; 89 3.8 0.97812 #&gt; 90 3.9 0.98016 #&gt; 91 4.0 0.98201 #&gt; 92 4.1 0.98370 #&gt; 93 4.2 0.98523 #&gt; 94 4.3 0.98661 #&gt; 95 4.4 0.98787 #&gt; 96 4.5 0.98901 #&gt; 97 4.6 0.99005 #&gt; 98 4.7 0.99099 #&gt; 99 4.8 0.99184 #&gt; 100 4.9 0.99261 #&gt; 101 5.0 0.99331 Plot the sigmoid function using an R custom-made function: sigmoid = function(x) { 1 / (1 + exp(-x)) } x &lt;- seq(-5, 5, 0.01) plot(x, sigmoid(x), col = &#39;blue&#39;, cex = 0.5, main = &quot;Sigmoid&quot;) B.2 The ReLU function Using the PyTorch relu() function: x &lt;- torch$range(-5., 5., 0.1) y &lt;- torch$relu(x) df &lt;- data.frame(x = x$numpy(), sx = y$numpy()) df ggplot(df, aes(x = x, y = sx)) + geom_point() + ggtitle(&quot;ReLU&quot;) B.3 The tanh function Using the PyTorch tanh() function: x &lt;- torch$range(-5., 5., 0.1) y &lt;- torch$tanh(x) df &lt;- data.frame(x = x$numpy(), sx = y$numpy()) df ggplot(df, aes(x = x, y = sx)) + geom_point() + ggtitle(&quot;tanh&quot;) B.4 The Softmax Activation function Using the PyTorch softmax() function: x &lt;- torch$range(-5.0, 5.0, 0.1) y &lt;- torch$softmax(x, dim=0L) df &lt;- data.frame(x = x$numpy(), sx = y$numpy()) ggplot(df, aes(x = x, y = sx)) + geom_point() + ggtitle(&quot;Softmax&quot;) B.5 Coding your own activation functions in Python library(rTorch) import numpy as np import matplotlib.pyplot as plt np.random.seed(42) Linear activation def Linear(x, derivative=False): &quot;&quot;&quot; Computes the Linear activation function for array x inputs: x: array derivative: if True, return the derivative else the forward pass &quot;&quot;&quot; if derivative: # Return derivative of the function at x return np.ones_like(x) else: # Return forward pass of the function at x return x Sigmoid activation def Sigmoid(x, derivative=False): &quot;&quot;&quot; Computes the Sigmoid activation function for array x inputs: x: array derivative: if True, return the derivative else the forward pass &quot;&quot;&quot; f = 1/(1+np.exp(-x)) if derivative: # Return derivative of the function at x return f*(1-f) else: # Return forward pass of the function at x return f Hyperbolic Tangent activation def Tanh(x, derivative=False): &quot;&quot;&quot; Computes the Hyperbolic Tangent activation function for array x inputs: x: array derivative: if True, return the derivative else the forward pass &quot;&quot;&quot; f = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x)) if derivative: # Return derivative of the function at x return 1-f**2 else: # Return the forward pass of the function at x return f Rectifier linear unit (ReLU) def ReLU(x, derivative=False): &quot;&quot;&quot; Computes the Rectifier Linear Unit activation function for array x inputs: x: array derivative: if True, return the derivative else the forward pass &quot;&quot;&quot; if derivative: # Return derivative of the function at x return (x&gt;0).astype(int) else: # Return forward pass of the function at x return np.maximum(x, 0) Visualization with matplotlib Plotting using matplotlib: x = np.linspace(-6, 6, 100) units = { &quot;Linear&quot;: lambda x: Linear(x), &quot;Sigmoid&quot;: lambda x: Sigmoid(x), &quot;ReLU&quot;: lambda x: ReLU(x), &quot;tanh&quot;: lambda x: Tanh(x) } plt.figure(figsize=(5, 5)) [plt.plot(x, unit(x), label=unit_name, lw=2) for unit_name, unit in units.items()] plt.legend(loc=2, fontsize=16) plt.title(&#39;Activation functions&#39;, fontsize=20) plt.ylim([-2, 5]) plt.xlim([-6, 6]) plt.show() B.6 Softmax in Python # Source: https://dataaspirant.com/2017/03/07/difference-between-softmax-function-and-sigmoid-function/ import numpy as np import matplotlib.pyplot as plt def softmax(inputs): &quot;&quot;&quot; Calculate the softmax for the give inputs (array) :param inputs: :return: &quot;&quot;&quot; return np.exp(inputs) / float(sum(np.exp(inputs))) def line_graph(x, y, x_title, y_title): &quot;&quot;&quot; Draw line graph with x and y values :param x: :param y: :param x_title: :param y_title: :return: &quot;&quot;&quot; plt.plot(x, y) plt.xlabel(x_title) plt.ylabel(y_title) plt.show() graph_x = np.linspace(-6, 6, 100) graph_y = softmax(graph_x) print(&quot;Graph X readings: {}&quot;.format(graph_x)) print(&quot;Graph Y readings: {}&quot;.format(graph_y)) line_graph(graph_x, graph_y, &quot;Inputs&quot;, &quot;Softmax Scores&quot;) "],
["references.html", "References", " References Cordes, David, and Marcus Brown. 1991. “The Literate-Programming Paradigm.” Computer 24 (6): 52–61. https://doi.org/10.1109/2.86838. Knuth, Donald E. 1983. “Literate Programming.” The Computer Journal 27 (Issue 2): 97–111. https://doi.org/https://doi.org/10.1093/comjnl/27.2.97. "]
]
