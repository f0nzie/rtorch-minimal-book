# A neural network step-by-step

_Last update: Thu Oct 22 16:46:28 2020 -0500 (54a46ea04)_

## Introduction
Source: https://github.com/jcjohnson/pytorch-examples#pytorch-nn

In this example we use the torch `nn` package to implement our two-layer network:

## Select device


```r
library(rTorch)

device = torch$device('cpu')
# device = torch.device('cuda') # Uncomment this to run on GPU
```

* `N` is batch size; 
* `D_in` is input dimension;
* `H` is hidden dimension; 
* `D_out` is output dimension.


## Create the dataset


```r
invisible(torch$manual_seed(0))   # do not show the generator output
N <- 64L; D_in <- 1000L; H <- 100L; D_out <- 10L

# Create random Tensors to hold inputs and outputs
x = torch$randn(N, D_in, device=device)
y = torch$randn(N, D_out, device=device)
```


## Define the model
We use the `nn` package to define our model as a sequence of layers. `nn.Sequential` applies these leayers in sequence to produce an output. Each _Linear Module_ computes the output by using a linear function, and holds also tensors for its weights and biases. After constructing the model we use the `.to()` method to move it to the desired device, which could be `CPU` or `GPU`. Remember that we selected `CPU` with `torch$device('cpu')`.


```r
model <- torch$nn$Sequential(
  torch$nn$Linear(D_in, H),              # first layer
  torch$nn$ReLU(),
  torch$nn$Linear(H, D_out))$to(device)  # output layer

print(model)
```

```
#> Sequential(
#>   (0): Linear(in_features=1000, out_features=100, bias=True)
#>   (1): ReLU()
#>   (2): Linear(in_features=100, out_features=10, bias=True)
#> )
```


## The Loss function
The `nn` package also contains definitions of several loss functions; in this case we will use __Mean Squared Error__ ($MSE$) as our loss function. Setting `reduction='sum'` means that we are computing the *sum* of squared errors rather than the __mean__; this is for consistency with the examples above where we manually compute the loss, but in practice it is more common to use the mean squared error as a loss by setting `reduction='elementwise_mean'`.


```r
loss_fn = torch$nn$MSELoss(reduction = 'sum')
```


## Iterate through the dataset


```r
learning_rate = 1e-4

for (t in 1:500) {
  # Forward pass: compute predicted y by passing x to the model. Module objects
  # override the __call__ operator so you can call them like functions. When
  # doing so you pass a Tensor of input data to the Module and it produces
  # a Tensor of output data.
  y_pred = model(x)

  # Compute and print loss. We pass Tensors containing the predicted and true
  # values of y, and the loss function returns a Tensor containing the loss.
  loss = loss_fn(y_pred, y)
  cat(t, "\t")
  cat(loss$item(), "\n")
  
  # Zero the gradients before running the backward pass.
  model$zero_grad()

  # Backward pass: compute gradient of the loss with respect to all the learnable
  # parameters of the model. Internally, the parameters of each Module are stored
  # in Tensors with requires_grad=True, so this call will compute gradients for
  # all learnable parameters in the model.
  loss$backward()

  # Update the weights using gradient descent. Each parameter is a Tensor, so
  # we can access its data and gradients like we did before.
  with(torch$no_grad(), {
      for (param in iterate(model$parameters())) {
        # in Python this code is much simpler. In R we have to do some conversions
        # param$data <- torch$sub(param$data,
        #                         torch$mul(param$grad$float(),
        #                           torch$scalar_tensor(learning_rate)))
        param$data <- param$data - param$grad * learning_rate
      }
   })
}  
```

```
#> 1 	628 
#> 2 	585 
#> 3 	547 
#> 4 	513 
#> 5 	482 
#> 6 	455 
#> 7 	430 
#> 8 	406 
#> 9 	385 
#> 10 	364 
#> 11 	345 
#> 12 	328 
#> 13 	311 
#> 14 	295 
#> 15 	280 
#> 16 	265 
#> 17 	252 
#> 18 	239 
#> 19 	226 
#> 20 	214 
#> 21 	203 
#> 22 	192 
#> 23 	181 
#> 24 	172 
#> 25 	162 
#> 26 	153 
#> 27 	145 
#> 28 	137 
#> 29 	129 
#> 30 	122 
#> 31 	115 
#> 32 	109 
#> 33 	103 
#> 34 	96.9 
#> 35 	91.5 
#> 36 	86.3 
#> 37 	81.5 
#> 38 	76.9 
#> 39 	72.6 
#> 40 	68.5 
#> 41 	64.6 
#> 42 	61 
#> 43 	57.6 
#> 44 	54.3 
#> 45 	51.3 
#> 46 	48.5 
#> 47 	45.8 
#> 48 	43.2 
#> 49 	40.9 
#> 50 	38.6 
#> 51 	36.5 
#> 52 	34.5 
#> 53 	32.7 
#> 54 	30.9 
#> 55 	29.3 
#> 56 	27.8 
#> 57 	26.3 
#> 58 	24.9 
#> 59 	23.7 
#> 60 	22.4 
#> 61 	21.3 
#> 62 	20.2 
#> 63 	19.2 
#> 64 	18.2 
#> 65 	17.3 
#> 66 	16.5 
#> 67 	15.7 
#> 68 	14.9 
#> 69 	14.2 
#> 70 	13.5 
#> 71 	12.9 
#> 72 	12.3 
#> 73 	11.7 
#> 74 	11.1 
#> 75 	10.6 
#> 76 	10.1 
#> 77 	9.67 
#> 78 	9.24 
#> 79 	8.82 
#> 80 	8.42 
#> 81 	8.05 
#> 82 	7.69 
#> 83 	7.35 
#> 84 	7.03 
#> 85 	6.72 
#> 86 	6.43 
#> 87 	6.16 
#> 88 	5.9 
#> 89 	5.65 
#> 90 	5.41 
#> 91 	5.18 
#> 92 	4.97 
#> 93 	4.76 
#> 94 	4.57 
#> 95 	4.38 
#> 96 	4.2 
#> 97 	4.03 
#> 98 	3.87 
#> 99 	3.72 
#> 100 	3.57 
#> 101 	3.43 
#> 102 	3.29 
#> 103 	3.17 
#> 104 	3.04 
#> 105 	2.92 
#> 106 	2.81 
#> 107 	2.7 
#> 108 	2.6 
#> 109 	2.5 
#> 110 	2.41 
#> 111 	2.31 
#> 112 	2.23 
#> 113 	2.14 
#> 114 	2.06 
#> 115 	1.99 
#> 116 	1.91 
#> 117 	1.84 
#> 118 	1.77 
#> 119 	1.71 
#> 120 	1.65 
#> 121 	1.59 
#> 122 	1.53 
#> 123 	1.47 
#> 124 	1.42 
#> 125 	1.37 
#> 126 	1.32 
#> 127 	1.27 
#> 128 	1.23 
#> 129 	1.18 
#> 130 	1.14 
#> 131 	1.1 
#> 132 	1.06 
#> 133 	1.02 
#> 134 	0.989 
#> 135 	0.954 
#> 136 	0.921 
#> 137 	0.889 
#> 138 	0.858 
#> 139 	0.828 
#> 140 	0.799 
#> 141 	0.772 
#> 142 	0.745 
#> 143 	0.719 
#> 144 	0.695 
#> 145 	0.671 
#> 146 	0.648 
#> 147 	0.626 
#> 148 	0.605 
#> 149 	0.584 
#> 150 	0.564 
#> 151 	0.545 
#> 152 	0.527 
#> 153 	0.509 
#> 154 	0.492 
#> 155 	0.476 
#> 156 	0.46 
#> 157 	0.444 
#> 158 	0.43 
#> 159 	0.415 
#> 160 	0.402 
#> 161 	0.388 
#> 162 	0.375 
#> 163 	0.363 
#> 164 	0.351 
#> 165 	0.339 
#> 166 	0.328 
#> 167 	0.318 
#> 168 	0.307 
#> 169 	0.297 
#> 170 	0.287 
#> 171 	0.278 
#> 172 	0.269 
#> 173 	0.26 
#> 174 	0.252 
#> 175 	0.244 
#> 176 	0.236 
#> 177 	0.228 
#> 178 	0.221 
#> 179 	0.214 
#> 180 	0.207 
#> 181 	0.2 
#> 182 	0.194 
#> 183 	0.187 
#> 184 	0.181 
#> 185 	0.176 
#> 186 	0.17 
#> 187 	0.165 
#> 188 	0.159 
#> 189 	0.154 
#> 190 	0.149 
#> 191 	0.145 
#> 192 	0.14 
#> 193 	0.136 
#> 194 	0.131 
#> 195 	0.127 
#> 196 	0.123 
#> 197 	0.119 
#> 198 	0.115 
#> 199 	0.112 
#> 200 	0.108 
#> 201 	0.105 
#> 202 	0.102 
#> 203 	0.0983 
#> 204 	0.0952 
#> 205 	0.0923 
#> 206 	0.0894 
#> 207 	0.0866 
#> 208 	0.0838 
#> 209 	0.0812 
#> 210 	0.0787 
#> 211 	0.0762 
#> 212 	0.0739 
#> 213 	0.0716 
#> 214 	0.0693 
#> 215 	0.0672 
#> 216 	0.0651 
#> 217 	0.0631 
#> 218 	0.0611 
#> 219 	0.0592 
#> 220 	0.0574 
#> 221 	0.0556 
#> 222 	0.0539 
#> 223 	0.0522 
#> 224 	0.0506 
#> 225 	0.0491 
#> 226 	0.0476 
#> 227 	0.0461 
#> 228 	0.0447 
#> 229 	0.0433 
#> 230 	0.042 
#> 231 	0.0407 
#> 232 	0.0394 
#> 233 	0.0382 
#> 234 	0.0371 
#> 235 	0.0359 
#> 236 	0.0348 
#> 237 	0.0338 
#> 238 	0.0327 
#> 239 	0.0317 
#> 240 	0.0308 
#> 241 	0.0298 
#> 242 	0.0289 
#> 243 	0.028 
#> 244 	0.0272 
#> 245 	0.0263 
#> 246 	0.0255 
#> 247 	0.0248 
#> 248 	0.024 
#> 249 	0.0233 
#> 250 	0.0226 
#> 251 	0.0219 
#> 252 	0.0212 
#> 253 	0.0206 
#> 254 	0.02 
#> 255 	0.0194 
#> 256 	0.0188 
#> 257 	0.0182 
#> 258 	0.0177 
#> 259 	0.0171 
#> 260 	0.0166 
#> 261 	0.0161 
#> 262 	0.0156 
#> 263 	0.0151 
#> 264 	0.0147 
#> 265 	0.0142 
#> 266 	0.0138 
#> 267 	0.0134 
#> 268 	0.013 
#> 269 	0.0126 
#> 270 	0.0122 
#> 271 	0.0119 
#> 272 	0.0115 
#> 273 	0.0112 
#> 274 	0.0108 
#> 275 	0.0105 
#> 276 	0.0102 
#> 277 	0.00988 
#> 278 	0.00959 
#> 279 	0.0093 
#> 280 	0.00902 
#> 281 	0.00875 
#> 282 	0.00849 
#> 283 	0.00824 
#> 284 	0.00799 
#> 285 	0.00775 
#> 286 	0.00752 
#> 287 	0.0073 
#> 288 	0.00708 
#> 289 	0.00687 
#> 290 	0.00666 
#> 291 	0.00647 
#> 292 	0.00627 
#> 293 	0.00609 
#> 294 	0.00591 
#> 295 	0.00573 
#> 296 	0.00556 
#> 297 	0.0054 
#> 298 	0.00524 
#> 299 	0.00508 
#> 300 	0.00493 
#> 301 	0.00478 
#> 302 	0.00464 
#> 303 	0.0045 
#> 304 	0.00437 
#> 305 	0.00424 
#> 306 	0.00412 
#> 307 	0.00399 
#> 308 	0.00388 
#> 309 	0.00376 
#> 310 	0.00365 
#> 311 	0.00354 
#> 312 	0.00344 
#> 313 	0.00334 
#> 314 	0.00324 
#> 315 	0.00314 
#> 316 	0.00305 
#> 317 	0.00296 
#> 318 	0.00287 
#> 319 	0.00279 
#> 320 	0.00271 
#> 321 	0.00263 
#> 322 	0.00255 
#> 323 	0.00248 
#> 324 	0.0024 
#> 325 	0.00233 
#> 326 	0.00226 
#> 327 	0.0022 
#> 328 	0.00213 
#> 329 	0.00207 
#> 330 	0.00201 
#> 331 	0.00195 
#> 332 	0.00189 
#> 333 	0.00184 
#> 334 	0.00178 
#> 335 	0.00173 
#> 336 	0.00168 
#> 337 	0.00163 
#> 338 	0.00158 
#> 339 	0.00154 
#> 340 	0.00149 
#> 341 	0.00145 
#> 342 	0.00141 
#> 343 	0.00137 
#> 344 	0.00133 
#> 345 	0.00129 
#> 346 	0.00125 
#> 347 	0.00121 
#> 348 	0.00118 
#> 349 	0.00114 
#> 350 	0.00111 
#> 351 	0.00108 
#> 352 	0.00105 
#> 353 	0.00102 
#> 354 	0.000987 
#> 355 	0.000958 
#> 356 	0.000931 
#> 357 	0.000904 
#> 358 	0.000877 
#> 359 	0.000852 
#> 360 	0.000827 
#> 361 	0.000803 
#> 362 	0.00078 
#> 363 	0.000757 
#> 364 	0.000735 
#> 365 	0.000714 
#> 366 	0.000693 
#> 367 	0.000673 
#> 368 	0.000654 
#> 369 	0.000635 
#> 370 	0.000617 
#> 371 	0.000599 
#> 372 	0.000581 
#> 373 	0.000565 
#> 374 	0.000548 
#> 375 	0.000532 
#> 376 	0.000517 
#> 377 	0.000502 
#> 378 	0.000488 
#> 379 	0.000474 
#> 380 	0.00046 
#> 381 	0.000447 
#> 382 	0.000434 
#> 383 	0.000421 
#> 384 	0.000409 
#> 385 	0.000397 
#> 386 	0.000386 
#> 387 	0.000375 
#> 388 	0.000364 
#> 389 	0.000354 
#> 390 	0.000343 
#> 391 	0.000334 
#> 392 	0.000324 
#> 393 	0.000315 
#> 394 	0.000306 
#> 395 	0.000297 
#> 396 	0.000288 
#> 397 	0.00028 
#> 398 	0.000272 
#> 399 	0.000264 
#> 400 	0.000257 
#> 401 	0.000249 
#> 402 	0.000242 
#> 403 	0.000235 
#> 404 	0.000228 
#> 405 	0.000222 
#> 406 	0.000216 
#> 407 	0.000209 
#> 408 	0.000203 
#> 409 	0.000198 
#> 410 	0.000192 
#> 411 	0.000186 
#> 412 	0.000181 
#> 413 	0.000176 
#> 414 	0.000171 
#> 415 	0.000166 
#> 416 	0.000161 
#> 417 	0.000157 
#> 418 	0.000152 
#> 419 	0.000148 
#> 420 	0.000144 
#> 421 	0.00014 
#> 422 	0.000136 
#> 423 	0.000132 
#> 424 	0.000128 
#> 425 	0.000124 
#> 426 	0.000121 
#> 427 	0.000117 
#> 428 	0.000114 
#> 429 	0.000111 
#> 430 	0.000108 
#> 431 	0.000105 
#> 432 	0.000102 
#> 433 	9.87e-05 
#> 434 	9.59e-05 
#> 435 	9.32e-05 
#> 436 	9.06e-05 
#> 437 	8.8e-05 
#> 438 	8.55e-05 
#> 439 	8.31e-05 
#> 440 	8.07e-05 
#> 441 	7.84e-05 
#> 442 	7.62e-05 
#> 443 	7.4e-05 
#> 444 	7.2e-05 
#> 445 	6.99e-05 
#> 446 	6.79e-05 
#> 447 	6.6e-05 
#> 448 	6.41e-05 
#> 449 	6.23e-05 
#> 450 	6.06e-05 
#> 451 	5.89e-05 
#> 452 	5.72e-05 
#> 453 	5.56e-05 
#> 454 	5.4e-05 
#> 455 	5.25e-05 
#> 456 	5.1e-05 
#> 457 	4.96e-05 
#> 458 	4.82e-05 
#> 459 	4.68e-05 
#> 460 	4.55e-05 
#> 461 	4.42e-05 
#> 462 	4.3e-05 
#> 463 	4.18e-05 
#> 464 	4.06e-05 
#> 465 	3.94e-05 
#> 466 	3.83e-05 
#> 467 	3.72e-05 
#> 468 	3.62e-05 
#> 469 	3.52e-05 
#> 470 	3.42e-05 
#> 471 	3.32e-05 
#> 472 	3.23e-05 
#> 473 	3.14e-05 
#> 474 	3.05e-05 
#> 475 	2.96e-05 
#> 476 	2.88e-05 
#> 477 	2.8e-05 
#> 478 	2.72e-05 
#> 479 	2.65e-05 
#> 480 	2.57e-05 
#> 481 	2.5e-05 
#> 482 	2.43e-05 
#> 483 	2.36e-05 
#> 484 	2.29e-05 
#> 485 	2.23e-05 
#> 486 	2.17e-05 
#> 487 	2.11e-05 
#> 488 	2.05e-05 
#> 489 	1.99e-05 
#> 490 	1.94e-05 
#> 491 	1.88e-05 
#> 492 	1.83e-05 
#> 493 	1.78e-05 
#> 494 	1.73e-05 
#> 495 	1.68e-05 
#> 496 	1.63e-05 
#> 497 	1.59e-05 
#> 498 	1.54e-05 
#> 499 	1.5e-05 
#> 500 	1.46e-05
```


## Using R generics

### Simplify tensor operations

The following two expressions are equivalent, with the first being the long version natural way of doing it in __PyTorch__. The second is using the generics in R for subtraction, multiplication and scalar conversion.


```r
param$data <- torch$sub(param$data,
                        torch$mul(param$grad$float(),
                          torch$scalar_tensor(learning_rate)))
```


```r
param$data <- param$data - param$grad * learning_rate
```


## An elegant neural network


```r
invisible(torch$manual_seed(0))   # do not show the generator output
# layer properties
N <- 64L; D_in <- 1000L; H <- 100L; D_out <- 10L

# Create random Tensors to hold inputs and outputs
x = torch$randn(N, D_in, device=device)
y = torch$randn(N, D_out, device=device)

# set up the neural network
model <- torch$nn$Sequential(
  torch$nn$Linear(D_in, H),              # first layer
  torch$nn$ReLU(),                       # activation
  torch$nn$Linear(H, D_out))$to(device)  # output layer

# specify how we will be computing the loss
loss_fn = torch$nn$MSELoss(reduction = 'sum')

learning_rate = 1e-4
loss_row <- list(vector())     # collect a list for the final dataframe

for (t in 1:500) {
  # Forward pass: compute predicted y by passing x to the model. Module objects
  # override the __call__ operator so you can call them like functions. When
  # doing so you pass a Tensor of input data to the Module and it produces
  # a Tensor of output data.
  y_pred = model(x)

  # Compute and print loss. We pass Tensors containing the predicted and true
  # values of y, and the loss function returns a Tensor containing the loss.
  loss = loss_fn(y_pred, y)  # (y_pred - y) is a tensor; loss_fn output is a scalar
  loss_row[[t]] <- c(t, loss$item())
  
  # Zero the gradients before running the backward pass.
  model$zero_grad()

  # Backward pass: compute gradient of the loss with respect to all the learnable
  # parameters of the model. Internally, the parameters of each module are stored
  # in tensors with `requires_grad=True`, so this call will compute gradients for
  # all learnable parameters in the model.
  loss$backward()

  # Update the weights using gradient descent. Each parameter is a tensor, so
  # we can access its data and gradients like we did before.
  with(torch$no_grad(), {
      for (param in iterate(model$parameters())) {
        # using R generics
        param$data <- param$data - param$grad * learning_rate
      }
   })
}  
```

## A browseable dataframe


```r
library(DT)
loss_df <- data.frame(Reduce(rbind, loss_row), row.names = NULL)
names(loss_df)[1] <- "iter"
names(loss_df)[2] <- "loss"
DT::datatable(loss_df)
```

<!--html_preserve--><div id="htmlwidget-1b4ff99564eb6e8884a5" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-1b4ff99564eb6e8884a5">{"x":{"filter":"none","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153","154","155","156","157","158","159","160","161","162","163","164","165","166","167","168","169","170","171","172","173","174","175","176","177","178","179","180","181","182","183","184","185","186","187","188","189","190","191","192","193","194","195","196","197","198","199","200","201","202","203","204","205","206","207","208","209","210","211","212","213","214","215","216","217","218","219","220","221","222","223","224","225","226","227","228","229","230","231","232","233","234","235","236","237","238","239","240","241","242","243","244","245","246","247","248","249","250","251","252","253","254","255","256","257","258","259","260","261","262","263","264","265","266","267","268","269","270","271","272","273","274","275","276","277","278","279","280","281","282","283","284","285","286","287","288","289","290","291","292","293","294","295","296","297","298","299","300","301","302","303","304","305","306","307","308","309","310","311","312","313","314","315","316","317","318","319","320","321","322","323","324","325","326","327","328","329","330","331","332","333","334","335","336","337","338","339","340","341","342","343","344","345","346","347","348","349","350","351","352","353","354","355","356","357","358","359","360","361","362","363","364","365","366","367","368","369","370","371","372","373","374","375","376","377","378","379","380","381","382","383","384","385","386","387","388","389","390","391","392","393","394","395","396","397","398","399","400","401","402","403","404","405","406","407","408","409","410","411","412","413","414","415","416","417","418","419","420","421","422","423","424","425","426","427","428","429","430","431","432","433","434","435","436","437","438","439","440","441","442","443","444","445","446","447","448","449","450","451","452","453","454","455","456","457","458","459","460","461","462","463","464","465","466","467","468","469","470","471","472","473","474","475","476","477","478","479","480","481","482","483","484","485","486","487","488","489","490","491","492","493","494","495","496","497","498","499","500"],[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500],[628.283874511719,584.959289550781,546.727966308594,512.688537597656,482.130554199219,454.626525878906,429.55517578125,406.398956298828,384.64404296875,364.312469482422,345.386596679688,327.625793457031,310.800231933594,294.837890625,279.732452392578,265.307952880859,251.623046875,238.536041259766,226.023147583008,214.102508544922,202.700271606445,191.805923461914,181.447341918945,171.594848632812,162.211395263672,153.292221069336,144.852615356445,136.859939575195,129.278610229492,122.084869384766,115.273597717285,108.814483642578,102.702018737793,96.928092956543,91.4746704101562,86.3277969360352,81.4750137329102,76.8914489746094,72.5689468383789,68.4777374267578,64.6260833740234,60.9971504211426,57.569149017334,54.3426971435547,51.3091125488281,48.4516296386719,45.7645149230957,43.2405242919922,40.8660736083984,38.6300811767578,36.5252914428711,34.5491638183594,32.6934509277344,30.9498176574707,29.3073463439941,27.7627716064453,26.310905456543,24.94309425354,23.65380859375,22.4388256072998,21.2925815582275,20.2127914428711,19.1928825378418,18.2330322265625,17.32932472229,16.4769058227539,15.6711559295654,14.9095163345337,14.1905403137207,13.51087474823,12.8681945800781,12.2604608535767,11.685152053833,11.1410474777222,10.6257915496826,10.1372718811035,9.67442607879639,9.23574733734131,8.81936073303223,8.423415184021,8.0479793548584,7.69129371643066,7.3524055480957,7.03039932250977,6.72467756271362,6.43443441390991,6.15827894210815,5.89544296264648,5.64577484130859,5.4085898399353,5.18278884887695,4.96778297424316,4.76282262802124,4.56794261932373,4.38167667388916,4.20395755767822,4.034255027771,3.87236881256104,3.7176661491394,3.56985235214233,3.42879033088684,3.29393482208252,3.1650059223175,3.04172992706299,2.92383289337158,2.8109929561615,2.70296669006348,2.5995466709137,2.50053334236145,2.40561032295227,2.31467843055725,2.22756838798523,2.14398527145386,2.06382441520691,1.98689556121826,1.91316115856171,1.84239768981934,1.77456092834473,1.70933353900909,1.64674973487854,1.58666336536407,1.52893197536469,1.47349846363068,1.42019391059875,1.36904418468475,1.31993734836578,1.27271342277527,1.22735118865967,1.18371605873108,1.14171314239502,1.10128891468048,1.06237864494324,1.02497482299805,0.9889817237854,0.95431661605835,0.920958697795868,0.888810038566589,0.857857048511505,0.828074038028717,0.799380540847778,0.771741092205048,0.745122790336609,0.719462692737579,0.694717228412628,0.670871675014496,0.647948026657104,0.625913023948669,0.604666292667389,0.584196388721466,0.564445912837982,0.545387148857117,0.527021527290344,0.509311974048615,0.4922194480896,0.47573059797287,0.459827750921249,0.444491237401962,0.429690569639206,0.415387392044067,0.40159210562706,0.388279020786285,0.375434100627899,0.363029330968857,0.35105288028717,0.339493781328201,0.328329622745514,0.317552447319031,0.307137221097946,0.297078490257263,0.287366539239883,0.277984440326691,0.268914103507996,0.260156899690628,0.251694798469543,0.243515461683273,0.235606402158737,0.227970317006111,0.220615655183792,0.213502466678619,0.206627368927002,0.19998787343502,0.193573549389839,0.187366172671318,0.181365713477135,0.175562530755997,0.169950991868973,0.16452294588089,0.159277930855751,0.154206499457359,0.149300888180733,0.144553974270821,0.139964535832405,0.135524183511734,0.131227552890778,0.127071633934975,0.123050883412361,0.119162112474442,0.115398794412613,0.111758626997471,0.108234480023384,0.104825533926487,0.101525321602821,0.0983332097530365,0.0952441319823265,0.092253103852272,0.0893576741218567,0.0865568742156029,0.0838496387004852,0.0812318399548531,0.0786957666277885,0.0762432739138603,0.0738693326711655,0.0715707764029503,0.0693463534116745,0.0671909749507904,0.0651053339242935,0.0630877539515495,0.0611334443092346,0.0592397749423981,0.0574058853089809,0.0556300282478333,0.0539103336632252,0.0522454231977463,0.0506339073181152,0.0490731671452522,0.0475621670484543,0.0460991337895393,0.0446819961071014,0.0433073565363884,0.0419757328927517,0.0406862944364548,0.0394376255571842,0.0382279492914677,0.0370563454926014,0.0359213463962078,0.0348213985562325,0.0337561704218388,0.0327244810760021,0.0317247584462166,0.030755840241909,0.0298167951405048,0.0289072245359421,0.0280260629951954,0.0271728727966547,0.0263454802334309,0.0255436822772026,0.0247666668146849,0.024014201015234,0.0232851449400187,0.0225782487541437,0.0218929927796125,0.021229138597846,0.0205857437103987,0.0199623163789511,0.0193577408790588,0.0187720693647861,0.0182042922824621,0.0176538955420256,0.0171204283833504,0.0166035108268261,0.0161023829132318,0.0156170753762126,0.0151462182402611,0.0146900489926338,0.0142474789172411,0.0138185834512115,0.0134028671309352,0.0129999183118343,0.0126090021803975,0.0122300619259477,0.0118627417832613,0.0115067670121789,0.0111616086214781,0.0108267990872264,0.0105021754279733,0.0101879462599754,0.00988284964114428,0.00958701968193054,0.00930015556514263,0.00902190059423447,0.00875221751630306,0.00849072355777025,0.00823704525828362,0.00799110066145658,0.00775274494662881,0.00752145890146494,0.00729728769510984,0.00707977823913097,0.00686886580660939,0.00666445214301348,0.00646621733903885,0.00627401378005743,0.00608737720176578,0.00590644683688879,0.00573085807263851,0.0055606896057725,0.00539560476318002,0.00523549551144242,0.00508022122085094,0.00492965802550316,0.00478361127898097,0.00464196549728513,0.00450445152819157,0.00437118066474795,0.00424178829416633,0.00411629024893045,0.00399459106847644,0.00387655268423259,0.00376211386173964,0.00365100242197514,0.00354327633976936,0.00343872653320432,0.00333727174438536,0.00323886075057089,0.00314340041950345,0.00305079785175622,0.00296098948456347,0.0028738162945956,0.00278921681456268,0.00270719011314213,0.00262756505981088,0.00255030859261751,0.00247536064125597,0.00240262248553336,0.00233207480050623,0.00226361863315105,0.00219722441397607,0.00213277456350625,0.00207025348208845,0.00200955709442496,0.00195070076733828,0.00189359067007899,0.00183812959585339,0.00178428704384714,0.00173210655339062,0.00168142572510988,0.00163229065947235,0.00158459157682955,0.00153828307520598,0.00149334024172276,0.00144973606802523,0.00140743353404105,0.00136640947312117,0.00132654793560505,0.00128783995751292,0.00125031580682844,0.0012138836318627,0.00117850385140628,0.00114419125020504,0.00111087993718684,0.00107855198439211,0.00104718515649438,0.00101672357413918,0.000987160135991871,0.000958467135205865,0.000930605630856007,0.000903585867490619,0.000877335842233151,0.000851894845254719,0.000827192387077957,0.000803192786406726,0.000779891153797507,0.000757286907173693,0.000735332549083978,0.000714045600034297,0.000693372450768948,0.000673287024255842,0.000653798750136048,0.000634883297607303,0.000616518955212086,0.000598690297920257,0.000581372412852943,0.000564583344385028,0.000548257492482662,0.000532434321939945,0.000517066335305572,0.000502136303111911,0.000487654033349827,0.000473582389531657,0.000459915958344936,0.000446654710685834,0.000433788838563487,0.000421288190409541,0.000409158878028393,0.0003973804123234,0.000385941122658551,0.000374841445591301,0.000364052131772041,0.000353573705069721,0.000343408260960132,0.00033354019979015,0.000323963875416666,0.000314654054818675,0.000305617926642299,0.00029684163746424,0.000288320414256305,0.000280042469967157,0.000272016652161255,0.000264211179455742,0.000256636907579377,0.000249282165896147,0.000242136855376884,0.000235196464927867,0.000228469507419504,0.000221923721255735,0.000215571082662791,0.00020940754620824,0.000203419767785817,0.000197598259546794,0.000191947096027434,0.000186454359209165,0.000181138180778362,0.000175959954503924,0.000170937855727971,0.000166058729519136,0.000161318763275631,0.000156715832417831,0.000152243199408986,0.000147900223964825,0.000143682380439714,0.000139584793942049,0.000135608483105898,0.000131745633552782,0.00012799448450096,0.000124349695397541,0.000120808712381404,0.000117372692329809,0.00011403467215132,0.000110786000732332,0.000107634958112612,0.000104573773569427,0.000101601093774661,9.8721677204594e-05,9.59186290856451e-05,9.32002149056643e-05,9.05551132746041e-05,8.79877770785242e-05,8.54965037433431e-05,8.3069535321556e-05,8.07150354376063e-05,7.84298099461012e-05,7.62071722419932e-05,7.40465184208006e-05,7.19509407645091e-05,6.99149532010779e-05,6.7937231506221e-05,6.60145524307154e-05,6.41479637124576e-05,6.23342202743515e-05,6.05726090725511e-05,5.88608891121112e-05,5.71999989915639e-05,5.55833394173533e-05,5.40162291144952e-05,5.24887218489312e-05,5.10077697981615e-05,4.95671047247015e-05,4.81654424220324e-05,4.68105063191615e-05,4.54920227639377e-05,4.42101554654073e-05,4.29607389378361e-05,4.17523551732302e-05,4.05779937864281e-05,3.94342314393725e-05,3.83241822419222e-05,3.72479225916322e-05,3.61980746674817e-05,3.51797971234191e-05,3.41916129400488e-05,3.32262679876294e-05,3.22919986501802e-05,3.13873752020299e-05,3.05041248793714e-05,2.96475081995595e-05,2.88136052404298e-05,2.80066851701122e-05,2.7218211471336e-05,2.64570189756341e-05,2.57146512012696e-05,2.49926233664155e-05,2.42910955421394e-05,2.36112318816595e-05,2.29497436521342e-05,2.23050792556023e-05,2.1680687495973e-05,2.10733996937051e-05,2.04816551558906e-05,1.99101468751905e-05,1.93514588318067e-05,1.88111043826211e-05,1.82838848559186e-05,1.7774073057808e-05,1.72761829162482e-05,1.67938087543007e-05,1.6324356693076e-05,1.58670663950033e-05,1.54242698044982e-05,1.49925199366407e-05,1.45731628435897e-05]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>iter<\/th>\n      <th>loss<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":[1,2]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script><!--/html_preserve-->

## Plot the loss at each iteration


```r
library(ggplot2)
# plot
ggplot(loss_df, aes(x = iter, y = loss)) +
    geom_point()
```

<img src="0502-neural_networks-steps_files/figure-html/plot-loss-1.png" width="70%" style="display: block; margin: auto;" />

