[{"path":"index.html","id":"prerequisites","chapter":"Prerequisites","heading":"Prerequisites","text":"Last update: Sun Oct 25 12:05:18 2020 -0500 (79503f6ee)need couple things get rTorch working:Install Python Anaconda. Preferably, 64-bits, Python 3.6+. successfully tested Anaconda four different operating systems: Windows (Win10 Windows Server 2008); macOS (Sierra, Mojave Catalina); Linux (Debian, Fedora Ubuntu); lastly, Solaris 10. tests required CRAN.Install Python Anaconda. Preferably, 64-bits, Python 3.6+. successfully tested Anaconda four different operating systems: Windows (Win10 Windows Server 2008); macOS (Sierra, Mojave Catalina); Linux (Debian, Fedora Ubuntu); lastly, Solaris 10. tests required CRAN.Install R, Rtools RStudio. used two R versions R-3.6.3 R-4.0.2.Install R, Rtools RStudio. used two R versions R-3.6.3 R-4.0.2.Install R package reticulate, one provides connection R Python.Install R package reticulate, one provides connection R Python.Install stable version rTorch CRAN, latest version development via GitHub.Install stable version rTorch CRAN, latest version development via GitHub.Note. mandatory previously created Python environment Anaconda, PyTorch TorchVision already installed, another option reason reticulate refuses communicate conda environment. Keep mind also get rTorch conda environment installed directly R console, similar fashion R-TensorFlow . Use function install_pytorch() install conda environment PyTorch.","code":""},{"path":"index.html","id":"installation","chapter":"Prerequisites","heading":"Installation","text":"rTorch package can installed CRAN Github.CRAN:GitHub, install rTorch :install rTorch main master branch.want play latest rTorch version, install develop branch, like :clone Git terminal :allow build rTorch source.","code":"\ninstall.packages(\"rTorch\")\ndevtools::install_github(\"f0nzie/rTorch\")\ndevtools::install_github(\"f0nzie/rTorch\", ref=\"develop\")git clone https://github.com/f0nzie/rTorch.git"},{"path":"index.html","id":"python-anaconda","chapter":"Prerequisites","heading":"Python Anaconda","text":"preference installing Anaconda environment first, steps:","code":""},{"path":"index.html","id":"example","chapter":"Prerequisites","heading":"Example","text":"Create conda environment terminal :Activate new environment withInstall PyTorch related packages :last part -c pytorch specifies stable conda channel download PyTorch packages. conda installation may work don’t indicate channel.Now, can load rTorch R RStudio :","code":"conda create -n r-torch python=3.7conda activate r-torchconda install python=3.6.6 pytorch torchvision cpuonly matplotlib pandas -c pytorch\nlibrary(rTorch)"},{"path":"index.html","id":"automatic-installation","chapter":"Prerequisites","heading":"Automatic installation","text":"used idea automatic installation tensorflow package R, create function rTorch::install_pytorch(). function allow install conda environment complete PyTorch requirements plus packages specify. Example:explained detailed rTorch package manual.Note. matplotlib pandas really necessary rTorch work, asked matplotlib pandas work PyTorch. , decided install testing experimentation. work.","code":"\nrTorch:::install_conda(package=\"pytorch=1.4\", envname=\"r-torch\", \n                       conda=\"auto\", conda_python_version = \"3.6\", pip=FALSE, \n                       channel=\"pytorch\", \n                       extra_packages=c(\"torchvision\", \n                                        \"cpuonly\", \n                                        \"matplotlib\", \n                                        \"pandas\"))"},{"path":"intro.html","id":"intro","chapter":"1 Introduction","heading":"1 Introduction","text":"Last update: Sun Oct 25 13:00:41 2020 -0500 (265c0b3c1)","code":""},{"path":"intro.html","id":"motivation","chapter":"1 Introduction","heading":"1.1 Motivation","text":"want package something already working well, PyTorch?several reasons, main one bring another machine learning framework R. Probably, just feel PyTorch comfortable work . Feels pretty much like everything else Python. pythonic. tried frameworks R. closest matches natural language like PyTorch, MXnet. Unfortunately, MXnet hardest install maintain updates.Yes. worked directly PyTorch native Python environment, Jupyter, PyCharm, vscode notebooks hard quit RMarkdown get used . real thing regards literate programming reproducibility. contribute improving quality code establishes workflow better understanding subject intended readers (Knuth 1983), called literate programming paradigm (Cordes Brown 1991).additional benefit giving ability write combination Python R code together document. times better create class Python; times R convenient handle data structure. show examples using data.frame data.table along PyTorch tensors.","code":""},{"path":"intro.html","id":"start-using-rtorch","chapter":"1 Introduction","heading":"1.2 Start using rTorch","text":"Start using rTorch simple. installing minimum system requirements -conda -, just call :several ways testing rTorch running. Let’s see :","code":"\nlibrary(rTorch)"},{"path":"intro.html","id":"get-the-pytorch-version","chapter":"1 Introduction","heading":"1.2.1 Get the PyTorch version","text":"","code":"\nrTorch::torch_version()#> [1] \"1.6\""},{"path":"intro.html","id":"pytorch-configuration","chapter":"1 Introduction","heading":"1.2.2 PyTorch configuration","text":"show PyTorch version current version Python installed, well paths folders reside.","code":"\nrTorch::torch_config()#> PyTorch v1.6.0 (~/miniconda3/envs/r-torch/lib/python3.7/site-packages/torch)\n#> Python v3.7 (~/miniconda3/envs/r-torch/bin/python)\n#> NumPy v1.19.4)"},{"path":"intro.html","id":"what-can-you-do-with-rtorch","chapter":"1 Introduction","heading":"1.3 What can you do with rTorch","text":"Practically, can everything PyTorch within R ecosystem. Additionally rTorch module, can extract methods, functions classes, available two modules: torchvision np, short numpy. use modules :","code":"\nrTorch::torchvision\nrTorch::np\nrTorch::torch#> Module(torchvision)\n#> Module(numpy)\n#> Module(torch)"},{"path":"intro.html","id":"getting-help","chapter":"1 Introduction","heading":"1.4 Getting help","text":"get glimpse first lines help(\"torch\") via Python chunk:Finally, classes module torchvision.datasets. using Python list using help function.words, functions, modules, classes PyTorch available rTorch.","code":"help(\"torch\")...\n#> NAME\n#>     torch\n#> \n#> DESCRIPTION\n#>     The torch package contains data structures for multi-dimensional\n#>     tensors and mathematical operations over these are defined.\n#>     Additionally, it provides many utilities for efficient serializing of\n#>     Tensors and arbitrary types, and other useful utilities.\n...help(\"torch.tensor\")...\n#> Help on built-in function tensor in torch:\n#> \n#> torch.tensor = tensor(...)\n#>     tensor(data, dtype=None, device=None, requires_grad=False, pin_memory=False) -> Tensor\n#>     \n#>     Constructs a tensor with :attr:`data`.\n#>     \n#>     .. warning::\n#>     \n#>         :func:`torch.tensor` always copies :attr:`data`. If you have a Tensor\n#>         ``data`` and want to avoid a copy, use :func:`torch.Tensor.requires_grad_`\n#>         or :func:`torch.Tensor.detach`.\n#>         If you have a NumPy ``ndarray`` and want to avoid a copy, use\n#>         :func:`torch.as_tensor`.\n#>     \n#>     .. warning::\n#>     \n#>         When data is a tensor `x`, :func:`torch.tensor` reads out 'the data' from whatever it is passed,\n#>         and constructs a leaf variable. Therefore ``torch.tensor(x)`` is equivalent to ``x.clone().detach()``\n#>         and ``torch.tensor(x, requires_grad=True)`` is equivalent to ``x.clone().detach().requires_grad_(True)``.\n...help(\"torch.cat\")...\n#> Help on built-in function cat in torch:\n#> \n#> torch.cat = cat(...)\n#>     cat(tensors, dim=0, out=None) -> Tensor\n#>     \n#>     Concatenates the given sequence of :attr:`seq` tensors in the given dimension.\n#>     All tensors must either have the same shape (except in the concatenating\n#>     dimension) or be empty.\n#>     \n#>     :func:`torch.cat` can be seen as an inverse operation for :func:`torch.split`\n#>     and :func:`torch.chunk`.\n#>     \n#>     :func:`torch.cat` can be best understood via examples.\n#>     \n#>     Args:\n#>         tensors (sequence of Tensors): any python sequence of tensors of the same type.\n#>             Non-empty tensors provided must have the same shape, except in the\n#>             cat dimension.\n#>         dim (int, optional): the dimension over which the tensors are concatenated\n#>         out (Tensor, optional): the output tensor.\n...help(\"numpy.arange\")...\n#> Help on built-in function arange in numpy:\n#> \n#> numpy.arange = arange(...)\n#>     arange([start,] stop[, step,], dtype=None)\n#>     \n#>     Return evenly spaced values within a given interval.\n#>     \n#>     Values are generated within the half-open interval ``[start, stop)``\n#>     (in other words, the interval including `start` but excluding `stop`).\n#>     For integer arguments the function is equivalent to the Python built-in\n#>     `range` function, but returns an ndarray rather than a list.\n#>     \n#>     When using a non-integer step, such as 0.1, the results will often not\n#>     be consistent.  It is better to use `numpy.linspace` for these cases.\n#>     \n#>     Parameters\n#>     ----------\n#>     start : number, optional\n#>         Start of interval.  The interval includes this value.  The default\n#>         start value is 0.\n#>     stop : number\n#>         End of interval.  The interval does not include this value, except\n#>         in some cases where `step` is not an integer and floating point\n#>         round-off affects the length of `out`.\n#>     step : number, optional\n...help(\"torchvision.datasets\")...\n#> Help on package torchvision.datasets in torchvision:\n#> \n#> NAME\n#>     torchvision.datasets\n#> \n#> PACKAGE CONTENTS\n#>     caltech\n#>     celeba\n#>     cifar\n#>     cityscapes\n#>     coco\n#>     fakedata\n#>     flickr\n#>     folder\n#>     hmdb51\n#>     imagenet\n#>     kinetics\n#>     lsun\n#>     mnist\n#>     omniglot\n#>     phototour\n#>     samplers (package)\n#>     sbd\n#>     sbu\n#>     semeion\n#>     stl10\n#>     svhn\n#>     ucf101\n#>     usps\n#>     utils\n#>     video_utils\n#>     vision\n#>     voc\n#> \n#> CLASSES\n..."},{"path":"working-with-datatable.html","id":"working-with-datatable","chapter":"2 Working with data●table","heading":"2 Working with data●table","text":"Last update: Thu Nov 19 14:24:08 2020 -0600 (ca4f8b4a0)","code":""},{"path":"working-with-datatable.html","id":"load-pytorch-libraries","chapter":"2 Working with data●table","heading":"2.1 Load PyTorch libraries","text":"","code":"\nlibrary(rTorch)\n\ntorch       <- import(\"torch\")\ntorchvision <- import(\"torchvision\")\nnn          <- import(\"torch.nn\")\ntransforms  <- import(\"torchvision.transforms\")\ndsets       <- import(\"torchvision.datasets\")\nbuiltins    <- import_builtins()\nnp          <- import(\"numpy\")"},{"path":"working-with-datatable.html","id":"load-dataset","chapter":"2 Working with data●table","heading":"2.2 Load dataset","text":"","code":"\n## Dataset iteration batch settings\n# folders where the images are located\ntrain_data_path = './mnist_png_full/training/'\ntest_data_path  = './mnist_png_full/testing/'"},{"path":"working-with-datatable.html","id":"datasets-without-normalization","chapter":"2 Working with data●table","heading":"2.3 Datasets without normalization","text":"","code":"\ntrain_dataset = torchvision$datasets$ImageFolder(root = train_data_path, \n    transform = torchvision$transforms$ToTensor()\n)\n\nprint(train_dataset)#> Dataset ImageFolder\n#>     Number of datapoints: 60000\n#>     Root location: ./mnist_png_full/training/\n#>     StandardTransform\n#> Transform: ToTensor()"},{"path":"working-with-datatable.html","id":"using-data.table","chapter":"2 Working with data●table","heading":"2.4 Using data.table","text":"Summary statistics:Elapsed time per size sample:","code":"\nlibrary(data.table)\nlibrary(tictoc)\n\n\ntic()\n\nfun_list <- list(\n    numel = c(\"numel\"),\n    sum   = c(\"sum\",    \"item\"),\n    mean  = c(\"mean\",   \"item\"),\n    std   = c(\"std\",    \"item\"),\n    med   = c(\"median\", \"item\"),\n    max   = c(\"max\",    \"item\"),\n    min   = c(\"min\",    \"item\")\n    )\n\nidx <- seq(0L, 599L)\n\nfun_get_tensor <- function(x) py_get_item(train_dataset, x)[[0]]\n\nstat_fun <- function(x, str_fun) {\n  fun_var <- paste0(\"fun_get_tensor(x)\", \"$\", str_fun, \"()\")\n  sapply(idx, function(x) \n    ifelse(is.numeric(eval(parse(text = fun_var))),  # size return character\n           eval(parse(text = fun_var)),              # all else are numeric\n           as.character(eval(parse(text = fun_var)))))\n}  \n\n\ndt <- data.table(ridx = idx+1,\n  do.call(data.table, \n          lapply(\n            sapply(fun_list, function(x) paste(x, collapse = \"()$\")), \n            function(y) stat_fun(1, y)\n          )\n  )\n)\nhead(dt)#>    ridx numel sum  mean   std med max min\n#> 1:    1  2352 366 0.156 0.329   0   1   0\n#> 2:    2  2352 284 0.121 0.297   0   1   0\n#> 3:    3  2352 645 0.274 0.420   0   1   0\n#> 4:    4  2352 410 0.174 0.355   0   1   0\n#> 5:    5  2352 321 0.137 0.312   0   1   0\n#> 6:    6  2352 654 0.278 0.421   0   1   0\ntoc()\n\n#    60    1.266 sec elapsed\n#   600   11.798 sec elapsed;\n#  6000  119.256 sec elapsed;\n# 60000 1117.619 sec elapsed#> 10.899 sec elapsed"},{"path":"appendixA.html","id":"appendixA","chapter":"A Statistical Background","heading":"A Statistical Background","text":"Last update: Thu Oct 22 16:46:28 2020 -0500 (54a46ea04)","code":""},{"path":"appendixA.html","id":"basic-statistical-terms","chapter":"A Statistical Background","heading":"A.1 Basic statistical terms","text":"","code":""},{"path":"appendixA.html","id":"five-number-summary","chapter":"A Statistical Background","heading":"A.1.1 Five-number summary","text":"five-number summary consists five values: minimum, first quantile, second quantile, third quantile, maximum. quantiles calculated :first quantile (\\(Q_1\\)): median first half sorted datathird quantile (\\(Q_3\\)): median second half sorted dataFirst quantile: 25th percentile.\nSecond quantile: 50th percentile.\nThird quantile: 75th percentile.interquartile range IQR defined \\(Q_3 - Q_1\\) measure spread middle 50% values .","code":""},{"path":"appendixB.html","id":"appendixB","chapter":"B Activation Functions","heading":"B Activation Functions","text":"Last update: Thu Oct 22 16:46:28 2020 -0500 (54a46ea04)","code":"\nlibrary(rTorch)\nlibrary(ggplot2)"},{"path":"appendixB.html","id":"sigmoid","chapter":"B Activation Functions","heading":"B.1 Sigmoid","text":"Using PyTorch sigmoid() function:Plot sigmoid function using R custom-made function:","code":"\nx <- torch$range(-5., 5., 0.1)\ny <- torch$sigmoid(x)\n\ndf <- data.frame(x = x$numpy(), sx = y$numpy())\ndf\n\nggplot(df, aes(x = x, y = sx)) + \n    geom_point() +\n    ggtitle(\"Sigmoid\")#>        x      sx\n#> 1   -5.0 0.00669\n#> 2   -4.9 0.00739\n#> 3   -4.8 0.00816\n#> 4   -4.7 0.00901\n#> 5   -4.6 0.00995\n#> 6   -4.5 0.01099\n#> 7   -4.4 0.01213\n#> 8   -4.3 0.01339\n#> 9   -4.2 0.01477\n#> 10  -4.1 0.01630\n#> 11  -4.0 0.01799\n#> 12  -3.9 0.01984\n#> 13  -3.8 0.02188\n#> 14  -3.7 0.02413\n#> 15  -3.6 0.02660\n#> 16  -3.5 0.02931\n#> 17  -3.4 0.03230\n#> 18  -3.3 0.03557\n#> 19  -3.2 0.03917\n#> 20  -3.1 0.04311\n#> 21  -3.0 0.04743\n#> 22  -2.9 0.05215\n#> 23  -2.8 0.05732\n#> 24  -2.7 0.06297\n#> 25  -2.6 0.06914\n#> 26  -2.5 0.07586\n#> 27  -2.4 0.08317\n#> 28  -2.3 0.09112\n#> 29  -2.2 0.09975\n#> 30  -2.1 0.10910\n#> 31  -2.0 0.11920\n#> 32  -1.9 0.13011\n#> 33  -1.8 0.14185\n#> 34  -1.7 0.15447\n#> 35  -1.6 0.16798\n#> 36  -1.5 0.18243\n#> 37  -1.4 0.19782\n#> 38  -1.3 0.21417\n#> 39  -1.2 0.23148\n#> 40  -1.1 0.24974\n#> 41  -1.0 0.26894\n#> 42  -0.9 0.28905\n#> 43  -0.8 0.31003\n#> 44  -0.7 0.33181\n#> 45  -0.6 0.35434\n#> 46  -0.5 0.37754\n#> 47  -0.4 0.40131\n#> 48  -0.3 0.42556\n#> 49  -0.2 0.45017\n#> 50  -0.1 0.47502\n#> 51   0.0 0.50000\n#> 52   0.1 0.52498\n#> 53   0.2 0.54983\n#> 54   0.3 0.57444\n#> 55   0.4 0.59869\n#> 56   0.5 0.62246\n#> 57   0.6 0.64566\n#> 58   0.7 0.66819\n#> 59   0.8 0.68997\n#> 60   0.9 0.71095\n#> 61   1.0 0.73106\n#> 62   1.1 0.75026\n#> 63   1.2 0.76852\n#> 64   1.3 0.78584\n#> 65   1.4 0.80218\n#> 66   1.5 0.81757\n#> 67   1.6 0.83202\n#> 68   1.7 0.84553\n#> 69   1.8 0.85815\n#> 70   1.9 0.86989\n#> 71   2.0 0.88080\n#> 72   2.1 0.89090\n#> 73   2.2 0.90025\n#> 74   2.3 0.90888\n#> 75   2.4 0.91683\n#> 76   2.5 0.92414\n#> 77   2.6 0.93086\n#> 78   2.7 0.93703\n#> 79   2.8 0.94268\n#> 80   2.9 0.94785\n#> 81   3.0 0.95257\n#> 82   3.1 0.95689\n#> 83   3.2 0.96083\n#> 84   3.3 0.96443\n#> 85   3.4 0.96770\n#> 86   3.5 0.97069\n#> 87   3.6 0.97340\n#> 88   3.7 0.97587\n#> 89   3.8 0.97812\n#> 90   3.9 0.98016\n#> 91   4.0 0.98201\n#> 92   4.1 0.98370\n#> 93   4.2 0.98523\n#> 94   4.3 0.98661\n#> 95   4.4 0.98787\n#> 96   4.5 0.98901\n#> 97   4.6 0.99005\n#> 98   4.7 0.99099\n#> 99   4.8 0.99184\n#> 100  4.9 0.99261\n#> 101  5.0 0.99331\nsigmoid = function(x) {\n   1 / (1 + exp(-x))\n}\n\nx <- seq(-5, 5, 0.01)\nplot(x, sigmoid(x), col = 'blue', cex = 0.5, main = \"Sigmoid\")"},{"path":"appendixB.html","id":"relu","chapter":"B Activation Functions","heading":"B.2 ReLU","text":"Using PyTorch relu() function:","code":"\nx <- torch$range(-5., 5., 0.1)\ny <- torch$relu(x)\n\ndf <- data.frame(x = x$numpy(), sx = y$numpy())\ndf\n\nggplot(df, aes(x = x, y = sx)) + \n    geom_point() +\n    ggtitle(\"ReLU\")"},{"path":"appendixB.html","id":"tanh","chapter":"B Activation Functions","heading":"B.3 tanh","text":"Using PyTorch tanh() function:","code":"\nx <- torch$range(-5., 5., 0.1)\ny <- torch$tanh(x)\n\ndf <- data.frame(x = x$numpy(), sx = y$numpy())\ndf\n\nggplot(df, aes(x = x, y = sx)) + \n    geom_point() +\n    ggtitle(\"tanh\")"},{"path":"appendixB.html","id":"softmax","chapter":"B Activation Functions","heading":"B.4 Softmax","text":"Using PyTorch softmax() function:","code":"\nx <- torch$range(-5.0, 5.0, 0.1)\ny <- torch$softmax(x, dim=0L)\n\ndf <- data.frame(x = x$numpy(), sx = y$numpy())\n\nggplot(df, aes(x = x, y = sx)) + \n    geom_point() +\n    ggtitle(\"Softmax\")"},{"path":"appendixB.html","id":"activation-functions-in-python","chapter":"B Activation Functions","heading":"B.5 Activation functions in Python","text":"","code":"\nlibrary(rTorch)import numpy as np\nimport matplotlib.pyplot as plt\nnp.random.seed(42)"},{"path":"appendixB.html","id":"linear-activation","chapter":"B Activation Functions","heading":"Linear activation","text":"","code":"def Linear(x, derivative=False):\n    \"\"\"\n    Computes the Linear activation function for array x\n    inputs:\n    x: array\n    derivative: if True, return the derivative else the forward pass\n    \"\"\"\n    \n    if derivative:              # Return derivative of the function at x\n        return np.ones_like(x)\n    else:                       # Return forward pass of the function at x\n        return x"},{"path":"appendixB.html","id":"sigmoid-activation","chapter":"B Activation Functions","heading":"Sigmoid activation","text":"","code":"def Sigmoid(x, derivative=False):\n    \"\"\"\n    Computes the Sigmoid activation function for array x\n    inputs:\n    x: array \n    derivative: if True, return the derivative else the forward pass\n    \"\"\"\n    f = 1/(1+np.exp(-x))\n    \n    if derivative:              # Return derivative of the function at x\n        return f*(1-f)\n    else:                       # Return forward pass of the function at x\n        return f"},{"path":"appendixB.html","id":"hyperbolic-tangent-activation","chapter":"B Activation Functions","heading":"Hyperbolic Tangent activation","text":"","code":"def Tanh(x, derivative=False):\n    \"\"\"\n    Computes the Hyperbolic Tangent activation function for array x\n    inputs:\n    x: array \n    derivative: if True, return the derivative else the forward pass\n    \"\"\"\n    f = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n    \n    if derivative:              # Return  derivative of the function at x\n        return 1-f**2\n    else:                       # Return the forward pass of the function at x\n        return f"},{"path":"appendixB.html","id":"rectifier-linear-unit-relu","chapter":"B Activation Functions","heading":"Rectifier linear unit (ReLU)","text":"","code":"def ReLU(x, derivative=False):\n    \"\"\"\n    Computes the Rectifier Linear Unit activation function for array x\n    inputs:\n    x: array\n    derivative: if True, return the derivative else the forward pass\n    \"\"\"\n    \n    if derivative:              # Return derivative of the function at x\n        return (x>0).astype(int)\n    else:                       # Return forward pass of the function at x\n        return np.maximum(x, 0)"},{"path":"appendixB.html","id":"visualization-with-matplotlib","chapter":"B Activation Functions","heading":"Visualization with matplotlib","text":"Plotting using matplotlib:","code":"x = np.linspace(-6, 6, 100)\nunits = {\n    \"Linear\": lambda x: Linear(x),\n    \"Sigmoid\": lambda x: Sigmoid(x),\n    \"ReLU\": lambda x: ReLU(x),\n    \"tanh\": lambda x: Tanh(x)\n}\n\nplt.figure(figsize=(5, 5))\n[plt.plot(x, unit(x), label=unit_name, lw=2) \n    for unit_name, unit in units.items()]plt.legend(loc=2, fontsize=16)\nplt.title('Activation functions', fontsize=20)\nplt.ylim([-2, 5])plt.xlim([-6, 6])plt.show()"},{"path":"appendixB.html","id":"softmax-code-in-python","chapter":"B Activation Functions","heading":"B.6 Softmax code in Python","text":"","code":"# Source: https://dataaspirant.com/2017/03/07/difference-between-softmax-function-and-sigmoid-function/\nimport numpy as np\nimport matplotlib.pyplot as plt\n \n \ndef softmax(inputs):\n    \"\"\"\n    Calculate the softmax for the give inputs (array)\n    :param inputs:\n    :return:\n    \"\"\"\n    return np.exp(inputs) / float(sum(np.exp(inputs)))\n \n \ndef line_graph(x, y, x_title, y_title):\n    \"\"\"\n    Draw line graph with x and y values\n    :param x:\n    :param y:\n    :param x_title:\n    :param y_title:\n    :return:\n    \"\"\"\n    plt.plot(x, y)\n    plt.xlabel(x_title)\n    plt.ylabel(y_title)\n    plt.show()\n \n \ngraph_x = np.linspace(-6, 6, 100)\ngraph_y = softmax(graph_x)\n \nprint(\"Graph X readings: {}\".format(graph_x))print(\"Graph Y readings: {}\".format(graph_y))\n line_graph(graph_x, graph_y, \"Inputs\", \"Softmax Scores\")"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
