<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 13 A neural network step-by-step | A Minimal rTorch Book</title>
<meta name="author" content="Alfonso R. Reyes">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.5.3/header-attrs.js"></script><script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.5.3/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.5.3/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.2.9000/tabs.js"></script><script src="libs/bs3compat-0.2.2.9000/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/htmlwidgets-1.5.2/htmlwidgets.js"></script><link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet">
<script src="libs/datatables-binding-0.16/datatables.js"></script><link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet">
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet">
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script><link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet">
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script><script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">A Minimal rTorch Book</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Prerequisites</a></li>
<li class="book-part">Getting Started</li>
<li><a class="" href="intro.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="pytorch-and-numpy.html"><span class="header-section-number">2</span> PyTorch and NumPy</a></li>
<li><a class="" href="rtorch-vs-pytorch.html"><span class="header-section-number">3</span> rTorch vs PyTorch</a></li>
<li><a class="" href="converting-tensors.html"><span class="header-section-number">4</span> Converting tensors</a></li>
<li class="book-part">Basic Tensor Operations</li>
<li><a class="" href="tensors.html"><span class="header-section-number">5</span> Tensors</a></li>
<li><a class="" href="linearalgebra.html"><span class="header-section-number">6</span> Linear Algebra with Torch</a></li>
<li><a class="" href="creating-pytorch-classes.html"><span class="header-section-number">7</span> Creating PyTorch classes</a></li>
<li class="book-part">Logistic Regression</li>
<li><a class="" href="example-1-a-classification-problem.html"><span class="header-section-number">8</span> Example 1: A classification problem</a></li>
<li><a class="" href="mnistdigits.html"><span class="header-section-number">9</span> Example 2: MNIST handwritten digits</a></li>
<li class="book-part">Linear Regression</li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">10</span> Linear Regression</a></li>
<li><a class="" href="linear-regression-1.html"><span class="header-section-number">11</span> Linear Regression</a></li>
<li class="book-part">Neural Networks</li>
<li><a class="" href="neural-networks.html"><span class="header-section-number">12</span> Neural Networks</a></li>
<li><a class="active" href="a-neural-network-step-by-step.html"><span class="header-section-number">13</span> A neural network step-by-step</a></li>
<li class="book-part">PyTorch and R data structures</li>
<li><a class="" href="working-with-a-dataframe.html"><span class="header-section-number">14</span> Working with a data●frame</a></li>
<li><a class="" href="working-with-datatable.html"><span class="header-section-number">15</span> Working with data●table</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="appendixA.html"><span class="header-section-number">A</span> Statistical Background</a></li>
<li><a class="" href="appendixB.html"><span class="header-section-number">B</span> Activation Functions</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/f0nzie/rtorch-minimal-book">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="a-neural-network-step-by-step" class="section level1" number="13">
<h1>
<span class="header-section-number">13</span> A neural network step-by-step<a class="anchor" aria-label="anchor" href="#a-neural-network-step-by-step"><i class="fas fa-link"></i></a>
</h1>
<p><em>Last update: Thu Oct 22 16:46:28 2020 -0500 (54a46ea04)</em></p>
<div id="introduction-1" class="section level2" number="13.1">
<h2>
<span class="header-section-number">13.1</span> Introduction<a class="anchor" aria-label="anchor" href="#introduction-1"><i class="fas fa-link"></i></a>
</h2>
<p>Source: <a href="https://github.com/jcjohnson/pytorch-examples#pytorch-nn" class="uri">https://github.com/jcjohnson/pytorch-examples#pytorch-nn</a></p>
<p>In this example we use the torch <code>nn</code> package to implement our two-layer network:</p>
</div>
<div id="select-device" class="section level2" number="13.2">
<h2>
<span class="header-section-number">13.2</span> Select device<a class="anchor" aria-label="anchor" href="#select-device"><i class="fas fa-link"></i></a>
</h2>
<div class="sourceCode" id="cb558"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/f0nzie/rTorch">rTorch</a></span><span class="op">)</span>

<span class="va">device</span> <span class="op">=</span> <span class="va">torch</span><span class="op">$</span><span class="fu">device</span><span class="op">(</span><span class="st">'cpu'</span><span class="op">)</span>
<span class="co"># device = torch.device('cuda') # Uncomment this to run on GPU</span></code></pre></div>
<ul>
<li>
<code>N</code> is batch size;</li>
<li>
<code>D_in</code> is input dimension;</li>
<li>
<code>H</code> is hidden dimension;</li>
<li>
<code>D_out</code> is output dimension.</li>
</ul>
</div>
<div id="create-the-dataset" class="section level2" number="13.3">
<h2>
<span class="header-section-number">13.3</span> Create the dataset<a class="anchor" aria-label="anchor" href="#create-the-dataset"><i class="fas fa-link"></i></a>
</h2>
<div class="sourceCode" id="cb559"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/invisible.html">invisible</a></span><span class="op">(</span><span class="va">torch</span><span class="op">$</span><span class="fu">manual_seed</span><span class="op">(</span><span class="fl">0</span><span class="op">)</span><span class="op">)</span>   <span class="co"># do not show the generator output</span>
<span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">64L</span>; <span class="va">D_in</span> <span class="op">&lt;-</span> <span class="fl">1000L</span>; <span class="va">H</span> <span class="op">&lt;-</span> <span class="fl">100L</span>; <span class="va">D_out</span> <span class="op">&lt;-</span> <span class="fl">10L</span>

<span class="co"># Create random Tensors to hold inputs and outputs</span>
<span class="va">x</span> <span class="op">=</span> <span class="va">torch</span><span class="op">$</span><span class="fu">randn</span><span class="op">(</span><span class="va">N</span>, <span class="va">D_in</span>, device<span class="op">=</span><span class="va">device</span><span class="op">)</span>
<span class="va">y</span> <span class="op">=</span> <span class="va">torch</span><span class="op">$</span><span class="fu">randn</span><span class="op">(</span><span class="va">N</span>, <span class="va">D_out</span>, device<span class="op">=</span><span class="va">device</span><span class="op">)</span></code></pre></div>
</div>
<div id="define-the-model-1" class="section level2" number="13.4">
<h2>
<span class="header-section-number">13.4</span> Define the model<a class="anchor" aria-label="anchor" href="#define-the-model-1"><i class="fas fa-link"></i></a>
</h2>
<p>We use the <code>nn</code> package to define our model as a sequence of layers. <code>nn.Sequential</code> applies these leayers in sequence to produce an output. Each <em>Linear Module</em> computes the output by using a linear function, and holds also tensors for its weights and biases. After constructing the model we use the <code>.to()</code> method to move it to the desired device, which could be <code>CPU</code> or <code>GPU</code>. Remember that we selected <code>CPU</code> with <code>torch$device('cpu')</code>.</p>
<div class="sourceCode" id="cb560"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">model</span> <span class="op">&lt;-</span> <span class="va">torch</span><span class="op">$</span><span class="va">nn</span><span class="op">$</span><span class="fu">Sequential</span><span class="op">(</span>
  <span class="va">torch</span><span class="op">$</span><span class="va">nn</span><span class="op">$</span><span class="fu">Linear</span><span class="op">(</span><span class="va">D_in</span>, <span class="va">H</span><span class="op">)</span>,              <span class="co"># first layer</span>
  <span class="va">torch</span><span class="op">$</span><span class="va">nn</span><span class="op">$</span><span class="fu">ReLU</span><span class="op">(</span><span class="op">)</span>,
  <span class="va">torch</span><span class="op">$</span><span class="va">nn</span><span class="op">$</span><span class="fu">Linear</span><span class="op">(</span><span class="va">H</span>, <span class="va">D_out</span><span class="op">)</span><span class="op">)</span><span class="op">$</span><span class="fu">to</span><span class="op">(</span><span class="va">device</span><span class="op">)</span>  <span class="co"># output layer</span>

<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span></code></pre></div>
<pre><code>#&gt; Sequential(
#&gt;   (0): Linear(in_features=1000, out_features=100, bias=True)
#&gt;   (1): ReLU()
#&gt;   (2): Linear(in_features=100, out_features=10, bias=True)
#&gt; )</code></pre>
</div>
<div id="the-loss-function" class="section level2" number="13.5">
<h2>
<span class="header-section-number">13.5</span> The Loss function<a class="anchor" aria-label="anchor" href="#the-loss-function"><i class="fas fa-link"></i></a>
</h2>
<p>The <code>nn</code> package also contains definitions of several loss functions; in this case we will use <strong>Mean Squared Error</strong> (<span class="math inline">\(MSE\)</span>) as our loss function. Setting <code>reduction='sum'</code> means that we are computing the <em>sum</em> of squared errors rather than the <strong>mean</strong>; this is for consistency with the examples above where we manually compute the loss, but in practice it is more common to use the mean squared error as a loss by setting <code>reduction='elementwise_mean'</code>.</p>
<div class="sourceCode" id="cb562"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">loss_fn</span> <span class="op">=</span> <span class="va">torch</span><span class="op">$</span><span class="va">nn</span><span class="op">$</span><span class="fu">MSELoss</span><span class="op">(</span>reduction <span class="op">=</span> <span class="st">'sum'</span><span class="op">)</span></code></pre></div>
</div>
<div id="iterate-through-the-dataset-1" class="section level2" number="13.6">
<h2>
<span class="header-section-number">13.6</span> Iterate through the dataset<a class="anchor" aria-label="anchor" href="#iterate-through-the-dataset-1"><i class="fas fa-link"></i></a>
</h2>
<div class="sourceCode" id="cb563"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">learning_rate</span> <span class="op">=</span> <span class="fl">1e-4</span>

<span class="kw">for</span> <span class="op">(</span><span class="va">t</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">500</span><span class="op">)</span> <span class="op">{</span>
  <span class="co"># Forward pass: compute predicted y by passing x to the model. Module objects</span>
  <span class="co"># override the __call__ operator so you can call them like functions. When</span>
  <span class="co"># doing so you pass a Tensor of input data to the Module and it produces</span>
  <span class="co"># a Tensor of output data.</span>
  <span class="va">y_pred</span> <span class="op">=</span> <span class="fu">model</span><span class="op">(</span><span class="va">x</span><span class="op">)</span>

  <span class="co"># Compute and print loss. We pass Tensors containing the predicted and true</span>
  <span class="co"># values of y, and the loss function returns a Tensor containing the loss.</span>
  <span class="va">loss</span> <span class="op">=</span> <span class="fu">loss_fn</span><span class="op">(</span><span class="va">y_pred</span>, <span class="va">y</span><span class="op">)</span>
  <span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="va">t</span>, <span class="st">"\t"</span><span class="op">)</span>
  <span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="va">loss</span><span class="op">$</span><span class="fu">item</span><span class="op">(</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span>
  
  <span class="co"># Zero the gradients before running the backward pass.</span>
  <span class="va">model</span><span class="op">$</span><span class="fu">zero_grad</span><span class="op">(</span><span class="op">)</span>

  <span class="co"># Backward pass: compute gradient of the loss with respect to all the learnable</span>
  <span class="co"># parameters of the model. Internally, the parameters of each Module are stored</span>
  <span class="co"># in Tensors with requires_grad=True, so this call will compute gradients for</span>
  <span class="co"># all learnable parameters in the model.</span>
  <span class="va">loss</span><span class="op">$</span><span class="fu">backward</span><span class="op">(</span><span class="op">)</span>

  <span class="co"># Update the weights using gradient descent. Each parameter is a Tensor, so</span>
  <span class="co"># we can access its data and gradients like we did before.</span>
  <span class="fu"><a href="https://rdrr.io/r/base/with.html">with</a></span><span class="op">(</span><span class="va">torch</span><span class="op">$</span><span class="fu">no_grad</span><span class="op">(</span><span class="op">)</span>, <span class="op">{</span>
      <span class="kw">for</span> <span class="op">(</span><span class="va">param</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/pkg/reticulate/man/iterate.html">iterate</a></span><span class="op">(</span><span class="va">model</span><span class="op">$</span><span class="fu">parameters</span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span>
        <span class="co"># in Python this code is much simpler. In R we have to do some conversions</span>
        <span class="co"># param$data &lt;- torch$sub(param$data,</span>
        <span class="co">#                         torch$mul(param$grad$float(),</span>
        <span class="co">#                           torch$scalar_tensor(learning_rate)))</span>
        <span class="va">param</span><span class="op">$</span><span class="va">data</span> <span class="op">&lt;-</span> <span class="va">param</span><span class="op">$</span><span class="va">data</span> <span class="op">-</span> <span class="va">param</span><span class="op">$</span><span class="va">grad</span> <span class="op">*</span> <span class="va">learning_rate</span>
      <span class="op">}</span>
   <span class="op">}</span><span class="op">)</span>
<span class="op">}</span>  </code></pre></div>
<pre><code>#&gt; 1    628 
#&gt; 2    585 
#&gt; 3    547 
#&gt; 4    513 
#&gt; 5    482 
#&gt; 6    455 
#&gt; 7    430 
#&gt; 8    406 
#&gt; 9    385 
#&gt; 10   364 
#&gt; 11   345 
#&gt; 12   328 
#&gt; 13   311 
#&gt; 14   295 
#&gt; 15   280 
#&gt; 16   265 
#&gt; 17   252 
#&gt; 18   239 
#&gt; 19   226 
#&gt; 20   214 
#&gt; 21   203 
#&gt; 22   192 
#&gt; 23   181 
#&gt; 24   172 
#&gt; 25   162 
#&gt; 26   153 
#&gt; 27   145 
#&gt; 28   137 
#&gt; 29   129 
#&gt; 30   122 
#&gt; 31   115 
#&gt; 32   109 
#&gt; 33   103 
#&gt; 34   96.9 
#&gt; 35   91.5 
#&gt; 36   86.3 
#&gt; 37   81.5 
#&gt; 38   76.9 
#&gt; 39   72.6 
#&gt; 40   68.5 
#&gt; 41   64.6 
#&gt; 42   61 
#&gt; 43   57.6 
#&gt; 44   54.3 
#&gt; 45   51.3 
#&gt; 46   48.5 
#&gt; 47   45.8 
#&gt; 48   43.2 
#&gt; 49   40.9 
#&gt; 50   38.6 
#&gt; 51   36.5 
#&gt; 52   34.5 
#&gt; 53   32.7 
#&gt; 54   30.9 
#&gt; 55   29.3 
#&gt; 56   27.8 
#&gt; 57   26.3 
#&gt; 58   24.9 
#&gt; 59   23.7 
#&gt; 60   22.4 
#&gt; 61   21.3 
#&gt; 62   20.2 
#&gt; 63   19.2 
#&gt; 64   18.2 
#&gt; 65   17.3 
#&gt; 66   16.5 
#&gt; 67   15.7 
#&gt; 68   14.9 
#&gt; 69   14.2 
#&gt; 70   13.5 
#&gt; 71   12.9 
#&gt; 72   12.3 
#&gt; 73   11.7 
#&gt; 74   11.1 
#&gt; 75   10.6 
#&gt; 76   10.1 
#&gt; 77   9.67 
#&gt; 78   9.24 
#&gt; 79   8.82 
#&gt; 80   8.42 
#&gt; 81   8.05 
#&gt; 82   7.69 
#&gt; 83   7.35 
#&gt; 84   7.03 
#&gt; 85   6.72 
#&gt; 86   6.43 
#&gt; 87   6.16 
#&gt; 88   5.9 
#&gt; 89   5.65 
#&gt; 90   5.41 
#&gt; 91   5.18 
#&gt; 92   4.97 
#&gt; 93   4.76 
#&gt; 94   4.57 
#&gt; 95   4.38 
#&gt; 96   4.2 
#&gt; 97   4.03 
#&gt; 98   3.87 
#&gt; 99   3.72 
#&gt; 100  3.57 
#&gt; 101  3.43 
#&gt; 102  3.29 
#&gt; 103  3.17 
#&gt; 104  3.04 
#&gt; 105  2.92 
#&gt; 106  2.81 
#&gt; 107  2.7 
#&gt; 108  2.6 
#&gt; 109  2.5 
#&gt; 110  2.41 
#&gt; 111  2.31 
#&gt; 112  2.23 
#&gt; 113  2.14 
#&gt; 114  2.06 
#&gt; 115  1.99 
#&gt; 116  1.91 
#&gt; 117  1.84 
#&gt; 118  1.77 
#&gt; 119  1.71 
#&gt; 120  1.65 
#&gt; 121  1.59 
#&gt; 122  1.53 
#&gt; 123  1.47 
#&gt; 124  1.42 
#&gt; 125  1.37 
#&gt; 126  1.32 
#&gt; 127  1.27 
#&gt; 128  1.23 
#&gt; 129  1.18 
#&gt; 130  1.14 
#&gt; 131  1.1 
#&gt; 132  1.06 
#&gt; 133  1.02 
#&gt; 134  0.989 
#&gt; 135  0.954 
#&gt; 136  0.921 
#&gt; 137  0.889 
#&gt; 138  0.858 
#&gt; 139  0.828 
#&gt; 140  0.799 
#&gt; 141  0.772 
#&gt; 142  0.745 
#&gt; 143  0.719 
#&gt; 144  0.695 
#&gt; 145  0.671 
#&gt; 146  0.648 
#&gt; 147  0.626 
#&gt; 148  0.605 
#&gt; 149  0.584 
#&gt; 150  0.564 
#&gt; 151  0.545 
#&gt; 152  0.527 
#&gt; 153  0.509 
#&gt; 154  0.492 
#&gt; 155  0.476 
#&gt; 156  0.46 
#&gt; 157  0.444 
#&gt; 158  0.43 
#&gt; 159  0.415 
#&gt; 160  0.402 
#&gt; 161  0.388 
#&gt; 162  0.375 
#&gt; 163  0.363 
#&gt; 164  0.351 
#&gt; 165  0.339 
#&gt; 166  0.328 
#&gt; 167  0.318 
#&gt; 168  0.307 
#&gt; 169  0.297 
#&gt; 170  0.287 
#&gt; 171  0.278 
#&gt; 172  0.269 
#&gt; 173  0.26 
#&gt; 174  0.252 
#&gt; 175  0.244 
#&gt; 176  0.236 
#&gt; 177  0.228 
#&gt; 178  0.221 
#&gt; 179  0.214 
#&gt; 180  0.207 
#&gt; 181  0.2 
#&gt; 182  0.194 
#&gt; 183  0.187 
#&gt; 184  0.181 
#&gt; 185  0.176 
#&gt; 186  0.17 
#&gt; 187  0.165 
#&gt; 188  0.159 
#&gt; 189  0.154 
#&gt; 190  0.149 
#&gt; 191  0.145 
#&gt; 192  0.14 
#&gt; 193  0.136 
#&gt; 194  0.131 
#&gt; 195  0.127 
#&gt; 196  0.123 
#&gt; 197  0.119 
#&gt; 198  0.115 
#&gt; 199  0.112 
#&gt; 200  0.108 
#&gt; 201  0.105 
#&gt; 202  0.102 
#&gt; 203  0.0983 
#&gt; 204  0.0952 
#&gt; 205  0.0923 
#&gt; 206  0.0894 
#&gt; 207  0.0866 
#&gt; 208  0.0838 
#&gt; 209  0.0812 
#&gt; 210  0.0787 
#&gt; 211  0.0762 
#&gt; 212  0.0739 
#&gt; 213  0.0716 
#&gt; 214  0.0693 
#&gt; 215  0.0672 
#&gt; 216  0.0651 
#&gt; 217  0.0631 
#&gt; 218  0.0611 
#&gt; 219  0.0592 
#&gt; 220  0.0574 
#&gt; 221  0.0556 
#&gt; 222  0.0539 
#&gt; 223  0.0522 
#&gt; 224  0.0506 
#&gt; 225  0.0491 
#&gt; 226  0.0476 
#&gt; 227  0.0461 
#&gt; 228  0.0447 
#&gt; 229  0.0433 
#&gt; 230  0.042 
#&gt; 231  0.0407 
#&gt; 232  0.0394 
#&gt; 233  0.0382 
#&gt; 234  0.0371 
#&gt; 235  0.0359 
#&gt; 236  0.0348 
#&gt; 237  0.0338 
#&gt; 238  0.0327 
#&gt; 239  0.0317 
#&gt; 240  0.0308 
#&gt; 241  0.0298 
#&gt; 242  0.0289 
#&gt; 243  0.028 
#&gt; 244  0.0272 
#&gt; 245  0.0263 
#&gt; 246  0.0255 
#&gt; 247  0.0248 
#&gt; 248  0.024 
#&gt; 249  0.0233 
#&gt; 250  0.0226 
#&gt; 251  0.0219 
#&gt; 252  0.0212 
#&gt; 253  0.0206 
#&gt; 254  0.02 
#&gt; 255  0.0194 
#&gt; 256  0.0188 
#&gt; 257  0.0182 
#&gt; 258  0.0177 
#&gt; 259  0.0171 
#&gt; 260  0.0166 
#&gt; 261  0.0161 
#&gt; 262  0.0156 
#&gt; 263  0.0151 
#&gt; 264  0.0147 
#&gt; 265  0.0142 
#&gt; 266  0.0138 
#&gt; 267  0.0134 
#&gt; 268  0.013 
#&gt; 269  0.0126 
#&gt; 270  0.0122 
#&gt; 271  0.0119 
#&gt; 272  0.0115 
#&gt; 273  0.0112 
#&gt; 274  0.0108 
#&gt; 275  0.0105 
#&gt; 276  0.0102 
#&gt; 277  0.00988 
#&gt; 278  0.00959 
#&gt; 279  0.0093 
#&gt; 280  0.00902 
#&gt; 281  0.00875 
#&gt; 282  0.00849 
#&gt; 283  0.00824 
#&gt; 284  0.00799 
#&gt; 285  0.00775 
#&gt; 286  0.00752 
#&gt; 287  0.0073 
#&gt; 288  0.00708 
#&gt; 289  0.00687 
#&gt; 290  0.00666 
#&gt; 291  0.00647 
#&gt; 292  0.00627 
#&gt; 293  0.00609 
#&gt; 294  0.00591 
#&gt; 295  0.00573 
#&gt; 296  0.00556 
#&gt; 297  0.0054 
#&gt; 298  0.00524 
#&gt; 299  0.00508 
#&gt; 300  0.00493 
#&gt; 301  0.00478 
#&gt; 302  0.00464 
#&gt; 303  0.0045 
#&gt; 304  0.00437 
#&gt; 305  0.00424 
#&gt; 306  0.00412 
#&gt; 307  0.00399 
#&gt; 308  0.00388 
#&gt; 309  0.00376 
#&gt; 310  0.00365 
#&gt; 311  0.00354 
#&gt; 312  0.00344 
#&gt; 313  0.00334 
#&gt; 314  0.00324 
#&gt; 315  0.00314 
#&gt; 316  0.00305 
#&gt; 317  0.00296 
#&gt; 318  0.00287 
#&gt; 319  0.00279 
#&gt; 320  0.00271 
#&gt; 321  0.00263 
#&gt; 322  0.00255 
#&gt; 323  0.00248 
#&gt; 324  0.0024 
#&gt; 325  0.00233 
#&gt; 326  0.00226 
#&gt; 327  0.0022 
#&gt; 328  0.00213 
#&gt; 329  0.00207 
#&gt; 330  0.00201 
#&gt; 331  0.00195 
#&gt; 332  0.00189 
#&gt; 333  0.00184 
#&gt; 334  0.00178 
#&gt; 335  0.00173 
#&gt; 336  0.00168 
#&gt; 337  0.00163 
#&gt; 338  0.00158 
#&gt; 339  0.00154 
#&gt; 340  0.00149 
#&gt; 341  0.00145 
#&gt; 342  0.00141 
#&gt; 343  0.00137 
#&gt; 344  0.00133 
#&gt; 345  0.00129 
#&gt; 346  0.00125 
#&gt; 347  0.00121 
#&gt; 348  0.00118 
#&gt; 349  0.00114 
#&gt; 350  0.00111 
#&gt; 351  0.00108 
#&gt; 352  0.00105 
#&gt; 353  0.00102 
#&gt; 354  0.000987 
#&gt; 355  0.000958 
#&gt; 356  0.000931 
#&gt; 357  0.000904 
#&gt; 358  0.000877 
#&gt; 359  0.000852 
#&gt; 360  0.000827 
#&gt; 361  0.000803 
#&gt; 362  0.00078 
#&gt; 363  0.000757 
#&gt; 364  0.000735 
#&gt; 365  0.000714 
#&gt; 366  0.000693 
#&gt; 367  0.000673 
#&gt; 368  0.000654 
#&gt; 369  0.000635 
#&gt; 370  0.000617 
#&gt; 371  0.000599 
#&gt; 372  0.000581 
#&gt; 373  0.000565 
#&gt; 374  0.000548 
#&gt; 375  0.000532 
#&gt; 376  0.000517 
#&gt; 377  0.000502 
#&gt; 378  0.000488 
#&gt; 379  0.000474 
#&gt; 380  0.00046 
#&gt; 381  0.000447 
#&gt; 382  0.000434 
#&gt; 383  0.000421 
#&gt; 384  0.000409 
#&gt; 385  0.000397 
#&gt; 386  0.000386 
#&gt; 387  0.000375 
#&gt; 388  0.000364 
#&gt; 389  0.000354 
#&gt; 390  0.000343 
#&gt; 391  0.000334 
#&gt; 392  0.000324 
#&gt; 393  0.000315 
#&gt; 394  0.000306 
#&gt; 395  0.000297 
#&gt; 396  0.000288 
#&gt; 397  0.00028 
#&gt; 398  0.000272 
#&gt; 399  0.000264 
#&gt; 400  0.000257 
#&gt; 401  0.000249 
#&gt; 402  0.000242 
#&gt; 403  0.000235 
#&gt; 404  0.000228 
#&gt; 405  0.000222 
#&gt; 406  0.000216 
#&gt; 407  0.000209 
#&gt; 408  0.000203 
#&gt; 409  0.000198 
#&gt; 410  0.000192 
#&gt; 411  0.000186 
#&gt; 412  0.000181 
#&gt; 413  0.000176 
#&gt; 414  0.000171 
#&gt; 415  0.000166 
#&gt; 416  0.000161 
#&gt; 417  0.000157 
#&gt; 418  0.000152 
#&gt; 419  0.000148 
#&gt; 420  0.000144 
#&gt; 421  0.00014 
#&gt; 422  0.000136 
#&gt; 423  0.000132 
#&gt; 424  0.000128 
#&gt; 425  0.000124 
#&gt; 426  0.000121 
#&gt; 427  0.000117 
#&gt; 428  0.000114 
#&gt; 429  0.000111 
#&gt; 430  0.000108 
#&gt; 431  0.000105 
#&gt; 432  0.000102 
#&gt; 433  9.87e-05 
#&gt; 434  9.59e-05 
#&gt; 435  9.32e-05 
#&gt; 436  9.06e-05 
#&gt; 437  8.8e-05 
#&gt; 438  8.55e-05 
#&gt; 439  8.31e-05 
#&gt; 440  8.07e-05 
#&gt; 441  7.84e-05 
#&gt; 442  7.62e-05 
#&gt; 443  7.4e-05 
#&gt; 444  7.2e-05 
#&gt; 445  6.99e-05 
#&gt; 446  6.79e-05 
#&gt; 447  6.6e-05 
#&gt; 448  6.41e-05 
#&gt; 449  6.23e-05 
#&gt; 450  6.06e-05 
#&gt; 451  5.89e-05 
#&gt; 452  5.72e-05 
#&gt; 453  5.56e-05 
#&gt; 454  5.4e-05 
#&gt; 455  5.25e-05 
#&gt; 456  5.1e-05 
#&gt; 457  4.96e-05 
#&gt; 458  4.82e-05 
#&gt; 459  4.68e-05 
#&gt; 460  4.55e-05 
#&gt; 461  4.42e-05 
#&gt; 462  4.3e-05 
#&gt; 463  4.18e-05 
#&gt; 464  4.06e-05 
#&gt; 465  3.94e-05 
#&gt; 466  3.83e-05 
#&gt; 467  3.72e-05 
#&gt; 468  3.62e-05 
#&gt; 469  3.52e-05 
#&gt; 470  3.42e-05 
#&gt; 471  3.32e-05 
#&gt; 472  3.23e-05 
#&gt; 473  3.14e-05 
#&gt; 474  3.05e-05 
#&gt; 475  2.96e-05 
#&gt; 476  2.88e-05 
#&gt; 477  2.8e-05 
#&gt; 478  2.72e-05 
#&gt; 479  2.65e-05 
#&gt; 480  2.57e-05 
#&gt; 481  2.5e-05 
#&gt; 482  2.43e-05 
#&gt; 483  2.36e-05 
#&gt; 484  2.29e-05 
#&gt; 485  2.23e-05 
#&gt; 486  2.17e-05 
#&gt; 487  2.11e-05 
#&gt; 488  2.05e-05 
#&gt; 489  1.99e-05 
#&gt; 490  1.94e-05 
#&gt; 491  1.88e-05 
#&gt; 492  1.83e-05 
#&gt; 493  1.78e-05 
#&gt; 494  1.73e-05 
#&gt; 495  1.68e-05 
#&gt; 496  1.63e-05 
#&gt; 497  1.59e-05 
#&gt; 498  1.54e-05 
#&gt; 499  1.5e-05 
#&gt; 500  1.46e-05</code></pre>
</div>
<div id="using-r-generics" class="section level2" number="13.7">
<h2>
<span class="header-section-number">13.7</span> Using R generics<a class="anchor" aria-label="anchor" href="#using-r-generics"><i class="fas fa-link"></i></a>
</h2>
<div id="simplify-tensor-operations" class="section level3" number="13.7.1">
<h3>
<span class="header-section-number">13.7.1</span> Simplify tensor operations<a class="anchor" aria-label="anchor" href="#simplify-tensor-operations"><i class="fas fa-link"></i></a>
</h3>
<p>The following two expressions are equivalent, with the first being the long version natural way of doing it in <strong>PyTorch</strong>. The second is using the generics in R for subtraction, multiplication and scalar conversion.</p>
<div class="sourceCode" id="cb565"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">param</span><span class="op">$</span><span class="va">data</span> <span class="op">&lt;-</span> <span class="va">torch</span><span class="op">$</span><span class="fu">sub</span><span class="op">(</span><span class="va">param</span><span class="op">$</span><span class="va">data</span>,
                        <span class="va">torch</span><span class="op">$</span><span class="fu">mul</span><span class="op">(</span><span class="va">param</span><span class="op">$</span><span class="va">grad</span><span class="op">$</span><span class="fu">float</span><span class="op">(</span><span class="op">)</span>,
                          <span class="va">torch</span><span class="op">$</span><span class="fu">scalar_tensor</span><span class="op">(</span><span class="va">learning_rate</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb566"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">param</span><span class="op">$</span><span class="va">data</span> <span class="op">&lt;-</span> <span class="va">param</span><span class="op">$</span><span class="va">data</span> <span class="op">-</span> <span class="va">param</span><span class="op">$</span><span class="va">grad</span> <span class="op">*</span> <span class="va">learning_rate</span></code></pre></div>
</div>
</div>
<div id="an-elegant-neural-network" class="section level2" number="13.8">
<h2>
<span class="header-section-number">13.8</span> An elegant neural network<a class="anchor" aria-label="anchor" href="#an-elegant-neural-network"><i class="fas fa-link"></i></a>
</h2>
<div class="sourceCode" id="cb567"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/invisible.html">invisible</a></span><span class="op">(</span><span class="va">torch</span><span class="op">$</span><span class="fu">manual_seed</span><span class="op">(</span><span class="fl">0</span><span class="op">)</span><span class="op">)</span>   <span class="co"># do not show the generator output</span>
<span class="co"># layer properties</span>
<span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">64L</span>; <span class="va">D_in</span> <span class="op">&lt;-</span> <span class="fl">1000L</span>; <span class="va">H</span> <span class="op">&lt;-</span> <span class="fl">100L</span>; <span class="va">D_out</span> <span class="op">&lt;-</span> <span class="fl">10L</span>

<span class="co"># Create random Tensors to hold inputs and outputs</span>
<span class="va">x</span> <span class="op">=</span> <span class="va">torch</span><span class="op">$</span><span class="fu">randn</span><span class="op">(</span><span class="va">N</span>, <span class="va">D_in</span>, device<span class="op">=</span><span class="va">device</span><span class="op">)</span>
<span class="va">y</span> <span class="op">=</span> <span class="va">torch</span><span class="op">$</span><span class="fu">randn</span><span class="op">(</span><span class="va">N</span>, <span class="va">D_out</span>, device<span class="op">=</span><span class="va">device</span><span class="op">)</span>

<span class="co"># set up the neural network</span>
<span class="va">model</span> <span class="op">&lt;-</span> <span class="va">torch</span><span class="op">$</span><span class="va">nn</span><span class="op">$</span><span class="fu">Sequential</span><span class="op">(</span>
  <span class="va">torch</span><span class="op">$</span><span class="va">nn</span><span class="op">$</span><span class="fu">Linear</span><span class="op">(</span><span class="va">D_in</span>, <span class="va">H</span><span class="op">)</span>,              <span class="co"># first layer</span>
  <span class="va">torch</span><span class="op">$</span><span class="va">nn</span><span class="op">$</span><span class="fu">ReLU</span><span class="op">(</span><span class="op">)</span>,                       <span class="co"># activation</span>
  <span class="va">torch</span><span class="op">$</span><span class="va">nn</span><span class="op">$</span><span class="fu">Linear</span><span class="op">(</span><span class="va">H</span>, <span class="va">D_out</span><span class="op">)</span><span class="op">)</span><span class="op">$</span><span class="fu">to</span><span class="op">(</span><span class="va">device</span><span class="op">)</span>  <span class="co"># output layer</span>

<span class="co"># specify how we will be computing the loss</span>
<span class="va">loss_fn</span> <span class="op">=</span> <span class="va">torch</span><span class="op">$</span><span class="va">nn</span><span class="op">$</span><span class="fu">MSELoss</span><span class="op">(</span>reduction <span class="op">=</span> <span class="st">'sum'</span><span class="op">)</span>

<span class="va">learning_rate</span> <span class="op">=</span> <span class="fl">1e-4</span>
<span class="va">loss_row</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/vector.html">vector</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span>     <span class="co"># collect a list for the final dataframe</span>

<span class="kw">for</span> <span class="op">(</span><span class="va">t</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">500</span><span class="op">)</span> <span class="op">{</span>
  <span class="co"># Forward pass: compute predicted y by passing x to the model. Module objects</span>
  <span class="co"># override the __call__ operator so you can call them like functions. When</span>
  <span class="co"># doing so you pass a Tensor of input data to the Module and it produces</span>
  <span class="co"># a Tensor of output data.</span>
  <span class="va">y_pred</span> <span class="op">=</span> <span class="fu">model</span><span class="op">(</span><span class="va">x</span><span class="op">)</span>

  <span class="co"># Compute and print loss. We pass Tensors containing the predicted and true</span>
  <span class="co"># values of y, and the loss function returns a Tensor containing the loss.</span>
  <span class="va">loss</span> <span class="op">=</span> <span class="fu">loss_fn</span><span class="op">(</span><span class="va">y_pred</span>, <span class="va">y</span><span class="op">)</span>  <span class="co"># (y_pred - y) is a tensor; loss_fn output is a scalar</span>
  <span class="va">loss_row</span><span class="op">[[</span><span class="va">t</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">t</span>, <span class="va">loss</span><span class="op">$</span><span class="fu">item</span><span class="op">(</span><span class="op">)</span><span class="op">)</span>
  
  <span class="co"># Zero the gradients before running the backward pass.</span>
  <span class="va">model</span><span class="op">$</span><span class="fu">zero_grad</span><span class="op">(</span><span class="op">)</span>

  <span class="co"># Backward pass: compute gradient of the loss with respect to all the learnable</span>
  <span class="co"># parameters of the model. Internally, the parameters of each module are stored</span>
  <span class="co"># in tensors with `requires_grad=True`, so this call will compute gradients for</span>
  <span class="co"># all learnable parameters in the model.</span>
  <span class="va">loss</span><span class="op">$</span><span class="fu">backward</span><span class="op">(</span><span class="op">)</span>

  <span class="co"># Update the weights using gradient descent. Each parameter is a tensor, so</span>
  <span class="co"># we can access its data and gradients like we did before.</span>
  <span class="fu"><a href="https://rdrr.io/r/base/with.html">with</a></span><span class="op">(</span><span class="va">torch</span><span class="op">$</span><span class="fu">no_grad</span><span class="op">(</span><span class="op">)</span>, <span class="op">{</span>
      <span class="kw">for</span> <span class="op">(</span><span class="va">param</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/pkg/reticulate/man/iterate.html">iterate</a></span><span class="op">(</span><span class="va">model</span><span class="op">$</span><span class="fu">parameters</span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span>
        <span class="co"># using R generics</span>
        <span class="va">param</span><span class="op">$</span><span class="va">data</span> <span class="op">&lt;-</span> <span class="va">param</span><span class="op">$</span><span class="va">data</span> <span class="op">-</span> <span class="va">param</span><span class="op">$</span><span class="va">grad</span> <span class="op">*</span> <span class="va">learning_rate</span>
      <span class="op">}</span>
   <span class="op">}</span><span class="op">)</span>
<span class="op">}</span>  </code></pre></div>
</div>
<div id="a-browseable-dataframe" class="section level2" number="13.9">
<h2>
<span class="header-section-number">13.9</span> A browseable dataframe<a class="anchor" aria-label="anchor" href="#a-browseable-dataframe"><i class="fas fa-link"></i></a>
</h2>
<div class="sourceCode" id="cb568"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/rstudio/DT">DT</a></span><span class="op">)</span>
<span class="va">loss_df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/funprog.html">Reduce</a></span><span class="op">(</span><span class="va">rbind</span>, <span class="va">loss_row</span><span class="op">)</span>, row.names <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">loss_df</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="st">"iter"</span>
<span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">loss_df</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="st">"loss"</span>
<span class="fu">DT</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/DT/man/datatable.html">datatable</a></span><span class="op">(</span><span class="va">loss_df</span><span class="op">)</span></code></pre></div>
<div id="htmlwidget-1b4ff99564eb6e8884a5" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-1b4ff99564eb6e8884a5">{"x":{"filter":"none","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153","154","155","156","157","158","159","160","161","162","163","164","165","166","167","168","169","170","171","172","173","174","175","176","177","178","179","180","181","182","183","184","185","186","187","188","189","190","191","192","193","194","195","196","197","198","199","200","201","202","203","204","205","206","207","208","209","210","211","212","213","214","215","216","217","218","219","220","221","222","223","224","225","226","227","228","229","230","231","232","233","234","235","236","237","238","239","240","241","242","243","244","245","246","247","248","249","250","251","252","253","254","255","256","257","258","259","260","261","262","263","264","265","266","267","268","269","270","271","272","273","274","275","276","277","278","279","280","281","282","283","284","285","286","287","288","289","290","291","292","293","294","295","296","297","298","299","300","301","302","303","304","305","306","307","308","309","310","311","312","313","314","315","316","317","318","319","320","321","322","323","324","325","326","327","328","329","330","331","332","333","334","335","336","337","338","339","340","341","342","343","344","345","346","347","348","349","350","351","352","353","354","355","356","357","358","359","360","361","362","363","364","365","366","367","368","369","370","371","372","373","374","375","376","377","378","379","380","381","382","383","384","385","386","387","388","389","390","391","392","393","394","395","396","397","398","399","400","401","402","403","404","405","406","407","408","409","410","411","412","413","414","415","416","417","418","419","420","421","422","423","424","425","426","427","428","429","430","431","432","433","434","435","436","437","438","439","440","441","442","443","444","445","446","447","448","449","450","451","452","453","454","455","456","457","458","459","460","461","462","463","464","465","466","467","468","469","470","471","472","473","474","475","476","477","478","479","480","481","482","483","484","485","486","487","488","489","490","491","492","493","494","495","496","497","498","499","500"],[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500],[628.283874511719,584.959289550781,546.727966308594,512.688537597656,482.130554199219,454.626525878906,429.55517578125,406.398956298828,384.64404296875,364.312469482422,345.386596679688,327.625793457031,310.800231933594,294.837890625,279.732452392578,265.307952880859,251.623046875,238.536041259766,226.023147583008,214.102508544922,202.700271606445,191.805923461914,181.447341918945,171.594848632812,162.211395263672,153.292221069336,144.852615356445,136.859939575195,129.278610229492,122.084869384766,115.273597717285,108.814483642578,102.702018737793,96.928092956543,91.4746704101562,86.3277969360352,81.4750137329102,76.8914489746094,72.5689468383789,68.4777374267578,64.6260833740234,60.9971504211426,57.569149017334,54.3426971435547,51.3091125488281,48.4516296386719,45.7645149230957,43.2405242919922,40.8660736083984,38.6300811767578,36.5252914428711,34.5491638183594,32.6934509277344,30.9498176574707,29.3073463439941,27.7627716064453,26.310905456543,24.94309425354,23.65380859375,22.4388256072998,21.2925815582275,20.2127914428711,19.1928825378418,18.2330322265625,17.32932472229,16.4769058227539,15.6711559295654,14.9095163345337,14.1905403137207,13.51087474823,12.8681945800781,12.2604608535767,11.685152053833,11.1410474777222,10.6257915496826,10.1372718811035,9.67442607879639,9.23574733734131,8.81936073303223,8.423415184021,8.0479793548584,7.69129371643066,7.3524055480957,7.03039932250977,6.72467756271362,6.43443441390991,6.15827894210815,5.89544296264648,5.64577484130859,5.4085898399353,5.18278884887695,4.96778297424316,4.76282262802124,4.56794261932373,4.38167667388916,4.20395755767822,4.034255027771,3.87236881256104,3.7176661491394,3.56985235214233,3.42879033088684,3.29393482208252,3.1650059223175,3.04172992706299,2.92383289337158,2.8109929561615,2.70296669006348,2.5995466709137,2.50053334236145,2.40561032295227,2.31467843055725,2.22756838798523,2.14398527145386,2.06382441520691,1.98689556121826,1.91316115856171,1.84239768981934,1.77456092834473,1.70933353900909,1.64674973487854,1.58666336536407,1.52893197536469,1.47349846363068,1.42019391059875,1.36904418468475,1.31993734836578,1.27271342277527,1.22735118865967,1.18371605873108,1.14171314239502,1.10128891468048,1.06237864494324,1.02497482299805,0.9889817237854,0.95431661605835,0.920958697795868,0.888810038566589,0.857857048511505,0.828074038028717,0.799380540847778,0.771741092205048,0.745122790336609,0.719462692737579,0.694717228412628,0.670871675014496,0.647948026657104,0.625913023948669,0.604666292667389,0.584196388721466,0.564445912837982,0.545387148857117,0.527021527290344,0.509311974048615,0.4922194480896,0.47573059797287,0.459827750921249,0.444491237401962,0.429690569639206,0.415387392044067,0.40159210562706,0.388279020786285,0.375434100627899,0.363029330968857,0.35105288028717,0.339493781328201,0.328329622745514,0.317552447319031,0.307137221097946,0.297078490257263,0.287366539239883,0.277984440326691,0.268914103507996,0.260156899690628,0.251694798469543,0.243515461683273,0.235606402158737,0.227970317006111,0.220615655183792,0.213502466678619,0.206627368927002,0.19998787343502,0.193573549389839,0.187366172671318,0.181365713477135,0.175562530755997,0.169950991868973,0.16452294588089,0.159277930855751,0.154206499457359,0.149300888180733,0.144553974270821,0.139964535832405,0.135524183511734,0.131227552890778,0.127071633934975,0.123050883412361,0.119162112474442,0.115398794412613,0.111758626997471,0.108234480023384,0.104825533926487,0.101525321602821,0.0983332097530365,0.0952441319823265,0.092253103852272,0.0893576741218567,0.0865568742156029,0.0838496387004852,0.0812318399548531,0.0786957666277885,0.0762432739138603,0.0738693326711655,0.0715707764029503,0.0693463534116745,0.0671909749507904,0.0651053339242935,0.0630877539515495,0.0611334443092346,0.0592397749423981,0.0574058853089809,0.0556300282478333,0.0539103336632252,0.0522454231977463,0.0506339073181152,0.0490731671452522,0.0475621670484543,0.0460991337895393,0.0446819961071014,0.0433073565363884,0.0419757328927517,0.0406862944364548,0.0394376255571842,0.0382279492914677,0.0370563454926014,0.0359213463962078,0.0348213985562325,0.0337561704218388,0.0327244810760021,0.0317247584462166,0.030755840241909,0.0298167951405048,0.0289072245359421,0.0280260629951954,0.0271728727966547,0.0263454802334309,0.0255436822772026,0.0247666668146849,0.024014201015234,0.0232851449400187,0.0225782487541437,0.0218929927796125,0.021229138597846,0.0205857437103987,0.0199623163789511,0.0193577408790588,0.0187720693647861,0.0182042922824621,0.0176538955420256,0.0171204283833504,0.0166035108268261,0.0161023829132318,0.0156170753762126,0.0151462182402611,0.0146900489926338,0.0142474789172411,0.0138185834512115,0.0134028671309352,0.0129999183118343,0.0126090021803975,0.0122300619259477,0.0118627417832613,0.0115067670121789,0.0111616086214781,0.0108267990872264,0.0105021754279733,0.0101879462599754,0.00988284964114428,0.00958701968193054,0.00930015556514263,0.00902190059423447,0.00875221751630306,0.00849072355777025,0.00823704525828362,0.00799110066145658,0.00775274494662881,0.00752145890146494,0.00729728769510984,0.00707977823913097,0.00686886580660939,0.00666445214301348,0.00646621733903885,0.00627401378005743,0.00608737720176578,0.00590644683688879,0.00573085807263851,0.0055606896057725,0.00539560476318002,0.00523549551144242,0.00508022122085094,0.00492965802550316,0.00478361127898097,0.00464196549728513,0.00450445152819157,0.00437118066474795,0.00424178829416633,0.00411629024893045,0.00399459106847644,0.00387655268423259,0.00376211386173964,0.00365100242197514,0.00354327633976936,0.00343872653320432,0.00333727174438536,0.00323886075057089,0.00314340041950345,0.00305079785175622,0.00296098948456347,0.0028738162945956,0.00278921681456268,0.00270719011314213,0.00262756505981088,0.00255030859261751,0.00247536064125597,0.00240262248553336,0.00233207480050623,0.00226361863315105,0.00219722441397607,0.00213277456350625,0.00207025348208845,0.00200955709442496,0.00195070076733828,0.00189359067007899,0.00183812959585339,0.00178428704384714,0.00173210655339062,0.00168142572510988,0.00163229065947235,0.00158459157682955,0.00153828307520598,0.00149334024172276,0.00144973606802523,0.00140743353404105,0.00136640947312117,0.00132654793560505,0.00128783995751292,0.00125031580682844,0.0012138836318627,0.00117850385140628,0.00114419125020504,0.00111087993718684,0.00107855198439211,0.00104718515649438,0.00101672357413918,0.000987160135991871,0.000958467135205865,0.000930605630856007,0.000903585867490619,0.000877335842233151,0.000851894845254719,0.000827192387077957,0.000803192786406726,0.000779891153797507,0.000757286907173693,0.000735332549083978,0.000714045600034297,0.000693372450768948,0.000673287024255842,0.000653798750136048,0.000634883297607303,0.000616518955212086,0.000598690297920257,0.000581372412852943,0.000564583344385028,0.000548257492482662,0.000532434321939945,0.000517066335305572,0.000502136303111911,0.000487654033349827,0.000473582389531657,0.000459915958344936,0.000446654710685834,0.000433788838563487,0.000421288190409541,0.000409158878028393,0.0003973804123234,0.000385941122658551,0.000374841445591301,0.000364052131772041,0.000353573705069721,0.000343408260960132,0.00033354019979015,0.000323963875416666,0.000314654054818675,0.000305617926642299,0.00029684163746424,0.000288320414256305,0.000280042469967157,0.000272016652161255,0.000264211179455742,0.000256636907579377,0.000249282165896147,0.000242136855376884,0.000235196464927867,0.000228469507419504,0.000221923721255735,0.000215571082662791,0.00020940754620824,0.000203419767785817,0.000197598259546794,0.000191947096027434,0.000186454359209165,0.000181138180778362,0.000175959954503924,0.000170937855727971,0.000166058729519136,0.000161318763275631,0.000156715832417831,0.000152243199408986,0.000147900223964825,0.000143682380439714,0.000139584793942049,0.000135608483105898,0.000131745633552782,0.00012799448450096,0.000124349695397541,0.000120808712381404,0.000117372692329809,0.00011403467215132,0.000110786000732332,0.000107634958112612,0.000104573773569427,0.000101601093774661,9.8721677204594e-05,9.59186290856451e-05,9.32002149056643e-05,9.05551132746041e-05,8.79877770785242e-05,8.54965037433431e-05,8.3069535321556e-05,8.07150354376063e-05,7.84298099461012e-05,7.62071722419932e-05,7.40465184208006e-05,7.19509407645091e-05,6.99149532010779e-05,6.7937231506221e-05,6.60145524307154e-05,6.41479637124576e-05,6.23342202743515e-05,6.05726090725511e-05,5.88608891121112e-05,5.71999989915639e-05,5.55833394173533e-05,5.40162291144952e-05,5.24887218489312e-05,5.10077697981615e-05,4.95671047247015e-05,4.81654424220324e-05,4.68105063191615e-05,4.54920227639377e-05,4.42101554654073e-05,4.29607389378361e-05,4.17523551732302e-05,4.05779937864281e-05,3.94342314393725e-05,3.83241822419222e-05,3.72479225916322e-05,3.61980746674817e-05,3.51797971234191e-05,3.41916129400488e-05,3.32262679876294e-05,3.22919986501802e-05,3.13873752020299e-05,3.05041248793714e-05,2.96475081995595e-05,2.88136052404298e-05,2.80066851701122e-05,2.7218211471336e-05,2.64570189756341e-05,2.57146512012696e-05,2.49926233664155e-05,2.42910955421394e-05,2.36112318816595e-05,2.29497436521342e-05,2.23050792556023e-05,2.1680687495973e-05,2.10733996937051e-05,2.04816551558906e-05,1.99101468751905e-05,1.93514588318067e-05,1.88111043826211e-05,1.82838848559186e-05,1.7774073057808e-05,1.72761829162482e-05,1.67938087543007e-05,1.6324356693076e-05,1.58670663950033e-05,1.54242698044982e-05,1.49925199366407e-05,1.45731628435897e-05]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>iter<\/th>\n      <th>loss<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":[1,2]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
</div>
<div id="plot-the-loss-at-each-iteration" class="section level2" number="13.10">
<h2>
<span class="header-section-number">13.10</span> Plot the loss at each iteration<a class="anchor" aria-label="anchor" href="#plot-the-loss-at-each-iteration"><i class="fas fa-link"></i></a>
</h2>
<div class="sourceCode" id="cb569"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span>
<span class="co"># plot</span>
<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">loss_df</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">iter</span>, y <span class="op">=</span> <span class="va">loss</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="0502-neural_networks-steps_files/figure-html/plot-loss-1.png" width="70%" style="display: block; margin: auto;"></div>

</div>
</div>



  <div class="chapter-nav">
<div class="prev"><a href="neural-networks.html"><span class="header-section-number">12</span> Neural Networks</a></div>
<div class="next"><a href="working-with-a-dataframe.html"><span class="header-section-number">14</span> Working with a data●frame</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#a-neural-network-step-by-step"><span class="header-section-number">13</span> A neural network step-by-step</a></li>
<li><a class="nav-link" href="#introduction-1"><span class="header-section-number">13.1</span> Introduction</a></li>
<li><a class="nav-link" href="#select-device"><span class="header-section-number">13.2</span> Select device</a></li>
<li><a class="nav-link" href="#create-the-dataset"><span class="header-section-number">13.3</span> Create the dataset</a></li>
<li><a class="nav-link" href="#define-the-model-1"><span class="header-section-number">13.4</span> Define the model</a></li>
<li><a class="nav-link" href="#the-loss-function"><span class="header-section-number">13.5</span> The Loss function</a></li>
<li><a class="nav-link" href="#iterate-through-the-dataset-1"><span class="header-section-number">13.6</span> Iterate through the dataset</a></li>
<li>
<a class="nav-link" href="#using-r-generics"><span class="header-section-number">13.7</span> Using R generics</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#simplify-tensor-operations"><span class="header-section-number">13.7.1</span> Simplify tensor operations</a></li></ul>
</li>
<li><a class="nav-link" href="#an-elegant-neural-network"><span class="header-section-number">13.8</span> An elegant neural network</a></li>
<li><a class="nav-link" href="#a-browseable-dataframe"><span class="header-section-number">13.9</span> A browseable dataframe</a></li>
<li><a class="nav-link" href="#plot-the-loss-at-each-iteration"><span class="header-section-number">13.10</span> Plot the loss at each iteration</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/f0nzie/rtorch-minimal-book/blob/master/0502-neural_networks-steps.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/f0nzie/rtorch-minimal-book/edit/master/0502-neural_networks-steps.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>A Minimal rTorch Book</strong>" was written by Alfonso R. Reyes. It was last built on 2020-11-19.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
