# rTorch vs PyTorch

_Last update: `r system("git log -1 --format=\"%ad (%h)\" -- 0103-rtorch_pytorch_whats_different.Rmd", intern = TRUE)`_

## What's different

This chapter will explain the main differences between `PyTorch` and `rTorch`. Most of the things work directly in `PyTorch` but we need to be aware of some minor differences when working with rTorch. Here is a review of existing methods.

Let's start by loading `rTorch`:

```{r}
library(rTorch)
```

## Calling objects from PyTorch

We use the dollar sign or `$` to call a class, function or method from the `rTorch` modules. In this case, from the `torch` module:

```{r}
torch$tensor(c(1, 2, 3))
```

In Python, what we do is using the dot to separate the sub-members of an object:

```{python, engine="python3"}
import torch
torch.tensor([1, 2, 3])
```

## Call functions from `torch`

```{r import-pytorch-modules}
library(rTorch)
# these are the equivalents of the Python import module
nn          <- torch$nn
transforms  <- torchvision$transforms
dsets       <- torchvision$datasets

torch$tensor(c(1, 2, 3))
```

The code above is equivalent to writing this code in Python:

```{python}
import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets

torch.tensor([1, 2, 3])
```


Then we can proceed to extract classes, methods and functions from the `nn`, `transforms`, and `dsets` objects. In this example we use the module `torchvision$datasets` and the function `transforms$ToTensor()`


```{r mnist-digits-train-dataset}
local_folder <- './datasets/mnist_digits'
train_dataset = torchvision$datasets$MNIST(root = local_folder, 
                                           train = TRUE, 
                                           transform = transforms$ToTensor(),
                                           download = TRUE)
train_dataset
```

## Python objects

Sometimes we are interested in knowing the internal components of a class. In that case, we use the `reticulate` function `py_list_attributes()`.

In this example, we want to show the attributes of `train_dataset`:

```{r list-attributes}
reticulate::py_list_attributes(train_dataset)
```

Knowing the internal methods of a class could be useful when we want to refer to a specific property of such class. For example, from the list above, we know that the object `train_dataset` has an attribute `__len__`. We can call it like this:

```{r len-dataset}
train_dataset$`__len__`()
```

## Iterating through datasets

### Enumeration

Given the following training dataset `x_train`, we want to find the number of elements of the tensor. We start by entering a `numpy` array, which then will convert to a tensor with the PyTorch function `from_numpy()`:

```{r create-r-array}
x_train_r <- array(c(3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 
                  9.779, 6.182, 7.59, 2.167, 7.042,
                  10.791, 5.313, 7.997, 3.1), dim = c(15,1))

x_train_np <- r_to_py(x_train_r)
x_train_   <- torch$from_numpy(x_train_np)          # convert to tensor
x_train    <- x_train_$type(torch$FloatTensor)      # make it a a FloatTensor
print(x_train$dtype)
print(x_train)
```

`length` is similar to `nelement` for number of elements:

```{r number-of-elements}
length(x_train)
x_train$nelement()    # number of elements in the tensor
```

### `enumerate` and `iterate`

```{r import-builtins}
py = import_builtins()

enum_x_train = py$enumerate(x_train)
enum_x_train

py$len(x_train)
```

If we directly use `iterate` over the `enum_x_train` object, we get an R list with the index and the value of the `1D` tensor:

```{r iterate-train}
xit = iterate(enum_x_train, simplify = TRUE)
xit
```

### `for-loop` for iteration

Another way of iterating through a dataset that you will see a lot in the PyTorch tutorials is a `loop` through the length of the dataset. In this case, `x_train`. We are using `cat()` for the index (an integer), and `print()` for the tensor, since `cat` doesn't know how to deal with tensors:

```{r loop-iterator}
# reset the iterator
enum_x_train = py$enumerate(x_train)

for (i in 1:py$len(x_train)) {
    obj <- iter_next(enum_x_train)    # next item
    cat(obj[[1]], "\t")     # 1st part or index
    print(obj[[2]])         # 2nd part or tensor
}
```

Similarly, if we want the scalar values but not as tensor, then we will need to use `item()`.

```{r}
# reset the iterator
enum_x_train = py$enumerate(x_train)

for (i in 1:py$len(x_train)) {
    obj <- iter_next(enum_x_train)    # next item
    cat(obj[[1]], "\t")               # 1st part or index
    print(obj[[2]]$item())            # 2nd part or tensor
}
```


> We will find very frequently this kind of iterators when we read a dataset read by `torchvision`. There are several different ways to iterate through these objects as you will find.

## Zero gradient

The zero gradient was one of the most difficult to implement in R if we don't pay attention to the content of the objects carrying the **weights** and **biases**. This happens when the algorithm written in **PyTorch** is not immediately translatable to **rTorch**. This can be appreciated in this example.

> We are using the same seed in the PyTorch and rTorch versions, so, we could compare the results.

### Code version in Python

```{python, python-rainfall-code}
import numpy as np
import torch

torch.manual_seed(0)  # reproducible

# Input (temp, rainfall, humidity)
inputs = np.array([[73, 67, 43],
                   [91, 88, 64],
                   [87, 134, 58],
                   [102, 43, 37],
                   [69, 96, 70]], dtype='float32')

# Targets (apples, oranges)
targets = np.array([[56, 70],
                    [81, 101],
                    [119, 133],
                    [22, 37],
                    [103, 119]], dtype='float32')
                    

# Convert inputs and targets to tensors
inputs  = torch.from_numpy(inputs)
targets = torch.from_numpy(targets)

# random weights and biases
w = torch.randn(2, 3, requires_grad=True)
b = torch.randn(2, requires_grad=True)

# function for the model
def model(x):
  wt = w.t()
  mm = x @ w.t()
  return x @ w.t() + b       # @ represents matrix multiplication in PyTorch

# MSE loss function
def mse(t1, t2):
  diff = t1 - t2
  return torch.sum(diff * diff) / diff.numel()

# Running all together
# Train for 100 epochs
for i in range(100):
  preds = model(inputs)
  loss = mse(preds, targets)
  loss.backward()
  with torch.no_grad():
    w -= w.grad * 0.00001
    b -= b.grad * 0.00001
    w_gz = w.grad.zero_()
    b_gz = b.grad.zero_()
    
# Calculate loss
preds = model(inputs)
loss = mse(preds, targets)
print("Loss: ", loss)    

# predictions
print("\nPredictions:")
preds

# Targets
print("\nTargets:")
targets
```

### Code version in R

```{r rtorch-rainfall-code}
library(rTorch)

torch$manual_seed(0)

device = torch$device('cpu')
# Input (temp, rainfall, humidity)
inputs = np$array(list(list(73, 67, 43),
                   list(91, 88, 64),
                   list(87, 134, 58),
                   list(102, 43, 37),
                   list(69, 96, 70)), dtype='float32')

# Targets (apples, oranges)
targets = np$array(list(list(56, 70), 
                    list(81, 101),
                    list(119, 133),
                    list(22, 37), 
                    list(103, 119)), dtype='float32')


# Convert inputs and targets to tensors
inputs = torch$from_numpy(inputs)
targets = torch$from_numpy(targets)

# random numbers for weights and biases. Then convert to double()
torch$set_default_dtype(torch$float64)

w = torch$randn(2L, 3L, requires_grad=TRUE) #$double()
b = torch$randn(2L, requires_grad=TRUE) #$double()

model <- function(x) {
  wt <- w$t()
  return(torch$add(torch$mm(x, wt), b))
}

# MSE loss
mse = function(t1, t2) {
  diff <- torch$sub(t1, t2)
  mul <- torch$sum(torch$mul(diff, diff))
  return(torch$div(mul, diff$numel()))
}

# Running all together
# Adjust weights and reset gradients
for (i in 1:100) {
  preds = model(inputs)
  loss = mse(preds, targets)
  loss$backward()
  with(torch$no_grad(), {
    w$data <- torch$sub(w$data, torch$mul(w$grad, torch$scalar_tensor(1e-5)))
    b$data <- torch$sub(b$data, torch$mul(b$grad, torch$scalar_tensor(1e-5)))
    
    w$grad$zero_()
    b$grad$zero_()
  })
}

# Calculate loss
preds = model(inputs)
loss = mse(preds, targets)
cat("Loss: "); print(loss)

# predictions
cat("\nPredictions:\n")
preds

# Targets
cat("\nTargets:\n")
targets
```

Notice that while we are in Python, the tensor operation, gradient ($\nabla$) of the weights $w$ times the **Learning Rate** $\alpha$, is:

$$w = -w + \nabla w \; \alpha$$

In Python, it is a very straight forwward and clean code:

```{python py-calc-gradient, eval=FALSE}
w -= w.grad * 1e-5
```

In R, without generics, it shows a little bit more convoluted:

```{r r-calc-gradient1, eval=FALSE}
w$data <- torch$sub(w$data, torch$mul(w$grad, torch$scalar_tensor(1e-5)))
```

## R generic functions

Which why we simplified these common operations using the R generic function. When we use the generic methods from **rTorch** the operation looks much neater.

```{r r-calc-gradient2, eval=FALSE}
w$data <- w$data - w$grad * 1e-5
```

The following two expressions are equivalent, with the first being the long version natural way of doing it in **PyTorch**. The second is using the generics in R for subtraction, multiplication and scalar conversion.

```{r eval=FALSE}
param$data <- torch$sub(param$data,
                        torch$mul(param$grad$float(),
                          torch$scalar_tensor(learning_rate)))
}
```

```{r eval=FALSE}
param$data <- param$data - param$grad * learning_rate
```
